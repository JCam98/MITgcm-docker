<!DOCTYPE html>
<!-- saved from url=(0043)https://docs.tacc.utexas.edu/hpc/stampede2/ -->
<html class="writer-html5" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="author" content="Susan Lindsey"><link rel="canonical" href="https://docs.tacc.utexas.edu/hpc/stampede2/">
      <link rel="shortcut icon" href="https://docs.tacc.utexas.edu/img/favicon.ico">
    <title>Stampede2 - TACC HPC Documentation</title>
    
    <style>
      /* Layers not used in this block are used in later stylesheets */
      @layer foundation.mkdocs, foundation.tacc, base, project;

      /* Import ReadTheDocs styles but in a bottom layer */
      @import url("../../css/theme.css") layer(foundation.mkdocs);
      @import url("../../css/theme_extra.css") layer(foundation.mkdocs);

      /* Undo ReadTheDocs styles for page content */
      @layer foundation.tacc {
        .rst-content :not(img) {
          all: revert;
        }
      }
    </style>
    
      
      
      <style>
        @import url("https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/a11y-light.min.css") layer(foundation);
      </style>
      
      <link href="./Stampede2 - TACC HPC Documentation_files/bootstrap.css" rel="stylesheet">
      <link href="./Stampede2 - TACC HPC Documentation_files/core-styles.css" rel="stylesheet">
      <link href="./Stampede2 - TACC HPC Documentation_files/tacc-docs.css" rel="stylesheet">
    
      <script>
        // Current page data
        var mkdocs_page_name = "Stampede2";
        var mkdocs_page_input_path = "hpc/stampede2.md";
        var mkdocs_page_url = "/hpc/stampede2/";
      </script>
    
    <script src="./Stampede2 - TACC HPC Documentation_files/jquery-3.6.0.min.js" defer=""></script>
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
    
    <script type="module">
      import hljs from 'https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/es/core.min.js';
      import bash from 'https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/es/languages/bash.min.js';
      import ruby from 'https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/es/languages/ruby.min.js';
      import plaintext from 'https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/es/languages/plaintext.min.js';
      import shell from 'https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/es/languages/shell.min.js';
      import nginx from 'https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/es/languages/nginx.min.js';
      import apache from 'https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/es/languages/apache.min.js';
      import xml from 'https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/es/languages/xml.min.js';
      import applescript from 'https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/es/languages/applescript.min.js';
      hljs.registerLanguage('bash', bash);
      hljs.registerLanguage('ruby', ruby);
      hljs.registerLanguage('plaintext', plaintext);
      hljs.registerLanguage('shell', shell);
      hljs.registerLanguage('nginx', nginx);
      hljs.registerLanguage('apache', apache);
      hljs.registerLanguage('xml', xml);
      hljs.registerLanguage('applescript', applescript);
      hljs.registerLanguage('cmd-line', ruby);
      hljs.registerLanguage('job-script', bash);
      hljs.registerLanguage('syntax', plaintext);
      hljs.configure({
        cssSelector: 'pre:is(.highlight, .codehilite) code'
      });
      hljs.highlightAll();
    </script>
     
<style type="text/css">img[onload^="SVGInject("]{visibility:hidden;}</style></head>

<body class="wy-body-for-nav" role="document" data-page-name="hpc/stampede2.md">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
    
          <a href="https://docs.tacc.utexas.edu/">
          <svg xmlns="http://www.w3.org/2000/svg" id="uuid-e2d48ce6-f5f9-4cae-9526-67ec6d765d6b" viewBox="0 0 283.44 111" data-inject-url="https://docs.tacc.utexas.edu/img/tacc-logo-with-words.svg" class="logo"><g id="uuid-fb469f8b-c32f-49f8-8350-3eb2c4d64c0e--inject-1"><rect x="-4.78" y="-4" width="290.67" height="119" fill="none"></rect><path d="m115.43,74.88l-4.96-20.09,13.81-12.12h-17.84l-5.99-20.25-6,20.25h-18.03l13.99,12.13-5,20.08,14.99-12.05,15.03,12.05Z" fill="#231f20"></path><path d="m5,7.8h64.88v17.06h-21.77v52.02h-21.34V24.86H5V7.8Z" fill="#231f20"></path><path d="m138.96,42.33c0-7.96,1.76-35.79,34.46-35.79s32.71,27.84,32.71,27.84l-19.54-.04s-.17-10.04-13.17-10.04-13.04,10.04-13.04,18.04,0,17.95,13.04,17.95,13.04-9.95,13.04-9.95h19.67s0,27.81-32.71,27.81-34.46-27.85-34.46-35.81h0Z" fill="#231f20"></path><path d="m212.46,42.33c0-7.96,1.76-35.79,34.46-35.79s32.71,27.84,32.71,27.84l-19.54-.04s-.17-10.04-13.17-10.04-13.04,10.04-13.04,18.04,0,17.95,13.04,17.95,13.04-9.95,13.04-9.95h19.67s0,27.81-32.71,27.81-34.46-27.85-34.46-35.81h0Z" fill="#231f20"></path></g><g id="uuid-a08da7fa-00d0-43b9-a011-d7ec9d63bda8--inject-1"><polygon points="62.75 76.88 82.02 76.88 86.95 55.83 73.6 44.27 62.75 76.88" fill="#231f20"></polygon><polygon points="137.99 76.88 118.73 76.88 113.79 55.83 127.15 44.27 137.99 76.88" fill="#231f20"></polygon><polygon points="75.13 39.62 92.1 39.62 100.35 11.81 108.55 39.61 125.58 39.61 115 7.81 85.7 7.81 75.13 39.62" fill="#231f20"></polygon><g><path d="m13.37,90.78v2.09h-2.77v10.57h-2.55v-10.57h-2.77v-2.09h8.09Z" fill="#231f20"></path><path d="m21.54,90.78v2.09h-4.52v2.98h4.26v2.09h-4.26v3.4h4.7v2.09h-7.25v-12.66h7.07Z" fill="#231f20"></path><path d="m22.47,90.78h2.82l1.63,4.1,1.61-4.1h2.78l-2.94,6.26,3.16,6.4h-2.87l-1.83-4.29-1.81,4.29h-2.8l3.16-6.4s-2.91-6.26-2.91-6.26Z" fill="#231f20"></path><path d="m34.99,90.78h3.16l3.35,12.66h-2.66l-.58-2.68h-3.37l-.58,2.68h-2.66l3.35-12.66h0Zm.34,7.89h2.48l-1.22-5.69h-.04s-1.22,5.69-1.22,5.69Z" fill="#231f20"></path><path d="m44.44,99.49v.37c0,1.19.34,1.95,1.68,1.95.78,0,1.52-.5,1.52-1.56,0-1.15-.58-1.58-2.27-2.16-2.27-.78-3.32-1.72-3.32-3.83,0-2.46,1.51-3.72,3.92-3.72,2.27,0,3.92.98,3.92,3.4v.35h-2.45c0-1.19-.41-1.88-1.38-1.88-1.15,0-1.45.73-1.45,1.47s.23,1.26,1.31,1.72l1.97.85c1.91.83,2.41,1.81,2.41,3.35,0,2.68-1.63,3.88-4.4,3.88s-4.02-1.35-4.02-3.69v-.51h2.56Z" fill="#231f20"></path><path d="m58.3,90.78h3.16l3.35,12.66h-2.66l-.58-2.68h-3.37l-.58,2.68h-2.66l3.35-12.66h0Zm.34,7.89h2.48l-1.22-5.69h-.04s-1.22,5.69-1.22,5.69Z" fill="#231f20"></path><path d="m65.82,90.78h4.18c3.55,0,4.18,2.43,4.18,6.15,0,4.49-.96,6.51-4.34,6.51h-4.02v-12.66Zm2.55,10.78h1.13c1.77,0,2.13-1.12,2.13-4.56,0-2.87-.23-4.34-2.18-4.34h-1.08s0,8.9,0,8.9Z" fill="#231f20"></path><path d="m77.77,90.78l1.79,9.36h.05l1.88-9.36h2.64l-2.91,12.66h-3.28l-2.91-12.66h2.74Z" fill="#231f20"></path><path d="m86.89,90.78h3.16l3.35,12.66h-2.66l-.58-2.68h-3.37l-.58,2.68h-2.66l3.35-12.66h0Zm.34,7.89h2.48l-1.22-5.69h-.04l-1.22,5.69Z" fill="#231f20"></path><path d="m97.23,90.78l3.09,8.67h.04v-8.67h2.34v12.66h-2.91l-3.12-8.87h-.04v8.87h-2.34v-12.66h2.94Z" fill="#231f20"></path><path d="m109.87,95.06c0-1.84-.37-2.64-1.52-2.64-1.29,0-1.6,1.29-1.6,4.88,0,3.83.53,4.52,1.63,4.52.92,0,1.6-.48,1.6-3.21h2.55c0,2.73-.67,5.09-4.02,5.09-3.83,0-4.31-2.73-4.31-6.58s.48-6.58,4.31-6.58,3.92,2.68,3.92,4.52h-2.56Z" fill="#231f20"></path><path d="m121.06,90.78v2.09h-4.52v2.98h4.26v2.09h-4.26v3.4h4.7v2.09h-7.25v-12.66h7.07Z" fill="#231f20"></path><path d="m122.64,90.78h4.18c3.55,0,4.18,2.43,4.18,6.15,0,4.49-.96,6.51-4.34,6.51h-4.02s0-12.66,0-12.66Zm2.55,10.78h1.13c1.77,0,2.13-1.12,2.13-4.56,0-2.87-.23-4.34-2.18-4.34h-1.08s0,8.9,0,8.9Z" fill="#231f20"></path><path d="m142.35,95.06c0-1.84-.37-2.64-1.52-2.64-1.29,0-1.6,1.29-1.6,4.88,0,3.83.53,4.52,1.63,4.52.92,0,1.6-.48,1.6-3.21h2.55c0,2.73-.67,5.09-4.02,5.09-3.83,0-4.31-2.73-4.31-6.58s.48-6.58,4.31-6.58,3.92,2.68,3.92,4.52h-2.56Z" fill="#231f20"></path><path d="m150.51,90.53c3.83,0,4.31,2.73,4.31,6.58s-.48,6.58-4.31,6.58-4.31-2.73-4.31-6.58.48-6.58,4.31-6.58Zm0,11.28c1.51,0,1.76-1.29,1.76-4.7s-.25-4.7-1.76-4.7-1.75,1.31-1.75,4.7.25,4.7,1.75,4.7Z" fill="#231f20"></path><path d="m156.38,90.78h3.79l1.81,8.94h.04l1.83-8.94h3.78v12.66h-2.34v-10.11h-.04l-2.25,10.11h-1.99l-2.25-10.11h-.04v10.11h-2.34v-12.66Z" fill="#231f20"></path><path d="m169.45,90.78h4.47c2.59,0,3.48,1.9,3.48,3.79,0,2.62-1.61,3.81-4.03,3.81h-1.37v5.05h-2.55v-12.66h0Zm2.55,5.74h1.13c1.01,0,1.7-.6,1.7-1.9s-.5-1.95-1.77-1.95h-1.06v3.85Z" fill="#231f20"></path><path d="m181.05,90.78v8.83c0,1.26.3,2.2,1.49,2.2s1.49-.94,1.49-2.2v-8.83h2.55v8.83c0,3.23-1.97,4.08-4.04,4.08s-4.04-.69-4.04-4.08v-8.83h2.55,0Z" fill="#231f20"></path><path d="m195.61,90.78v2.09h-2.77v10.57h-2.55v-10.57h-2.77v-2.09h8.09Z" fill="#231f20"></path><path d="m196.83,103.44v-12.66h2.55v12.66h-2.55Z" fill="#231f20"></path><path d="m204.22,90.78l3.08,8.67h.04v-8.67h2.34v12.66h-2.91l-3.12-8.87h-.04v8.87h-2.34v-12.66h2.95Z" fill="#231f20"></path><path d="m217.08,94.54c0-.94-.42-2.13-1.45-2.13-1.24,0-1.74,1.28-1.74,4.43s.28,4.96,1.76,4.96c.6,0,1.58-.66,1.58-2.02v-1.29h-1.68v-1.88h4.13v6.83h-1.88v-1.17h-.04c-.6.99-1.47,1.42-2.66,1.42-3.14,0-3.76-2.34-3.76-6.72s.99-6.44,4.25-6.44c2.8,0,3.94,1.44,3.94,4.01h-2.45Z" fill="#231f20"></path><path d="m230.98,95.06c0-1.84-.37-2.64-1.52-2.64-1.29,0-1.6,1.29-1.6,4.88,0,3.83.53,4.52,1.63,4.52.92,0,1.6-.48,1.6-3.21h2.55c0,2.73-.67,5.09-4.03,5.09-3.83,0-4.31-2.73-4.31-6.58s.48-6.58,4.31-6.58,3.92,2.68,3.92,4.52c0,0-2.55,0-2.55,0Z" fill="#231f20"></path><path d="m242.17,90.78v2.09h-4.52v2.98h4.26v2.09h-4.26v3.4h4.7v2.09h-7.25v-12.66h7.07Z" fill="#231f20"></path><path d="m246.57,90.78l3.08,8.67h.04v-8.67h2.34v12.66h-2.91l-3.12-8.87h-.04v8.87h-2.34v-12.66h2.95Z" fill="#231f20"></path><path d="m261.22,90.78v2.09h-2.77v10.57h-2.55v-10.57h-2.77v-2.09h8.09Z" fill="#231f20"></path><path d="m269.39,90.78v2.09h-4.52v2.98h4.26v2.09h-4.26v3.4h4.7v2.09h-7.25v-12.66h7.07,0Z" fill="#231f20"></path><path d="m270.84,90.78h4.72c2.09,0,3.37,1.1,3.37,3.26,0,1.68-.67,2.82-2.11,3.12v.04c1.74.23,2,1.19,2.04,3.87.02,1.33.09,2.02.57,2.27v.11h-2.77c-.25-.35-.3-.83-.32-1.33l-.07-2.43c-.04-.99-.48-1.6-1.51-1.6h-1.37v5.35h-2.55v-12.66h0Zm2.55,5.53h1.06c1.22,0,1.92-.53,1.92-1.9,0-1.17-.62-1.75-1.77-1.75h-1.21v3.65h0Z" fill="#231f20"></path></g></g></svg>
        </a>
    <a href="https://tacc.utexas.edu/" class="cms-link x-link" target="_blank">tacc.utexas.edu</a>




<form id="rtd-search-form" class="wy-form" action="https://docs.tacc.utexas.edu/search.html" method="get" role="search">
    <input type="search" name="q" placeholder="Search docs" title="Type search term here">
</form>

      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">TACC Essentials</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="https://docs.tacc.utexas.edu/"><button class="toctree-expand" title="Open/close menu"></button>Documentation Overview</a>
    <ul>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/basics/conduct/"><button class="toctree-expand" title="Open/close menu"></button>Good Conduct</a>
    <ul>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/basics/conduct/#conduct-loginnodes"><button class="toctree-expand" title="Open/close menu"></button>1. Do Not Run Jobs on the Login Nodes</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/basics/conduct/#conduct-loginnodes-examples">Dos &amp; Don'ts on the Login Nodes</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/basics/conduct/#conduct-filesystems"><button class="toctree-expand" title="Open/close menu"></button>2. Do Not Stress the Shared File Systems</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/basics/conduct/#table-file-system-usage-recommendations">File System Usage Recommendations</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/basics/conduct/#scratchpolicy">Scratch File System Purge Policy</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/basics/conduct/#conduct-filesystems-tips">More File System Tips</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/basics/conduct/#conduct-io">3. Limit Input/Output (I/O) Activity</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/basics/conduct/#conduct-transfers">4. File Transfer Guidelines</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/basics/conduct/#conduct-jobs">5. Job Submission Tips</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/mfa/">Multi-Factor Authentication</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="https://docs.tacc.utexas.edu/basics/help/"><button class="toctree-expand" title="Open/close menu"></button>Help</a>
    <ul>
    </ul>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">HPC</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="" href="https://chameleoncloud.readthedocs.io/en/latest" target="_blank"><i class="fa fa-external-link"></i>Chameleon</a>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/corral/"><button class="toctree-expand" title="Open/close menu"></button>Corral</a>
    <ul>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/corral/#overview"><button class="toctree-expand" title="Open/close menu"></button>System Overview</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/corral/#overview-services">Available Services</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/corral/#overview-plans">Consulting and Data Management Plans</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/corral/#overview-cloudstorage">Object/Cloud Storage on Corral</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/corral/#access"><button class="toctree-expand" title="Open/close menu"></button>System Access</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/corral/#access-external">Basic File System Access from Lonestar6 and Other TACC systems</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/corral/#policies"><button class="toctree-expand" title="Open/close menu"></button>Usage Policies</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/corral/#policies-category1">"Category 1", HIPAA-PHI, and other restricted data types</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/corral/#policies-quotas">Quotas</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/corral/#policies-retention">Data Retention Policies</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/corral/#transferring"><button class="toctree-expand" title="Open/close menu"></button>Transferring your Files to Corral</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/corral/#transferring-cl">Command-line data transfer</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/corral/#transferring-staging">Staging to and from Lonestar6 File Systems</a>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/corral/#cyberduck"><button class="toctree-expand" title="Open/close menu"></button>Transferring Using Cyberduck</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/corral/#cyberduck-config">Cyberduck Configuration and Use</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/corral/#managing"><button class="toctree-expand" title="Open/close menu"></button>Managing Files &amp; Permissions</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/corral/#managing-unix">Unix Permissions</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/corral/#managing-acls">Managing Files and Permissions using ACLs</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/corral/#managing-chmod">Managing Permissions with chmod</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/corral/#snapshots">Snapshots and File Retrieval</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/corral/#refs">References</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/"><button class="toctree-expand" title="Open/close menu"></button>Frontera</a>
    <ul>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#notices">Notices</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#intro">Introduction</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#quickstart">Quickstart</a>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#admin"><button class="toctree-expand" title="Open/close menu"></button>Account Administration</a>
        <ul>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#admin-account"><button class="toctree-expand" title="Open/close menu"></button>Setting up Your Account</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#admin-account-allocation">Check your Allocation Status</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#admin-account-mfa">Multi-Factor Authentication</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#admin-access"><button class="toctree-expand" title="Open/close menu"></button>Access the System</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#admin-access-ssh">Secure Shell (SSH)</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#admin-configuring"><button class="toctree-expand" title="Open/close menu"></button>Configuring Your Account</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#admin-configuring-shell">Linux Shell</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#admin-configuring-envvars">Environment Variables</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#admin-configuring-diagnostics">Account-Level Diagnostics</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#admin-configuring-modules">Using Modules to Manage your Environment</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#portal">Frontera User Portal</a>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#system"><button class="toctree-expand" title="Open/close menu"></button>System Architecture</a>
        <ul>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#system-clx"><button class="toctree-expand" title="Open/close menu"></button>Cascade Lake (CLX) Compute Nodes</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#table1">Table 1. CLX Specifications</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#system-largememory"><button class="toctree-expand" title="Open/close menu"></button>Large Memory Nodes</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#table2">Table 2. Large Memory Nodes</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#system-gpu"><button class="toctree-expand" title="Open/close menu"></button>GPU Nodes</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#table3">Table 3. Frontera GPU node specifications</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#system-login">Login Nodes</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#system-network">Network</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#files"><button class="toctree-expand" title="Open/close menu"></button>Managing Files</a>
        <ul>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#files-filesystems"><button class="toctree-expand" title="Open/close menu"></button>File Systems</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#table4a">Table 4a. File Systems</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#table4b">Table 4b. Scratch File Systems</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#scratchpolicy">Scratch File System Purge Policy</a>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#files-navigating"><button class="toctree-expand" title="Open/close menu"></button>Navigating the Shared File Systems</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#figure3">Figure 3. Stockyard File System</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#table5">Table 5. Built-in Account Level Aliases</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#files-striping">Striping Large Files</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#transferring"><button class="toctree-expand" title="Open/close menu"></button>Transferring your Files</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#transferring-windows">Windows Users</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#transferring-gct">Grid Community Toolkit</a>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#transferring-ssh"><button class="toctree-expand" title="Open/close menu"></button>SSH Utilities: scp &amp; rsync</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#transferring-scp">Transferring Files with scp</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#transferring-rsync">Transferring Files with rsync</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#transferring-sharing">Sharing Files with Collaborators</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#launching-applications"><button class="toctree-expand" title="Open/close menu"></button>Launching Applications</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#one-serial-application">One Serial Application</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#one-multi-threaded-application">One Multi-Threaded Application</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#one-mpi-application">One MPI Application</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#one-hybrid-mpithreads-application">One Hybrid (MPI+Threads) Application</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#more-than-one-serial-application-in-the-same-job">More Than One Serial Application in the Same Job</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#mpi-applications-one-at-a-time">MPI Applications One at a Time</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#more-than-one-mpi-application-running-concurrently">More than One MPI Application Running Concurrently</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#more-than-one-openmp-application-running-concurrently">More than One OpenMP Application Running Concurrently</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#running"><button class="toctree-expand" title="Open/close menu"></button>Running Jobs</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#running-jobaccounting">Job Accounting</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#running-requesting">Requesting Resources</a>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#running-queues"><button class="toctree-expand" title="Open/close menu"></button>Frontera Production Queues</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#table6">Table 6. Frontera Production Queues</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#running-computenodes"><button class="toctree-expand" title="Open/close menu"></button>Accessing the Compute Nodes</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#figure2">Figure 2. Login and Compute Nodes</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#running-sbatch"><button class="toctree-expand" title="Open/close menu"></button>Submitting Batch Jobs with sbatch</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#table7">Table 7. Common sbatch Options</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#running-interactive">Interactive Sessions with idev and srun</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#running-ssh">Interactive Sessions using SSH</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#running-slurmenvvars">Slurm Environment Variables</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#sample-job-scripts">Sample Job Scripts</a>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#monitoring"><button class="toctree-expand" title="Open/close menu"></button>Job Management</a>
        <ul>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#monitoring-queues"><button class="toctree-expand" title="Open/close menu"></button>Monitoring Queue Status with sinfo and qlimits</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#monitoring-queues-qlimits">TACC's qlimits command</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#monitoring-queues-sinfo">Slurm's sinfo command</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#monitoring-jobs"><button class="toctree-expand" title="Open/close menu"></button>Monitoring Job Status</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#monitoring-jobs-squeue">Slurm's squeue command</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#monitoring-jobs-showq">TACC's showq utility</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#monitoring-dependent">Dependent Jobs using sbatch</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#building"><button class="toctree-expand" title="Open/close menu"></button>Building Software</a>
        <ul>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#building-basics"><button class="toctree-expand" title="Open/close menu"></button>Basics of Building Software</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#building-basics-intel">Intel Compilers</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#building-basics-gnu">GNU Compilers</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#building-basics-steps">Compiling and Linking as Separate Steps</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#building-basics-paths">Include and Library Paths</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#building-basics-mpi">Compiling and Linking MPI Programs</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#building-basics-thirdparty">Building Third-Party Software</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#building-mkl"><button class="toctree-expand" title="Open/close menu"></button>The Intel Math Kernel Library (MKL)</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#building-mkl-intel">MKL with Intel Compilers</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#building-mkl-gnu">MKL with GNU Compilers</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#building-mkl-thirdparty">MKL with BLAS/LAPACK and Third-Party Software</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#building-mkl-blas">MKL with BLAS/LAPACK and TACC's MATLAB, Python, and R Modules</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#building-mkl-threading">Controlling Threading in MKL</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#building-mkl-cluster">Using ScaLAPACK, Cluster FFT, and Other MKL Cluster Capabilities</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#building-performance"><button class="toctree-expand" title="Open/close menu"></button>Building for Performance on Frontera</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#building-performance-compiler">Recommended Compiler</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#building-performance-flags">Architecture-Specific Flags</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#programming"><button class="toctree-expand" title="Open/close menu"></button>Programming and Performance</a>
        <ul>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#programming-general"><button class="toctree-expand" title="Open/close menu"></button>Programming and Performance: General</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#programming-general-profiling">Timing and Profiling</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#programming-general-datalocality">Data Locality</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#programming-vectorization">Vectorization</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#programming-more">Learning More</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#programming-clx">Programming and Performance: CLX</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#programming-fileio">File Operations: I/O Performance</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#ml"><button class="toctree-expand" title="Open/close menu"></button>Machine Learning</a>
        <ul>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#ml-pytorch"><button class="toctree-expand" title="Open/close menu"></button>Running PyTorch</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#ml-pytorch-single">Single-Node</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#ml-pytorch-multi">Multi-Node</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#ml-tensorflow"><button class="toctree-expand" title="Open/close menu"></button>Running Tensorflow</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#single-node">Single-Node</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#ml-tensorflow-multi">Multi-Node</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#visualization-and-vnc-sessions"><button class="toctree-expand" title="Open/close menu"></button>Visualization and VNC Sessions</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#remote-desktop-access">Remote Desktop Access</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#running-applications-on-the-remote-desktop">Running Applications on the Remote Desktop</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#running-parallel-applications-from-the-desktop">Running Parallel Applications from the Desktop</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#running-openglx-applications-on-the-desktop">Running OpenGL/X Applications On The Desktop</a>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#parallel-visit-on-frontera"><button class="toctree-expand" title="Open/close menu"></button>Parallel VisIt on Frontera</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#preparing-data-for-parallel-visit">Preparing Data for Parallel Visit</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#parallel-paraview-on-frontera">Parallel ParaView on Frontera</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#jupyter"><button class="toctree-expand" title="Open/close menu"></button>Jupyter</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#jupyter-launch">Launch a Session</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#jupyter-refs">References</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#cloudservices"><button class="toctree-expand" title="Open/close menu"></button>Cloud Services Integration</a>
        <ul>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#cloudservices-google"><button class="toctree-expand" title="Open/close menu"></button>Google Cloud Platform</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#cloudservices-google-requestaccess">Request Access</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#cloudservices-google-storage">Storage basics</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#cloudservices-amazon"><button class="toctree-expand" title="Open/close menu"></button>Amazon Web Services (AWS)</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#cloudservices-amazon-requestaccess">Request Access</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#cloudservices-amazon-login">Log In to the Console</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#cloudservices-amazon-mfa">Add MFA</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#cloudservices-amazon-keys">Add CLI and API access key</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#cloudservices-amazon-allset">All Set</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#cloudservices-azure"><button class="toctree-expand" title="Open/close menu"></button>Microsoft's Azure Cloud Service</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#cloudservices-azure-requestaccess">Request Access</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#cloudservices-azure-storage">Create a Storage Group and Account</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#cloudservices-azure-keys">Retrieve Account Access Keys</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#cloudservices-azure-cli">Install the Azure Client for CLI Access</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#cloudservices-azure-upload">Upload a File</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#containers">Containers</a>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#help"><button class="toctree-expand" title="Open/close menu"></button>Help Desk</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#best-practices-aka-help-us-help-you">Best Practices aka Help Us Help You</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/frontera/#refs">References</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/"><button class="toctree-expand" title="Open/close menu"></button>Lonestar6</a>
    <ul>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#notices">Notices</a>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#intro"><button class="toctree-expand" title="Open/close menu"></button>Introduction</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#intro-allocations">Allocations</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#system"><button class="toctree-expand" title="Open/close menu"></button>System Architecture</a>
        <ul>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#system-compute"><button class="toctree-expand" title="Open/close menu"></button>Compute Nodes</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#table1">Table 1. Compute Node Specifications</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#system-login">Login Nodes</a>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#system-vmsmall"><button class="toctree-expand" title="Open/close menu"></button>vm-small Queue Nodes</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#table15">Table 1.5. vm-small Compute Node Specifications</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#system-gpu"><button class="toctree-expand" title="Open/close menu"></button>GPU Nodes</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#table2">Table 2. GPU Node Specifications</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#system-network">Network</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#files"><button class="toctree-expand" title="Open/close menu"></button>Managing Files</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#table3">Table 3. File Systems</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#scratchpolicy">Scratch File System Purge Policy</a>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#files-navigating"><button class="toctree-expand" title="Open/close menu"></button>Navigating the Shared File Systems</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#table4">Table 4. Built-in Account Level Aliases</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#files-transferring"><button class="toctree-expand" title="Open/close menu"></button>Transferring your Files</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#files-transferring-scp">Transferring with scp</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#files-transferring-rsync">Transferring with rsync</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#files-sharing">Sharing Files with Collaborators</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#access"><button class="toctree-expand" title="Open/close menu"></button>Access the System</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#access-ssh">Secure Shell (SSH)</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#admin"><button class="toctree-expand" title="Open/close menu"></button>Account Administration</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#admin-allocations">Check your Allocation Status</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#admin-mfa">Multi-Factor Authentication</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#admin-shell">Linux Shell</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#admin-envvars">Environment Variables</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#account-level-diagnostics">Account-Level Diagnostics</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#admin-modules">Using Modules to Manage your Environment</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#building"><button class="toctree-expand" title="Open/close menu"></button>Building Software</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#building-basics">Basics of Building Software</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#building-intelcompiler">Intel Compilers</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#building-gnucompilers">GNU Compilers</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#building-compiling">Compiling and Linking as Separate Steps</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#building-include">Include and Library Paths</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#building-mpi">Compiling and Linking MPI Programs</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#building-thirdparty">Building Third-Party Software</a>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#mkl"><button class="toctree-expand" title="Open/close menu"></button>Intel Math Kernel Library (MKL)</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#mkl-intel">MKL with Intel C, C++, and Fortran Compilers</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#mkl-gnu">MKL with GNU C, C++, and Fortran Compilers</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#mkl-thirdparty">Using MKL as BLAS/LAPACK with Third-Party Software</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#mkl-tacc">Using MKL as BLAS/LAPACK with TACC's MATLAB, Python, and R Modules</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#mkl-threading">Controlling Threading in MKL</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#mkl-othercapabilities">Using ScaLAPACK, Cluster FFT, and Other MKL Cluster Capabilities</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#launching"><button class="toctree-expand" title="Open/close menu"></button>Launching Applications</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#launching-serial">One Serial Application</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#launching-parametric">Parametric Sweep / HTC jobs</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#launching-multithreaded">One Multi-Threaded Application</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#launching-mpi">One MPI Application</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#launching-hybrid">One Hybrid (MPI+Threads) Application</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#launching-serialmorethanone">More Than One Serial Application in the Same Job</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#launching-mpioneatatime">MPI Applications One at a Time</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#launching-mpiconcurrent">More Than One MPI Application Running Concurrently</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#launching-openmpconcurrent">More than One OpenMP Application Running Concurrently</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#running"><button class="toctree-expand" title="Open/close menu"></button>Running Jobs</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#jobaccounting">Job Accounting</a>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#queues"><button class="toctree-expand" title="Open/close menu"></button>Production Queues</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#table5">Table 5. Production Queues</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#scripts">Sample Job Scripts</a>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#jobs"><button class="toctree-expand" title="Open/close menu"></button>Job Management</a>
        <ul>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#jobs-monitoring"><button class="toctree-expand" title="Open/close menu"></button>Monitoring Queue Status</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#jobs-monitoring-qlimits">TACC's qlimits command</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#jobs-monitoring-sinfo">Slurm's sinfo command</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#jobs-monitoring-jobstatus"><button class="toctree-expand" title="Open/close menu"></button>Monitoring Job Status</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#sjobs-monitoring-queuestatus">Slurm's squeue command</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#jobs-monitoring-showq">TACC's showq utility</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#jobs-dependencies">Dependent Jobs using sbatch</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#jobs-other">Other Job Management Commands</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#vis"><button class="toctree-expand" title="Open/close menu"></button>Visualization and VNC Sessions</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#vis-remote">Remote Desktop Access</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#vis-apps">Applications on the Remote Desktop</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#vis-parallelapps">Parallel Applications from the Desktop</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#vis-opengl">OpenGL/X Applications On The Desktop</a>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#vis-visit"><button class="toctree-expand" title="Open/close menu"></button>Parallel VisIt on Lonestar6</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#vis-visit-preparingdata">Preparing Data for Parallel Visit</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#vis-paraview">Parallel ParaView on Lonestar6</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/lonestar6/#help">Help Desk</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="" href="https://docs.jetstream-cloud.org/" target="_blank"><i class="fa fa-external-link"></i>Jetstream2</a>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/"><button class="toctree-expand" title="Open/close menu"></button>Maverick2</a>
    <ul>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#notices">Notices</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#intro">Introduction</a>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#overview"><button class="toctree-expand" title="Open/close menu"></button>System Overview</a>
        <ul>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#overview-computenodes"><button class="toctree-expand" title="Open/close menu"></button>GTX Compute Nodes</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#table-1-gtx-compute-node-specifications">Table 1. GTX Compute Node Specifications</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#overview-v100"><button class="toctree-expand" title="Open/close menu"></button>V100 Compute Nodes</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#table-2-v100-compute-node-specifications">Table 2. V100 Compute Node Specifications</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#overview-p100"><button class="toctree-expand" title="Open/close menu"></button>P100 Compute Nodes</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#table-3-p100-compute-node-specifications">Table 3. P100 Compute Node Specifications</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#overview-loginnodes">Login Nodes</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#overview-network">Network</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#overview-filesystems">File Systems</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#access"><button class="toctree-expand" title="Open/close menu"></button>Accessing the System</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#access-ssh">Secure Shell (SSH)</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#files"><button class="toctree-expand" title="Open/close menu"></button>Managing Your Files</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#table4">Table 4. File Systems</a>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#files-navigating"><button class="toctree-expand" title="Open/close menu"></button>Navigating the Shared File Systems</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#table5">Table 5. Built-in Account Level Aliases</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#transferring-scp">Transferring Files Using scp and rsync</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#files-sharing">Sharing Files with Collaborators</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#files-smallfiles">Notes on Small Files Under Lustre</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#files-striping">Striping Large Files</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#running"><button class="toctree-expand" title="Open/close menu"></button>Running Jobs</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#jobaccounting">Job Accounting</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#running-slurm">Slurm Job Scheduler</a>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#running-queues"><button class="toctree-expand" title="Open/close menu"></button>Slurm Partitions (Queues)</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#table6">Table 6. Maverick2 Production Queues</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#scripts">Job Scripts</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#remote-desktop-access">Remote Desktop Access</a>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#software"><button class="toctree-expand" title="Open/close menu"></button>Software</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#software-ml">Deep Learning Packages</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#software-building">Building Software</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#help">Help Desk</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/maverick2/#refs">References</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/ranch/"><button class="toctree-expand" title="Open/close menu"></button>Ranch</a>
    <ul>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/ranch/#notices">Notices</a>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/ranch/#intro"><button class="toctree-expand" title="Open/close menu"></button>Introduction</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/ranch/#intro-use">Intended Use</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/ranch/#intro-configuration">System Configuration</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/ranch/#access"><button class="toctree-expand" title="Open/close menu"></button>System Access</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/ranch/#access-envvars">Ranch Environment Variables</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/ranch/#access-programs">Accessing Files from Within Running Programs</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/ranch/#organizing"><button class="toctree-expand" title="Open/close menu"></button>Organizing Your Data</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/ranch/#organizing-quotas">Ranch Quotas</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/ranch/#organizing-quotas">Monitor your Disk Usage and File Counts</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/ranch/#projects">Ranch "Project" Storage</a>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/ranch/#transferring"><button class="toctree-expand" title="Open/close menu"></button>Transferring Data</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/ranch/#transferring-manipulating">Manipulating Files within Ranch</a>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/ranch/#transferring-methods"><button class="toctree-expand" title="Open/close menu"></button>Data Transfer Methods</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/ranch/#transferring-methods-scp">Secure Copy with scp Command</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/ranch/#transferring-methods-rsync">Remote Sync with rsync command</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/ranch/#transferring-largedata"><button class="toctree-expand" title="Open/close menu"></button>Large Data Usage and Transfers</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/ranch/#transferring-largedata-examples">Examples</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/ranch/#conduct">Good Conduct on Ranch</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/ranch/#help">Help Desk</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/ranch/#refs">References</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list" aria-expanded="true"><a class="reference internal current" href="https://docs.tacc.utexas.edu/hpc/stampede2/"><button class="toctree-expand" title="Open/close menu"></button>Stampede2</a>
    <ul class="current">
    <li class="toctree-l2" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#notices">Notices</a>
    </li>
    <li class="toctree-l2" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#intro">Introduction</a>
    </li>
    <li class="toctree-l2 has-list" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#overview"><button class="toctree-expand" title="Open/close menu"></button>System Overview</a>
        <ul>
    <li class="toctree-l3 has-list" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#overview-phase1computenodes"><button class="toctree-expand" title="Open/close menu"></button>KNL Compute Nodes</a>
        <ul>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#table1">Table 1. KNL Compute Node Specifications</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3 has-list" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#overview-skxcomputenodes"><button class="toctree-expand" title="Open/close menu"></button>SKX Compute Nodes</a>
        <ul>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#table2">Table 2. Stampede2 SKX Compute Node Specifications</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3 has-list" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#overview-icxcomputenodes"><button class="toctree-expand" title="Open/close menu"></button>ICX Compute Nodes</a>
        <ul>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#table2a">Table 2a. Stampede2 ICX Compute Node Specifications</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#overview-loginnodes">Login Nodes</a>
    </li>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#overview-network">Network</a>
    </li>
    <li class="toctree-l3 has-list" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#overview-filesystems"><button class="toctree-expand" title="Open/close menu"></button>File Systems Introduction</a>
        <ul>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#table3">Table 3. Stampede2 File Systems</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#scratchpolicy">Scratch File System Purge Policy</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#access"><button class="toctree-expand" title="Open/close menu"></button>Accessing the System</a>
        <ul>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#access-ssh">Secure Shell (SSH)</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#using"><button class="toctree-expand" title="Open/close menu"></button>Using Stampede2</a>
        <ul>
    <li class="toctree-l3 has-list" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#using-account"><button class="toctree-expand" title="Open/close menu"></button>Configuring Your Account</a>
        <ul>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#using-account-shell">Linux Shell</a>
    </li>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#using-account-envvars">Environment Variables</a>
    </li>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#using-account-diagnostics">Account-Level Diagnostics</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#using-computenodes">Accessing the Compute Nodes</a>
    </li>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#using-modules">Using Modules to Manage your Environment</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#files"><button class="toctree-expand" title="Open/close menu"></button>Managing Your Files</a>
        <ul>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#files-filesystems">Navigating the Shared File Systems</a>
    </li>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#files-striping">Striping Large Files</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#transferring"><button class="toctree-expand" title="Open/close menu"></button>Transferring Files</a>
        <ul>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#transferring-scp">with scp</a>
    </li>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#transferring-rsync">with rsync</a>
    </li>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#transferring-gct">with  Grid Community Toolkit</a>
    </li>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#files-sharing">Sharing Files with Collaborators</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#building"><button class="toctree-expand" title="Open/close menu"></button>Building Software</a>
        <ul>
    <li class="toctree-l3 has-list" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#building-basics"><button class="toctree-expand" title="Open/close menu"></button>Basics of Building Software</a>
        <ul>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#building-basics-intel">Intel Compilers</a>
    </li>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#building-basics-gnu">GNU Compilers</a>
    </li>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#building-basics-complink">Compiling and Linking as Separate Steps</a>
    </li>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#building-basics-inclib">Include and Library Paths</a>
    </li>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#building-basics-mpi">Compiling and Linking MPI Programs</a>
    </li>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#building-basics-thirdparty">Building Third-Party Software in Your Own Account</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3 has-list" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#mkl"><button class="toctree-expand" title="Open/close menu"></button>Intel Math Kernel Library (MKL)</a>
        <ul>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#mkl-intel">MKL with Intel C, C++, and Fortran Compilers</a>
    </li>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#mkl-gnu">MKL with GNU C, C++, and Fortran Compilers</a>
    </li>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#mkl-thirdparty">Using MKL as BLAS/LAPACK with Third-Party Software</a>
    </li>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#mkl-tacc">Using MKL as BLAS/LAPACK with TACC's MATLAB, Python, and R Modules</a>
    </li>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#mkl-threading">Controlling Threading in MKL</a>
    </li>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#mkl-othercapabilities">Using ScaLAPACK, Cluster FFT, and Other MKL Cluster Capabilities</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3 has-list" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#building-performance"><button class="toctree-expand" title="Open/close menu"></button>Building for Performance on Stampede2</a>
        <ul>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#building-performance-compiler">Compiler</a>
    </li>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#building-performance-architecture">Architecture-Specific Flags</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#running"><button class="toctree-expand" title="Open/close menu"></button>Running Jobs</a>
        <ul>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#jobaccounting">Job Accounting</a>
    </li>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-slurm">Slurm Job Scheduler</a>
    </li>
    <li class="toctree-l3 has-list" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-queues"><button class="toctree-expand" title="Open/close menu"></button>Slurm Partitions (Queues)</a>
        <ul>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#table5">Table 5. Production Queues</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3 has-list" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-sbatch"><button class="toctree-expand" title="Open/close menu"></button>Submitting Batch Jobs with sbatch</a>
        <ul>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#table6">Table 6. Common sbatch Options</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3 has-list" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-launching"><button class="toctree-expand" title="Open/close menu"></button>Launching Applications</a>
        <ul>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-launching-serial">Launching One Serial Application</a>
    </li>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-launching-multi">Launching One Multi-Threaded Application</a>
    </li>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-launching-mpi">Launching One MPI Application</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-launching-hybrid">Launching One Hybrid (MPI+Threads) Application</a>
    </li>
    <li class="toctree-l3 has-list" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-launching-serialmorethanone"><button class="toctree-expand" title="Open/close menu"></button>More Than One Serial Application in the Same Job</a>
        <ul>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-launching-consecutivempi">MPI Applications One at a Time</a>
    </li>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-launching-mpisimultaneous">More than One MPI Application Running Concurrently</a>
    </li>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-launching-openmpsimultaneous">More than One OpenMP Application Running Concurrently</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-idev">Interactive Sessions with idev and srun</a>
    </li>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-ssh">Interactive Sessions using ssh</a>
    </li>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-slurmenvvars">SLURM Environment Variables</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#scripts"><button class="toctree-expand" title="Open/close menu"></button>Job Scripts</a>
        <ul>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#scripts-knl">KNL Nodes</a>
    </li>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#scripts-skx">SKX Nodes</a>
    </li>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#scripts-icx">ICX Nodes</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#monitoring"><button class="toctree-expand" title="Open/close menu"></button>Monitoring Jobs and Queues</a>
        <ul>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#monitoring-queue">Monitoring Queue Status with sinfo and qlimits</a>
    </li>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#monitoring-squeue">Monitoring Job Status with squeue</a>
    </li>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#monitoring-showq">Monitoring Job Status with showq</a>
    </li>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#monitoring-other">Other Job Management Commands (scancel, scontrol, and sacct)</a>
    </li>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#monitoring-dependent">Dependent Jobs using sbatch</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#vis"><button class="toctree-expand" title="Open/close menu"></button>Visualization and VNC Sessions</a>
        <ul>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#vis-remote">Remote Desktop Access</a>
    </li>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#vis-apps">Running Applications on the VNC Desktop</a>
    </li>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#vis-parallelapps">Running Parallel Applications from the Desktop</a>
    </li>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#vis-opengl">Running OpenGL/X Applications On The Desktop</a>
    </li>
    <li class="toctree-l3 has-list" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#vis-visit"><button class="toctree-expand" title="Open/close menu"></button>Parallel VisIt on Stampede2</a>
        <ul>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#vis-visit-preparingdata">Preparing Data for Parallel Visit</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#vis-paraview">Parallel ParaView on Stampede2</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming"><button class="toctree-expand" title="Open/close menu"></button>Programming and Performance</a>
        <ul>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-general-timingprofiling">Timing and Profiling</a>
    </li>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-general-datalocality">Data Locality</a>
    </li>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-general-vectorization">Vectorization</a>
    </li>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-general-more">Learning More</a>
    </li>
    <li class="toctree-l3 has-list" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-knl"><button class="toctree-expand" title="Open/close menu"></button>Programming and Performance: KNL</a>
        <ul>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-knl-architecture">Architecture</a>
    </li>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-knl-memorymodes">Memory Modes</a>
    </li>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-knl-clustermodes">Cluster Modes</a>
    </li>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-knl-managingmemory">Managing Memory</a>
    </li>
    <li class="toctree-l4" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-knl-bestpractices">Best Known Practices and Preliminary Observations (KNL)</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-skx">Programming and Performance: SKX and ICX</a>
    </li>
    <li class="toctree-l3" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-fileio">File Operations: I/O Performance</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#refs">References</a>
    </li>
    <li class="toctree-l2" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#help">Help Desk</a>
    </li>
    <li class="toctree-l2" aria-expanded="false"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stampede2/#history">Revision History</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stallion/"><button class="toctree-expand" title="Open/close menu"></button>Stallion</a>
    <ul>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stallion/#system-configuration"><button class="toctree-expand" title="Open/close menu"></button>System Configuration</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stallion/#file-systems">File Systems</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stallion/#computing-environment">Computing Environment</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stallion/#applications"><button class="toctree-expand" title="Open/close menu"></button>Applications</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stallion/#displaycluster">DisplayCluster</a>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stallion/#scalable-adaptive-graphics-environment-sage"><button class="toctree-expand" title="Open/close menu"></button>Scalable Adaptive Graphics Environment (SAGE)</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stallion/#launching-sage">Launching SAGE</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stallion/#viewing-images-with-sage">Viewing images with SAGE</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stallion/#playing-animations-with-sage">Playing animations with SAGE</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/hpc/stallion/#references">References</a>
    </li>
    </ul>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Software</span></p>
              <ul>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/abaqus/"><button class="toctree-expand" title="Open/close menu"></button>ABAQUS</a>
    <ul>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/abaqus/#access"><button class="toctree-expand" title="Open/close menu"></button>Request Access</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/abaqus/#access-license">License Tokens</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/abaqus/#installations">Installations</a>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/abaqus/#running"><button class="toctree-expand" title="Open/close menu"></button>Running ABAQUS</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/abaqus/#running-interactive">Interactively</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/abaqus/#running-batch">Batch Mode</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/abaqus/#jobscript">Job Script</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/abaqus/#ips">TACC Resources IP Address Ranges</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/abaqus/#refs">References</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/alphafold/"><button class="toctree-expand" title="Open/close menu"></button>AlphaFold</a>
    <ul>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/alphafold/#installations">Installations</a>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/alphafold/#running"><button class="toctree-expand" title="Open/close menu"></button>Running AlphaFold</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/alphafold/#running-singlesequence">Single Sequence Prediction</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/alphafold/#running-multiplesequence">Multiple Sequence Predictions</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/alphafold/#refs">References</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/ansys/"><button class="toctree-expand" title="Open/close menu"></button>ANSYS</a>
    <ul>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/ansys/#licenses">Licenses</a>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/ansys/#installations"><button class="toctree-expand" title="Open/close menu"></button>Installations</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/ansys/#table-1-installations-at-tacc">Table 1. Installations at TACC</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/ansys/#running-ansys"><button class="toctree-expand" title="Open/close menu"></button>Running ANSYS</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/ansys/#interactive-mode">Interactive Mode</a>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/ansys/#running-batch"><button class="toctree-expand" title="Open/close menu"></button>Batch Mode</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/ansys/#table2">Table 2. Binaries Location</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/ansys/#table3">Table 3. User Guides - Running Jobs</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/ansys/#references">References</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/cdtools/"><button class="toctree-expand" title="Open/close menu"></button>CD Tools</a>
    <ul>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/cdtools/#setup"><button class="toctree-expand" title="Open/close menu"></button>Using CD Tools</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/cdtools/#setup-1">1. Initialize CD Tools Environment Variable</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/cdtools/#setup-2">2. Distribute Files to Each Node's /tmp Space</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/cdtools/#setup-3">3. Collect your Output Files</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/cdtools/#notes">Notes</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/gaussian/"><button class="toctree-expand" title="Open/close menu"></button>Gaussian</a>
    <ul>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/gaussian/#licenses"><button class="toctree-expand" title="Open/close menu"></button>Licenses</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/gaussian/#access-ut">UT Austin Students, Staff, and Faculty</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/gaussian/#access-other">Other Academic Users</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/gaussian/#running">Running Gaussian</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/gaussian/#script">Sample Job Script</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/gaussian/#refs">References</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/gromacs/"><button class="toctree-expand" title="Open/close menu"></button>GROMACS</a>
    <ul>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/gromacs/#installations"><button class="toctree-expand" title="Open/close menu"></button>Installations</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/gromacs/#table1">Table 1. Environment Variables</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/gromacs/#running"><button class="toctree-expand" title="Open/close menu"></button>Running GROMACS</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/gromacs/#running-sbatch">Batch Mode</a>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/gromacs/#jobscript-frontera"><button class="toctree-expand" title="Open/close menu"></button>Frontera Job Scripts</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/gromacs/#jobscript-frontera-cpu">CPU</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/gromacs/#jobscript-frontera-gpu">GPU</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/gromacs/#jobscript-stampede2">Stampede2 Job Script</a>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/gromacs/#jobscript-lonestar6"><button class="toctree-expand" title="Open/close menu"></button>Lonestar6 Job Scripts</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/gromacs/#jobscript-lonestar6-cpu">CPU</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/gromacs/#jobscript-lonestar6-gpu">GPU</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/gromacs/#references">References</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/idev/"><button class="toctree-expand" title="Open/close menu"></button>idev</a>
    <ul>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/idev/#intro">Introduction</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/idev/#works">How it works</a>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/idev/#interactive"><button class="toctree-expand" title="Open/close menu"></button>Accessing Nodes Interactively</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/idev/#examples">Examples</a>
    </li>
        </ul>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/irods/"><button class="toctree-expand" title="Open/close menu"></button>IRODS</a>
    <ul>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/irods/#intro"><button class="toctree-expand" title="Open/close menu"></button>Introduction</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/irods/#table1">Table 1. iRODS Resource Names and Corresponding Filesystems</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/irods/#setup"><button class="toctree-expand" title="Open/close menu"></button>iRODS Setup</a>
        <ul>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/irods/#setup-cli"><button class="toctree-expand" title="Open/close menu"></button>Command-line Usage</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/irods/#example1">Example 1. iRODS Configuration Template</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/irods/#setup-gui">GUI Usage</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/irods/#icommands"><button class="toctree-expand" title="Open/close menu"></button>Basic i-Commands</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/irods/#table2">Table 2. Common i-Commands</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/irods/#icommands-irsync">irsync</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/irods/#icommands-irm">irm</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/irods/#icommands-ichmod">ichmod</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/irods/#installing">Installing the iRODS client</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/irods/#refs">References</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/lammps/"><button class="toctree-expand" title="Open/close menu"></button>LAMMPS</a>
    <ul>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/lammps/#installations">Installations</a>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/lammps/#running-batch"><button class="toctree-expand" title="Open/close menu"></button>Batch Mode</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/lammps/#running-batch-jobscript">Sample Job Script: LAMMPS on Stampede2</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/lammps/#running-batch-examples">Example command-line invocations:</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/lammps/#running-interactive">Running within idev</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/lammps/#refs">References</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/launcher/"><button class="toctree-expand" title="Open/close menu"></button>Launcher</a>
    <ul>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/launcher/#intro">Introduction</a>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/launcher/#serial"><button class="toctree-expand" title="Open/close menu"></button>Serial Jobs</a>
        <ul>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/launcher/#serial-jobscript"><button class="toctree-expand" title="Open/close menu"></button>Sample Job Script</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/launcher/#serial-jobfiles">Sample Job Files</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/launcher/#multithreaded"><button class="toctree-expand" title="Open/close menu"></button>Multithreaded Jobs</a>
        <ul>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/launcher/#multithreaded-jobscript"><button class="toctree-expand" title="Open/close menu"></button>Sample Job Script</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/launcher/#multithreaded-jobscript">Sample Job File</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/launcher/#launchergpu-jobfile">Sample Job File</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/launcher/#notes">Notes</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/launcher/#refs">References</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="" href="https://lmod.readthedocs.io/en/latest" target="_blank"><i class="fa fa-external-link"></i>Lmod</a>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/matlab/"><button class="toctree-expand" title="Open/close menu"></button>MATLAB</a>
    <ul>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/matlab/#license">Licenses</a>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/matlab/#interactive"><button class="toctree-expand" title="Open/close menu"></button>Interactive Mode</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/matlab/#figure1">Figure 1. MATLAB launched in a VNC session</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/matlab/#batch"><button class="toctree-expand" title="Open/close menu"></button>Batch Mode</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/matlab/#example-1-sample-matlab-job-script-to-run-on-stampede2">Example 1. Sample MATLAB job script to run on Stampede2</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/matlab/#parallelmatlab"><button class="toctree-expand" title="Open/close menu"></button>Parallel MATLAB</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/matlab/#example-2-matlab-parfor">Example 2. MATLAB parfor</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/matlab/#example-3-matlab-matlabpool">Example 3. MATLAB matlabpool</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/matlab/#toolbox">MATLAB Toolboxes</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/matlab/#refs">Mathworks References</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/matlab/#help">Help</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/namd/"><button class="toctree-expand" title="Open/close menu"></button>NAMD</a>
    <ul>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/namd/#installations">Installations</a>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/namd/#running-frontera"><button class="toctree-expand" title="Open/close menu"></button>NAMD on Frontera</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/namd/#running-frontera-jobscript">Job Script</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/namd/#running-stampede2"><button class="toctree-expand" title="Open/close menu"></button>NAMD on Stampede2</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/namd/#jobscript-stampede2-knl">Job Script</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/namd/#jobscript-stampede2-skx">Job Script: NAMD on Stampede2's SKX Nodes</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/namd/#running-lonestar6"><button class="toctree-expand" title="Open/close menu"></button>NAMD on Lonestar6</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/namd/#running-lonestar6-jobscript">Job Script</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/namd/#refs">References</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/openfoam/"><button class="toctree-expand" title="Open/close menu"></button>OpenFOAM</a>
    <ul>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/openfoam/#running"><button class="toctree-expand" title="Open/close menu"></button>Running on Frontera and Stampede2</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/openfoam/#running-compenv">Set Up Computing Environment</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/openfoam/#tutorials">Run Tutorials</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/openfoam/#refs">References</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/paraview-visit/"><button class="toctree-expand" title="Open/close menu"></button>Paraview VisIt</a>
    <ul>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/paraview-visit/#paraview"><button class="toctree-expand" title="Open/close menu"></button>ParaView</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/paraview-visit/#table-1-paraview-modules-per-tacc-resource">Table 1. ParaView Modules per TACC Resource</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/paraview-visit/#running-paraview-gui-on-a-compute-node-desktop">Running ParaView GUI On A Compute-Node Desktop</a>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/paraview-visit/#running-paraview-in-parallel"><button class="toctree-expand" title="Open/close menu"></button>Running ParaView In Parallel</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/paraview-visit/#notes-on-paraview-in-parallel">Notes on Paraview in Parallel</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/paraview-visit/#running-paraview-in-batch-mode"><button class="toctree-expand" title="Open/close menu"></button>Running ParaView In Batch Mode</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/paraview-visit/#notes-from-the-vis-team">Notes from the Vis Team</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/paraview-visit/#visit"><button class="toctree-expand" title="Open/close menu"></button>VisIt</a>
        <ul>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/paraview-visit/#table-3-running-visit"><button class="toctree-expand" title="Open/close menu"></button>Table 3. Running VisIt</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/paraview-visit/#notes">Notes:</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/paraview-visit/#preparing-data-for-parallel-visit">Preparing Data for Parallel Visit</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/paraview-visit/#help">Help</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/quantumespresso/"><button class="toctree-expand" title="Open/close menu"></button>Quantum Espresso</a>
    <ul>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/quantumespresso/#installations">Installations</a>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/quantumespresso/#running"><button class="toctree-expand" title="Open/close menu"></button>Running QE</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/quantumespresso/#jobscript-frontera">Sample Job Script: Frontera</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/quantumespresso/#jobscript-stampede2">Sample Job Script: Stampede2</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/quantumespresso/#refs">References</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/remora/"><button class="toctree-expand" title="Open/close menu"></button>REMORA</a>
    <ul>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/remora/#running"><button class="toctree-expand" title="Open/close menu"></button>Running Remora</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/remora/#running-interactively">Interactively</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/remora/#running-batch">Remora in Job Scripts</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/remora/#options">Remora Options</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/remora/#output">Remora Output</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/remora/#refs">References</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/tau/"><button class="toctree-expand" title="Open/close menu"></button>TAU</a>
    <ul>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/tau/#tacc"><button class="toctree-expand" title="Open/close menu"></button>Using TAU</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/tau/#using-instrument">1. Instrumenting your Code</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/tau/#launch">2. Running</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/tau/#process">3. Process program output</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/tau/#refs">References</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/tensorflow/"><button class="toctree-expand" title="Open/close menu"></button>Tensorflow</a>
    <ul>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/tensorflow/#installations">Installations</a>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/tensorflow/#lonestar6"><button class="toctree-expand" title="Open/close menu"></button>TensorFlow on Lonestar6</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/tensorflow/#lonestar6-singlenode">Single-Node</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/tensorflow/#lonestar6-multinode">Multi-Node</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/tensorflow/#maverick2"><button class="toctree-expand" title="Open/close menu"></button>TensorFlow on Maverick2</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/tensorflow/#maverick2-singlenode">Single-Node</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/tensorflow/#maverick2-multinode">Multi-Node</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/tensorflow/#frontera"><button class="toctree-expand" title="Open/close menu"></button>TensorFlow on Frontera</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/tensorflow/#frontera-singlenode">Single-Node</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/tensorflow/#frontera-multinode">Multi-Node</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/tensorflow/#stampede2"><button class="toctree-expand" title="Open/close menu"></button>TensorFlow on Stampede2</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/tensorflow/#stampede2-singlenode">Single-Node</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/tensorflow/#stampede2-multinode">Multi-Node</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/tensorflow/#faq">FAQ</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/tensorflow/#refs">References</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/vasp/"><button class="toctree-expand" title="Open/close menu"></button>VASP</a>
    <ul>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/vasp/#licenses"><button class="toctree-expand" title="Open/close menu"></button>Licenses</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/vasp/#installations">Installations</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/vasp/#running-interactive"><button class="toctree-expand" title="Open/close menu"></button>Running VASP</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/vasp/#jobscript-ls6">Sample Job Script: VASP on Lonestar6</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/vasp/#jobscript-frontera"><button class="toctree-expand" title="Open/close menu"></button>Sample Job Script: VASP on Frontera</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/vasp/#jobscript-stampede2">Sample Job Script: VASP on Stampede2</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/software/vasp/#refs">References</a>
    </li>
    </ul>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Tutorials</span></p>
              <ul>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/acls/"><button class="toctree-expand" title="Open/close menu"></button>Access Control Lists</a>
    <ul>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/acls/#viewing">Viewing ACLs</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/acls/#setting">Setting ACLs from the Command-Line</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/acls/#multiplefiles">Setting Complex ACLs</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/acls/#defaultacls">Default ACLs</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/acls/#refs">References</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/bashstartup/"><button class="toctree-expand" title="Open/close menu"></button>Bash Quick Start Guide</a>
    <ul>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/bashstartup/#troubleshooting">Troubleshooting</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/bashstartup/#refs">Reference</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/blas-lapack/"><button class="toctree-expand" title="Open/close menu"></button>BLAS/LAPACK</a>
    <ul>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/blas-lapack/#blas">Implementations</a>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/blas-lapack/#makefile"><button class="toctree-expand" title="Open/close menu"></button>Updating your makefile</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/blas-lapack/#mkl">MKL</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/blas-lapack/#blis">BLIS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/blas-lapack/#refs">Reference BLAS/LAPACK</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/blas-lapack/#goto">Goto Blas and OpenBlas</a>
    </li>
        </ul>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/ddt/"><button class="toctree-expand" title="Open/close menu"></button>DDT Debugger</a>
    <ul>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/ddt/#env">Set up Debugging Environment</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/ddt/#running">Running DDT</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/ddt/#reverse">DDT with Reverse Connect</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/ddt/#refs">References</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/gridcommunitytoolkit/"><button class="toctree-expand" title="Open/close menu"></button>Grid Community Toolkit</a>
    <ul>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/gridcommunitytoolkit/#step1">Step 1: Find a DN from a MyProxy or MyProxy OAuth provider</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/gridcommunitytoolkit/#step2">Step 2: Associate the DN with our TACC Account</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/gridcommunitytoolkit/#step3">Step 3: Activate Grid Community Toolkit Endpoints</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/gridcommunitytoolkit/#step4">Step 4: Transfer Files</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/managingio/"><button class="toctree-expand" title="Open/close menu"></button>Managing I/O</a>
    <ul>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/managingio/#io">What is I/O?</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/managingio/#files">Recommended File Systems Usage</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/managingio/#table1">Table 1. TACC File System Usage Recommendations</a>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/managingio/#bestpractices"><button class="toctree-expand" title="Open/close menu"></button>Best Practices for Minimizing I/O</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/managingio/#bestpractices-tmp">Use Each Compute Node's /tmp Storage Space</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/managingio/#table2"><button class="toctree-expand" title="Open/close menu"></button>Table 2. TACC Resources Compute Node (/tmp) Storage</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/managingio/#bestpractices-redirect-scratch">Run Jobs Out of Each Resource's Scratch File System</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/managingio/#bestpractices-perprocess">Avoid Writing One File Per Process</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/managingio/#bestpractices-contention">Avoid Repeated Reading/Writing to the Same File</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/managingio/#bestpractices-quotas">Monitor Your File System Quotas</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/managingio/#bestpractices-memory">Manipulate Data in Memory, not on Disk</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/managingio/#striping">Stripe Large Files on $SCRATCH and $WORK</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/managingio/#ooops"><button class="toctree-expand" title="Open/close menu"></button>Govern I/O with OOOPS</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/managingio/#ooops-functions">Functions</a>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/managingio/#ooops-howto"><button class="toctree-expand" title="Open/close menu"></button>How to Use OOOPS</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/managingio/#ooops-howto-singlenode">Example: Single-Node Job on $SCRATCH</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/managingio/#ooops-howto-multinode">Example: Multi-Node Job on $SCRATCH</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/managingio/#ooops-warnings">I/O Warning</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/managingio/#ooops-dev">Developers</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/managingio/#python">Python I/O Management</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/managingio/#tracking">Tracking Job I/O</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/managingio/#table3">I/O Do's and Don'ts</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/map/"><button class="toctree-expand" title="Open/close menu"></button>MAP</a>
    <ul>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/map/#env">Set up Profiling Environment</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/map/#running">Running MAP</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/map/#reverse">MAP with Reverse Connect</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/map/#refs">References</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/mfa/"><button class="toctree-expand" title="Open/close menu"></button>Multi-Factor Authentication</a>
    <ul>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/mfa/#whatismfa">What is Multi-Factor Authentication?</a>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/mfa/#setupmfa"><button class="toctree-expand" title="Open/close menu"></button>Setting up MFA at TACC</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/mfa/#setupmfa-step1">1. Manage Account</a>
    </li>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/mfa/#setupmfa-step2"><button class="toctree-expand" title="Open/close menu"></button>2. Select Pairing Method</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/mfa/#mfaapps">Authentication Apps</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/mfa/#table1">Table 1. MFA Apps</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/mfa/#sms">SMS (text) Messaging</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/mfa/#authapp">Example: Pairing with an Authentication App</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/mfa/#login">Logging into TACC Resources</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/mfa/#international">International Users and Travelers</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/mfa/#unpair">Unpairing your Device</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/remotedesktopaccess/"><button class="toctree-expand" title="Open/close menu"></button>Remote Desktop Access</a>
    <ul>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/remotedesktopaccess/#methods">Remote Desktop Methods</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/remotedesktopaccess/#tap">TACC Analysis Portal</a>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/remotedesktopaccess/#scripts"><button class="toctree-expand" title="Open/close menu"></button>DCV &amp; VNC at TACC</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/remotedesktopaccess/#table1">Table 1. Job Scripts</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/remotedesktopaccess/#dcv">Start a DCV Session</a>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/remotedesktopaccess/#vnc"><button class="toctree-expand" title="Open/close menu"></button>Start a VNC Session</a>
        <ul>
    <li class="toctree-l3 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/remotedesktopaccess/#vncsession"><button class="toctree-expand" title="Open/close menu"></button>Sample VNC session</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/remotedesktopaccess/#vncwindow1">Window 1</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/remotedesktopaccess/#vncwindow2">Window 2</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/remotedesktopaccess/#running">Running Apps on the Desktop</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/sharingprojectfiles/"><button class="toctree-expand" title="Open/close menu"></button>Sharing Project Files</a>
    <ul>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/sharingprojectfiles/#taccgroups">TACC, UNIX groups and Project Numbers</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/sharingprojectfiles/#displaypermissions">Display a File's Owner, Group and Permissions Information</a>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/sharingprojectfiles/#createworkspace"><button class="toctree-expand" title="Open/close menu"></button>Create a Shared Project Workspace</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/sharingprojectfiles/#createworkspace-read">Readable</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/sharingprojectfiles/#createworkspace-write">Writeable</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/sharingprojectfiles/#addusers">Adding users to TACC Projects</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/sharingprojectfiles/#refs">References</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/TAP/"><button class="toctree-expand" title="Open/close menu"></button>TACC Analysis Portal</a>
    <ul>
    <li class="toctree-l2"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/TAP/#access">Accessing the Portal</a>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/TAP/#jobmanagement"><button class="toctree-expand" title="Open/close menu"></button>Job Management</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/TAP/#jobmanagement-submit">Submitting a Job using TAP</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/TAP/#jobmanagement-end">Ending a Submitted Job</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/TAP/#jobmanagement-resubmit">Resubmitting a Past Job</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/TAP/#utilities"><button class="toctree-expand" title="Open/close menu"></button>Utilities</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/TAP/#utilities-status">Obtaining TACC Account Status</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/TAP/#utilities-fullscreen">Setting a Remote Desktop to Full Screen Mode</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2 has-list"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/TAP/#troubleshooting"><button class="toctree-expand" title="Open/close menu"></button>Troubleshooting</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/TAP/#troubleshooting-noallocation">No Allocation Available</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/TAP/#troubleshooting-pending">Job Submission returns PENDING</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="https://docs.tacc.utexas.edu/tutorials/TAP/#troubleshooting-error">Job Submission returns ERROR</a>
    </li>
        </ul>
    </li>
    </ul>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="https://docs.tacc.utexas.edu/">TACC HPC Documentation</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="https://docs.tacc.utexas.edu/" class="icon icon-home" alt="Docs"></a> »</li>
          <li>HPC »</li>
      <li>Stampede2</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="" itemprop="articleBody">
              
                <h1 id="stampede2-user-guide">Stampede2 User Guide</h1>
<p>Last update: September 16, 2022 
see <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#history">revision history</a></p>
<h2 id="notices"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#notices">Notices</a></h2>
<ul>
<li><strong>The XSEDE project concluded formal operations as an NSF-funded project on August 31, 2022</strong>.  Similar services are now operated through NSF's follow-on program, Advanced Cyberinfrastructure Coordination Ecosystem: Services &amp; Support, or ACCESS.  Find out more at the <a href="http://access-ci.org/" target="_blank">ACCESS website</a>. (09/01/2022)</li>
<li><strong>Stampede2 has deployed 240 Intel "Ice Lake" (ICX) compute nodes, replacing 448 KNL compute nodes.</strong>  Each ICX processor has 80 cores on 2 sockets (40 cores/socket). Hyperthreading is enabled: there are two hardware threads per core, for a total of 80 x 2 = 160 hardware threads per node. See <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#table2a">ICX Compute Node</a> specifications, new <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#scripts">ICX job scripts</a>, and the new <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#queues"><code>icx-normal</code> queue</a> for more information. (03/09/22)</li>
</ul>
<h2 id="intro"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#intro">Introduction</a></h2>
<p>Stampede2, generously funded by the National Science Foundation (NSF) through <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1540931" target="_blank">award  ACI-1540931</a>, is one of the Texas Advanced Computing Center (TACC), University of Texas at Austin's flagship supercomputers. Stampede2 entered full production in the Fall 2017 as an 18-petaflop national resource that builds on the successes of the original Stampede system it replaces. The first phase of the Stampede2 rollout featured the second generation of processors based on Intel's Many Integrated Core (MIC) architecture. Stampede2's 4,200 Knights Landing (KNL) nodes represent a radical break with the first-generation Knights Corner (KNC) MIC coprocessor. Unlike the legacy KNC, a Stampede2 KNL is not a coprocessor: each 68-core KNL is a stand-alone, self-booting processor that is the sole processor in its node. Phase 2 added to Stampede2 a total of 1,736 Intel Xeon Skylake (SKX) nodes. The final phase of Stampede2 features the replacement of 448 KNL nodes with 224 Ice Lake nodes. </p>
<h2 id="overview"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#overview">System Overview</a></h2>
<h3 id="overview-phase1computenodes"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#overview-phase1computenodes">KNL Compute Nodes</a></h3>
<p>Stampede2 hosts 4,200 KNL compute nodes, including 504 KNL nodes that were formerly configured as a Stampede1 sub-system.  </p>
<p>Each of Stampede2's KNL nodes includes 96GB of traditional DDR4 Random Access Memory (RAM). They also feature an additional 16GB of high bandwidth, on-package memory known as Multi-Channel Dynamic Random Access Memory (<strong>MCDRAM</strong>) that is up to four times faster than DDR4. The KNL's memory is configurable in two important ways: there are BIOS settings that determine at boot time the processor's <strong>memory mode</strong> and <strong>cluster mode</strong>. The processor's <strong>memory mode</strong> determines whether the fast MCDRAM operates as RAM, as direct-mapped L3 cache, or as a mixture of the two. The <strong>cluster mode</strong> determines the mechanisms for achieving cache coherency, which in turn determines latency: roughly speaking, this mode specifies the degree to which some memory addresses are "closer" to some cores than to others. See "<a href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-knl">Programming and Performance: KNL</a>" below for a top-level description of these and other available memory and cluster modes.</p>
<h4 id="table1"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#table1">Table 1. KNL Compute Node Specifications</a></h4>
<p>Table 1. Stampede2 KNL Compute Node Specifications.</p>
<table>
<thead>
<tr>
<th>Specification</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Model:</td>
<td>Intel Xeon Phi 7250 ("Knights Landing")</td>
</tr>
<tr>
<td>Total cores per KNL node:</td>
<td>68 cores on a single socket</td>
</tr>
<tr>
<td>Hardware threads per core:</td>
<td>4</td>
</tr>
<tr>
<td>Hardware threads per node:</td>
<td>68 x 4 = 272</td>
</tr>
<tr>
<td>Clock rate:</td>
<td>1.4GHz</td>
</tr>
<tr>
<td>RAM:</td>
<td>96GB DDR4 plus 16GB high-speed MCDRAM. Configurable in two important ways; see "<a href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-knl">Programming and Performance: KNL</a>" for more info.</td>
</tr>
<tr>
<td>Cache:</td>
<td>32KB L1 data cache per core; 1MB L2 per two-core tile. In default config, <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-knl-memorymodes">MCDRAM</a> operates as 16GB direct-mapped L3.</td>
</tr>
<tr>
<td>Local storage:</td>
<td>All but 504 KNL nodes have a 107GB <code>/tmp</code> partition on a 200GB Solid State Drive (SSD). The 504 KNLs originally installed as the Stampede1 KNL sub-system each have a 32GB <code>/tmp</code> partition on 112GB SSDs. The latter nodes currently make up the <code>development</code>, <code>long</code> and <span style="white-space: nowrap;"><code>flat-quadrant</code></span> <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-queues">queues</a>. Size of <code>/tmp</code> partitions as of 24 Apr 2018.</td>
</tr>
</tbody>
</table>
<h3 id="overview-skxcomputenodes"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#overview-skxcomputenodes">SKX Compute Nodes</a></h3>
<p>Stampede2 hosts 1,736 SKX compute nodes.</p>
<h4 id="table2"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#table2">Table 2. Stampede2 SKX Compute Node Specifications</a></h4>
<table>
<thead>
<tr>
<th>Specification</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Model:</td>
<td>Intel Xeon Platinum 8160 ("Skylake")</td>
</tr>
<tr>
<td>Total cores per SKX node:</td>
<td>48 cores on two sockets (24 cores/socket)</td>
</tr>
<tr>
<td>Hardware threads per core:</td>
<td>2</td>
</tr>
<tr>
<td>Hardware threads per node:</td>
<td>48 x 2 = 96</td>
</tr>
<tr>
<td>Clock rate:</td>
<td>2.1GHz nominal (1.4-3.7GHz depending on instruction set and number of active cores)</td>
</tr>
<tr>
<td>RAM:</td>
<td>192GB (2.67GHz) DDR4</td>
</tr>
<tr>
<td>Cache:</td>
<td>32KB L1 data cache per core; 1MB L2 per core; 33MB L3 per socket. Each socket can cache up to 57MB (sum of L2 and L3 capacity).</td>
</tr>
<tr>
<td>Local storage:</td>
<td>144GB <code>/tmp</code> partition on a 200GB SSD. Size of <code>/tmp</code> partition as of 14 Nov 2017.</td>
</tr>
</tbody>
</table>
<h3 id="overview-icxcomputenodes"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#overview-icxcomputenodes">ICX Compute Nodes</a></h3>
<p>Stampede2 hosts 224 ICX compute nodes.</p>
<h4 id="table2a"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#table2a">Table 2a. Stampede2 ICX Compute Node Specifications</a></h4>
<table>
<thead>
<tr>
<th>Specification</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Model:</td>
<td>Intel Xeon Platinum 8380 ("Ice Lake")</td>
</tr>
<tr>
<td>Total cores per ICX node:</td>
<td>80 cores on two sockets (40 cores/socket)</td>
</tr>
<tr>
<td>Hardware threads per core:</td>
<td>2</td>
</tr>
<tr>
<td>Hardware threads per node:</td>
<td>80 x 2 = 160</td>
</tr>
<tr>
<td>Clock rate:</td>
<td>2.3 GHz nominal (3.4GHz max frequency depending on instruction set and number of active cores)</td>
</tr>
<tr>
<td>RAM:</td>
<td>256GB (3.2 GHz) DDR4</td>
</tr>
<tr>
<td>Cache:</td>
<td>48KB L1 data cache per core; 1.25 MB L2 per core; 60 MB L3 per socket. Each socket can cache up to 110 MB (sum of L2 and L3 capacity)</td>
</tr>
<tr>
<td>Local storage:</td>
<td>342 GB <code>/tmp</code> partition</td>
</tr>
</tbody>
</table>
<h3 id="overview-loginnodes"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#overview-loginnodes">Login Nodes</a></h3>
<p>The Stampede2 login nodes, upgraded at the start of Phase 2, are Intel Xeon Gold 6132 (SKX) nodes, each with 28 cores on two sockets (14 cores/socket). They replace the decommissioned Broadwell login nodes used during Phase 1.</p>
<h3 id="overview-network"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#overview-network">Network</a></h3>
<p>The interconnect is a 100Gb/sec Intel Omni-Path (OPA) network with a fat tree topology employing six core switches. There is one leaf switch for each 28-node half rack, each with 20 leaf-to-core uplinks (28/20 oversubscription).</p>
<h3 id="overview-filesystems"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#overview-filesystems">File Systems Introduction</a></h3>
<p>Stampede2 mounts three shared Lustre file systems on which each user has corresponding account-specific directories <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#files-filesystems"><code>$HOME</code>, <code>$WORK</code>, and <code>$SCRATCH</code></a>. Each file system is available from all Stampede2 nodes; the <a href="https://www.tacc.utexas.edu/systems/stockyard" target="_blank">Stockyard-hosted work file system</a> is available on most other TACC HPC systems as well.  See <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#files-filesystems">Navigating the Shared File Systems</a> for detailed information as well as the <a href="https://docs.tacc.utexas.edu/hpc/stampede2/##conduct-filesystems">Good Conduct</a> file system guidelines. </p>
<h4 id="table3"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#table3">Table 3. Stampede2 File Systems</a></h4>
<table>
<thead>
<tr>
<th><u>File System</u></th>
<th>Quota</th>
<th>Key Features</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>$HOME</code></td>
<td>10GB, 200,000 files</td>
<td><b>Not intended for parallel or high-intensity file operations.</b><br>Backed up regularly.<br>Overall capacity ~1PB. Two Meta-Data Servers (MDS), four Object Storage Targets (OSTs).<br>Defaults: 1 stripe, 1MB stripe size.<br>Not purged.<br></td>
</tr>
<tr>
<td><code>$WORK</code></td>
<td>1TB, 3,000,000 files across all TACC systems,<br>regardless of where on the file system the files reside.</td>
<td><b>Not intended for high-intensity file operations or jobs involving very large files.</b><br>On the Global Shared File System that is mounted on most TACC systems.<br>See <a href="https://www.tacc.utexas.edu/systems/stockyard" target="_blank">Stockyard system description</a> for more information.<br>Defaults: 1 stripe, 1MB stripe size<br>Not backed up.<br>Not purged.<br></td>
</tr>
<tr>
<td><code>$SCRATCH</code></td>
<td>no quota</td>
<td>Overall capacity ~30PB. Four MDSs, 66 OSTs.<br>Defaults: 1 stripe, 1MB stripe size.<br>Not backed up.<br><b>Files are <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#scratchpolicy">subject to purge</a> if access time* is more than 10 days old</b>.</td>
</tr>
</tbody>
</table>
<h3 id="scratchpolicy"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#scratchpolicy">Scratch File System Purge Policy</a></h3>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The <code>$SCRATCH</code> file system, as its name indicates, is a temporary storage space.  Files that have not been accessed* in ten days are subject to purge.  Deliberately modifying file access time (using any method, tool, or program) for the purpose of circumventing purge policies is prohibited.</p>
</div>
<p>*The operating system updates a file's access time when that file is modified on a login or compute node or any time that file is read. Reading or executing a file/script will update the access time.  Use the <code>ls -ul</code> command to view access times.</p>
<h2 id="access"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#access">Accessing the System</a></h2>
<p>Access to all TACC systems now requires Multi-Factor Authentication (MFA). You can create an MFA pairing on the TACC User Portal. After login on the portal, go to your account profile (Home-&gt;Account Profile), then click the "Manage" button under "Multi-Factor Authentication" on the right side of the page. See <a href="https://docs.tacc.utexas.edu/tutorials/mfa">Multi-Factor Authentication at TACC</a> for further information. </p>
<h3 id="access-ssh"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#access-ssh">Secure Shell (SSH)</a></h3>
<p>The <code>ssh</code> command (SSH protocol) is the standard way to connect to Stampede2. SSH also includes support for the file transfer utilities <code>scp</code> and <code>sftp</code>. <a href="https://en.wikipedia.org/wiki/Secure_Shell" target="_blank">Wikipedia</a> is a good source of information on SSH. SSH is available within Linux and from the terminal app in the Mac OS. If you are using Windows, you will need an SSH client that supports the SSH-2 protocol: e.g. <a href="http://www.bitvise.com/" target="_blank">Bitvise</a>, <a href="http://www.openssh.com/" target="_blank">OpenSSH</a>, <a href="http://www.putty.org/" target="_blank">PuTTY</a>, or <a href="https://www.vandyke.com/products/securecrt/" target="_blank">SecureCRT</a>. Initiate a session using the <code>ssh</code> command or the equivalent; from the Linux command line the launch command looks like this:</p>
<pre class="highlight"><code class="language-cmd-line hljs">localhost<span class="hljs-variable">$ </span>ssh myusername<span class="hljs-variable">@stampede2</span>.tacc.utexas.edu</code></pre>
<p>The above command will rotate connections across all available login nodes and route your connection to one of them. To connect to a specific login node, use its full domain name:</p>
<pre class="highlight"><code class="language-cmd-line hljs">localhost<span class="hljs-variable">$ </span>ssh myusername<span class="hljs-variable">@login2</span>.stampede2.tacc.utexas.edu</code></pre>
<p>To connect with X11 support on Stampede2 (usually required for applications with graphical user interfaces), use the <code>-X</code> or <code>-Y</code> switch:</p>
<pre class="highlight"><code class="language-cmd-line hljs">localhost<span class="hljs-variable">$ </span>ssh -X myusername<span class="hljs-variable">@stampede2</span>.tacc.utexas.edu</code></pre>
<p>Use your TACC password for direct logins to TACC resources. You can change your TACC password through the <a href="https://tacc.utexas.edu/portal/login" title="TACC Portal login" target="_blank">TACC User Portal</a>. Log into the portal, then select "Change Password" under the "HOME" tab. If you've forgotten your password, go to the <a href="https://tacc.utexas.edu/portal/login" title="TACC Portal login" target="_blank">TACC User Portal</a> home page and select "Password Reset" under the Home tab.</p>
<p>To report a connection problem, execute the <code>ssh</code> command with the <code>-vvv</code> option and include the verbose output when submitting a help ticket.</p>
<p><strong>Do not run the <code>ssh-keygen</code> command on Stampede2.</strong> This command will create and configure a key pair that will interfere with the execution of job scripts in the batch system. If you do this by mistake, you can recover by renaming or deleting the <code>.ssh</code> directory located in your home directory; the system will automatically generate a new one for you when you next log into Stampede2.</p>
<ol>
<li>execute <code>mv .ssh dot.ssh.old</code> </li>
<li>log out</li>
<li>log into Stampede2 again</li>
</ol>
<p>After logging in again the system will generate a properly configured key pair.</p>
<h2 id="using"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#using">Using Stampede2</a></h2>
<p>Stampede2 nodes run Red Hat Enterprise Linux 7. Regardless of your research workflow, <strong>you’ll need to master Linux basics</strong> and a Linux-based text editor (e.g. <code>emacs</code>, <code>nano</code>, <code>gedit</code>, or <code>vi/vim</code>) to use the system properly. <!-- SDL This user guide does not address these topics, however. There are numerous resources in a variety of formats that are available to help you learn Linux, including some listed on the <a href="https://xortal.tacc.utexas.edu/training/course-materials">TACC</a> training sites. --> If you encounter a term or concept in this user guide that is new to you, a quick internet search should help you resolve the matter quickly.</p>
<h3 id="using-account"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#using-account">Configuring Your Account</a></h3>
<h4 id="using-account-shell"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#using-account-shell">Linux Shell</a></h4>
<p>The default login shell for your user account is Bash.  To determine your current login shell, execute: </p>
<pre class="highlight"><code class="language-cmd-line hljs"><span class="hljs-variable">$ </span>echo $<span class="hljs-variable constant_">SHELL</span></code></pre>
<p>If you'd like to change your login shell to <code>csh</code>, <code>sh</code>, <code>tcsh</code>, or <code>zsh</code>, submit a ticket through the <a href="https://tacc.utexas.edu/portal/login" title="TACC Portal login" target="_blank">TACC User Portal</a>. The <code>chsh</code> ("change shell") command will not work on TACC systems. </p>
<p>When you start a shell on Stampede2, system-level startup files initialize your account-level environment and aliases before the system sources your own user-level startup scripts. You can use these startup scripts to customize your shell by defining your own environment variables, aliases, and functions. These scripts (e.g. <code>.profile</code> and <code>.bashrc</code>) are generally hidden files: so-called dotfiles that begin with a period, visible when you execute: <code>ls -a</code>.</p>
<p>Before editing your startup files, however, it's worth taking the time to understand the basics of how your shell manages startup. Bash startup behavior is very different from the simpler <code>csh</code> behavior, for example. The Bash startup sequence varies depending on how you start the shell (e.g. using <code>ssh</code> to open a login shell, executing the <code>bash</code> command to begin an interactive shell, or launching a script to start a non-interactive shell). Moreover, Bash does not automatically source your <code>.bashrc</code> when you start a login shell by using  <code>ssh</code> to connect to a node. Unless you have specialized needs, however, this is undoubtedly more flexibility than you want: you will probably want your environment to be the same regardless of how you start the shell. The easiest way to achieve this is to execute <code>source ~/.bashrc</code> from your <code>.profile</code>, then put all your customizations in <code>.bashrc</code>.  The system-generated default startup scripts demonstrate this approach. We recommend that you use these default files as templates.</p>
<p>For more information see the <a href="https://docs.tacc.utexas.edu/tutorials/bashstartup" title="Bash Quick Start Guide">Bash Users' Startup Files: Quick Start Guide</a> and other online resources that explain shell startup. To recover the originals that appear in a newly created account, execute <code>/usr/local/startup_scripts/install_default_scripts</code>.</p>
<h4 id="using-account-envvars"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#using-account-envvars">Environment Variables</a></h4>
<p>Your environment includes the environment variables and functions defined in your current shell: those initialized by the system, those you define or modify in your account-level startup scripts, and those defined or modified by the <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#using-modules">modules</a> that you load to configure your software environment. Be sure to distinguish between an environment variable's name (e.g. <code>HISTSIZE</code>) and its value (<code>$HISTSIZE</code>). Understand as well that a sub-shell (e.g. a script) inherits environment variables from its parent, but does not inherit ordinary shell variables or aliases. Use <code>export</code> (in Bash) or <code>setenv</code> (in <code>csh</code>) to define an environment variable.</p>
<p>Execute the <code>env</code> command to see the environment variables that define the way your shell and child shells behave. </p>
<p>Pipe the results of <code>env</code> into <code>grep</code> to focus on specific environment variables. For example, to see all environment variables that contain the string GIT (in all caps), execute:</p>
<pre class="highlight"><code class="language-cmd-line hljs"><span class="hljs-variable">$ </span>env |<span class="hljs-params"> grep GIT</span></code></pre>
<p>The environment variables <code>PATH</code> and <code>LD_LIBRARY_PATH</code> are especially important. <code>PATH</code> is a colon-separated list of directory paths that determines where the system looks for your executables. <code>LD_LIBRARY_PATH</code> is a similar list that determines where the system looks for shared libraries.</p>
<h4 id="using-account-diagnostics"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#using-account-diagnostics">Account-Level Diagnostics</a></h4>
<p>TACC's <code>sanitytool</code> module loads an account-level diagnostic package that detects common account-level issues and often walks you through the fixes. You should certainly run the package's <code>sanitycheck</code> utility when you encounter unexpected behavior. You may also want to run <code>sanitycheck</code> periodically as preventive maintenance. To run <code>sanitytool</code>'s account-level diagnostics, execute the following commands:</p>
<pre class="highlight"><code class="language-cmd-line hljs">
login1<span class="hljs-variable">$ </span><span class="hljs-keyword">module</span> load sanitytool
login1<span class="hljs-variable">$ </span>sanitycheck</code></pre>
<p>Execute <code>module help sanitytool</code> for more information.</p>
<h3 id="using-computenodes"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#using-computenodes">Accessing the Compute Nodes</a></h3>
<p>You connect to Stampede2 through one of four "front-end" login nodes. The login nodes are shared resources: at any given time, there are many users logged into each of these login nodes, each preparing to access the "back-end" compute nodes (<a href="https://docs.tacc.utexas.edu/hpc/stampede2/#figure2">Figure 2. Login and Compute Nodes</a>). What you do on the login nodes affects other users directly because you are competing for the same memory and processing power. This is the reason you should not run your applications on the login nodes or otherwise abuse them. Think of the login nodes as a prep area where you can manage files and compile code before accessing the compute nodes to perform research computations. See <a href="https://docs.tacc.utexas.edu/basics/conduct/#conduct-loginnodes">Good Conduct</a> for more information. </p>
<p><strong>You can use your command-line prompt, or the <code>hostname</code> command, to tell you whether you are on a login node or a compute node</strong>. The default prompt, or any custom prompt containing <code>\h</code>, displays the short form of the hostname (e.g. <code>c401-064</code>). The hostname for a Stampede2 login node begins with the string <code>login</code> (e.g. <code>login2.stampede2.tacc.utexas.edu</code>), while compute node hostnames begin with the character <code>c</code> (e.g. <code>c401-064.stampede2.tacc.utexas.edu</code>). Note that the default prompts on the compute nodes include the node type (<code>knl</code>, <code>skx</code> or <code>icx</code>) as well. The environment variable <code>TACC_NODE_TYPE</code>, defined only on the compute nodes, also displays the node type. The simplified prompts in the User Guide examples are shorter than Stampede2's actual default prompts.</p>
<p>While some workflows, tools, and applications hide the details, there are three basic ways to access the compute nodes:</p>
<ol>
<li><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-sbatch">Submit a <strong>batch job</strong> using the <code>sbatch</code> command</a>. This directs the scheduler to run the job unattended when there are resources available. Until your batch job begins it will wait in a <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-queues">queue</a>. You do not need to remain connected while the job is waiting or executing. See <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running">Running Jobs</a> for more information. Note that the scheduler does not start jobs on a first come, first served basis; it juggles many variables to keep the machine busy while balancing the competing needs of all users. The best way to minimize wait time is to request only the resources you really need: the scheduler will have an easier time finding a slot for the two hours you need than for the 48 hours you unnecessarily request.</li>
<li>Begin an <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-idev"><strong>interactive session</strong> using <code>idev</code> or <code>srun</code></a>. This will log you into a compute node and give you a command prompt there, where you can issue commands and run code as if you were doing so on your personal machine. An interactive session is a great way to develop, test, and debug code. When you request an interactive session, the scheduler submits a job on your behalf. You will need to remain logged in until the interactive session begins.</li>
<li>Begin an <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-ssh">interactive session using <strong><code>ssh</code></strong></a> to connect to a compute node on which you are already running a job. This is a good way to open a second window into a node so that you can monitor a job while it runs.</li>
</ol>
<p>Be sure to request computing resources that are consistent with the type of application(s) you are running:</p>
<ul>
<li>A <strong>serial</strong> (non-parallel) application can only make use of a single core on a single node, and will only see that node's memory.</li>
<li>A threaded program (e.g. one that uses <strong>OpenMP</strong>) employs a shared memory programming model and is also restricted to a single node, but the program's individual threads can run on multiple cores on that node. </li>
<li>An <strong>MPI</strong> (Message Passing Interface) program can exploit the distributed computing power of multiple nodes: it launches multiple copies of its executable (MPI <strong>tasks</strong>, each assigned unique IDs called <strong>ranks</strong>) that can communicate with each other across the network. The tasks on a given node, however, can only directly access the memory on that node. Depending on the program's memory requirements, it may not be possible to run a task on every core of every node assigned to your job. If it appears that your MPI job is running out of memory, try  launching it with fewer tasks per node to increase the amount of memory available to individual tasks.</li>
<li>A popular type of <strong>parameter sweep</strong> (sometimes called <strong>high throughput computing</strong>) involves submitting a job that simultaneously runs many copies of one serial or threaded application, each with its own input parameters ("Single Program Multiple Data", or SPMD). The <code>launcher</code> tool is designed to make it easy to submit this type of job. For more information:</li>
</ul>
<pre class="highlight"><code class="language-cmd-line hljs">
<span class="hljs-variable">$ </span><span class="hljs-keyword">module</span> load launcher
<span class="hljs-variable">$ </span><span class="hljs-keyword">module</span> help launcher</code></pre>
<figure id="figure2">
<img alt="Stampede2" src="./Stampede2 - TACC HPC Documentation_files/Stampede2.jpg">
<figcaption>Figure 2. Login and compute nodes</figcaption></figure>

<h3 id="using-modules"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#using-modules">Using Modules to Manage your Environment</a></h3>
<p>Lmod, a module system developed and maintained at TACC, makes it easy to manage your environment so you have access to the software packages and versions that you need to using your research. This is especially important on a system like Stampede2 that serves thousands of users with an enormous range of needs. Loading a module amounts to choosing a specific package from among available alternatives:</p>
<pre class="highlight"><code class="language-cmd-line hljs">
<span class="hljs-variable">$ </span><span class="hljs-keyword">module</span> load intel          <span class="hljs-comment"># load the default Intel compiler</span>
<span class="hljs-variable">$ </span><span class="hljs-keyword">module</span> load intel/<span class="hljs-number">17.0</span>.<span class="hljs-number">4</span>   <span class="hljs-comment"># load a specific version of Intel compiler</span></code></pre>
<p>A module does its job by defining or modifying environment variables (and sometimes aliases and functions). For example, a module may prepend appropriate paths to <code>$PATH</code> and <code>$LD_LIBRARY_PATH</code> so that the system can find the executables and libraries associated with a given software package. The module creates the illusion that the system is installing software for your personal use. Unloading a module reverses these changes and creates the illusion that the system just uninstalled the software:</p>
<pre class="highlight"><code class="language-cmd-line hljs">
<span class="hljs-variable">$ </span><span class="hljs-keyword">module</span> load   ddt  <span class="hljs-comment"># defines DDT-related env vars; modifies others</span>
<span class="hljs-variable">$ </span><span class="hljs-keyword">module</span> unload ddt  <span class="hljs-comment"># undoes changes made by load</span></code></pre>
<p>The module system does more, however. When you load a given module, the module system can automatically replace or deactivate modules to ensure the packages you have loaded are compatible with each other. In the example below, the module system automatically unloads one compiler when you load another, and replaces Intel-compatible versions of IMPI and PETSc with versions compatible with gcc:</p>
<pre class="highlight"><code class="language-cmd-line hljs"><span class="hljs-variable">$ </span><span class="hljs-keyword">module</span> load intel  <span class="hljs-comment"># load default version of Intel compiler</span>
<span class="hljs-variable">$ </span><span class="hljs-keyword">module</span> load petsc  <span class="hljs-comment"># load default version of PETSc</span>
<span class="hljs-variable">$ </span><span class="hljs-keyword">module</span> load gcc    <span class="hljs-comment"># change compiler</span>

<span class="hljs-title class_">Lmod</span> is automatically replacing <span class="hljs-string">"intel/17.0.4"</span> with <span class="hljs-string">"gcc/7.1.0"</span>.

<span class="hljs-title class_">Due</span> to <span class="hljs-variable constant_">MODULEPATH</span> changes, the following have been <span class="hljs-symbol">reloaded:</span>
<span class="hljs-number">1</span>) impi/<span class="hljs-number">17.0</span>.<span class="hljs-number">3</span>     <span class="hljs-number">2</span>) petsc/<span class="hljs-number">3.7</span></code></pre>
<p>On Stampede2, modules generally adhere to a TACC naming convention when defining environment variables that are helpful for building and running software. For example, the <code>papi</code> module defines <code>TACC_PAPI_BIN</code> (the path to PAPI executables), <code>TACC_PAPI_LIB</code> (the path to PAPI libraries), <code>TACC_PAPI_INC</code> (the path to PAPI include files), and <code>TACC_PAPI_DIR</code> (top-level PAPI directory). After loading a module, here are some easy ways to observe its effects:</p>
<pre class="highlight"><code class="language-cmd-line hljs"><span class="hljs-variable">$ </span><span class="hljs-keyword">module</span> show papi   <span class="hljs-comment"># see what this module does to your environment</span>
<span class="hljs-variable">$ </span>env |<span class="hljs-params"> grep PAPI    # see env vars that contain the string PAPI
$ env </span>| grep -i papi <span class="hljs-comment"># case-insensitive search for 'papi' in environment</span></code></pre>
<p>To see the modules you currently have loaded:</p>
<pre class="highlight"><code class="language-cmd-line hljs"><span class="hljs-variable">$ </span><span class="hljs-keyword">module</span> list</code></pre>
<p>To see all modules that you can load right now because they are compatible with the currently loaded modules:</p>
<pre class="highlight"><code class="language-cmd-line hljs"><span class="hljs-variable">$ </span><span class="hljs-keyword">module</span> avail</code></pre>
<p>To see all installed modules, even if they are not currently available because they are incompatible with your currently loaded modules:</p>
<pre class="highlight"><code class="language-cmd-line hljs"><span class="hljs-variable">$ </span><span class="hljs-keyword">module</span> spider   <span class="hljs-comment"># list all modules, even those not available to load</span></code></pre>
<p>To filter your search:</p>
<pre class="highlight"><code class="language-cmd-line hljs"><span class="hljs-variable">$ </span><span class="hljs-keyword">module</span> spider slep             <span class="hljs-comment"># all modules with names containing 'slep'</span>
<span class="hljs-variable">$ </span><span class="hljs-keyword">module</span> spider sundials/<span class="hljs-number">2.5</span>.<span class="hljs-number">0</span>   <span class="hljs-comment"># additional details on a specific module</span></code></pre>
<p>Among other things, the latter command will tell you which modules you need to load before the module is available to load. You might also search for modules that are tagged with a keyword related to your needs (though your success here depends on the diligence of the module writers). For example:</p>
<pre class="highlight"><code class="language-cmd-line hljs"><span class="hljs-variable">$ </span><span class="hljs-keyword">module</span> keyword performance</code></pre>
<p>You can save a collection of modules as a personal default collection that will load every time you log into Stampede2. To do so, load the modules you want in your collection, then execute:</p>
<pre class="highlight"><code class="language-cmd-line hljs"><span class="hljs-variable">$ </span><span class="hljs-keyword">module</span> save    <span class="hljs-comment"># save the currently loaded collection of modules </span></code></pre>
<p>Two commands make it easy to return to a known, reproducible state:</p>
<pre class="highlight"><code class="language-cmd-line hljs"><span class="hljs-variable">$ </span><span class="hljs-keyword">module</span> reset   <span class="hljs-comment"># load the system default collection of modules</span>
<span class="hljs-variable">$ </span><span class="hljs-keyword">module</span> restore <span class="hljs-comment"># load your personal default collection of modules</span></code></pre>
<p>On TACC systems, the command <code>module reset</code> is equivalent to <code>module purge; module load TACC</code>. It's a safer, easier way to get to a known baseline state than issuing the two commands separately.</p>
<p>Help text is available for both individual modules and the module system itself:</p>
<pre class="highlight"><code class="language-cmd-line hljs"><span class="hljs-variable">$ </span><span class="hljs-keyword">module</span> help swr     <span class="hljs-comment"># show help text for software package swr</span>
<span class="hljs-variable">$ </span><span class="hljs-keyword">module</span> help         <span class="hljs-comment"># show help text for the module system itself</span></code></pre>
<p>See <a href="http://lmod.readthedocs.org/" target="_blank">Lmod's online documentation</a> for more extensive documentation. The online documentation addresses the basics in more detail, but also covers several topics beyond the scope of the help text (e.g. writing and using your own module files).</p>
<p>It's safe to execute module commands in job scripts. In fact, this is a good way to write self-documenting, portable job scripts that produce reproducible results. If you use <code>module save</code> to define a personal default module collection, it's rarely necessary to execute module commands in shell startup scripts, and it can be tricky to do so safely. If you do wish to put module commands in your startup scripts, see Stampede2's default startup scripts for a safe way to do so.</p>
<h2 id="files"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#files">Managing Your Files</a></h2>
<p>Stampede2 mounts three file Lustre file systems that are shared across all nodes: the home, work, and scratch file systems. Stampede2's startup mechanisms define corresponding account-level environment variables <code>$HOME</code>, <code>$SCRATCH</code>, and <code>$WORK</code> that store the paths to directories that you own on each of these file systems. Consult the <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#table3">Stampede2 File Systems</a> table for the basic characteristics of these file systems, <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-fileio">File Operations: I/O Performance</a> for advice on performance issues, and <a href="https://docs.tacc.utexas.edu/basics/conduct">Good Conduct</a> for tips on file system etiquette.</p>
<h3 id="files-filesystems"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#files-filesystems">Navigating the Shared File Systems</a></h3>
<p>Stampede2's <code>/home</code> and <code>/scratch</code> file systems are mounted only on Stampede2, but the work file system mounted on Stampede2 is the Global Shared File System hosted on <a href="https://www.tacc.utexas.edu/systems/stockyard" target="_blank">Stockyard</a>. Stockyard is the same work file system that is currently available on Frontera, Lonestar6, and several other TACC resources. </p>
<p>The <code>$STOCKYARD</code> environment variable points to the highest-level directory that you own on the Global Shared File System. The definition of the <code>$STOCKYARD</code> environment variable is of course account-specific, but you will see the same value on all TACC systems that provide access to the Global Shared File System. This directory is an excellent place to store files you want to access regularly from multiple TACC resources.</p>
<p>Your account-specific <code>$WORK</code> environment variable varies from system to system and is a sub-directory of <code>$STOCKYARD</code> (<a href="https://docs.tacc.utexas.edu/hpc/stampede2/#figure3">Figure 3</a>). The sub-directory name corresponds to the associated TACC resource. The <code>$WORK</code> environment variable on Stampede2 points to the <code>$STOCKYARD/stampede2</code> subdirectory, a convenient location for files you use and jobs you run on Stampede2. Remember, however, that all subdirectories contained in your <code>$STOCKYARD</code> directory are available to you from any system that mounts the file system. If you have accounts on both Stampede2 and Frontera, for example, the <code>$STOCKYARD/stampede2</code> directory is available from your Frontera account, and <code>$STOCKYARD/frontera</code> is available from your Stampede2 account. </p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Your quota and reported usage on the Global Shared File System reflects all files that you own on Stockyard, regardless of their actual location on the file system.</p>
</div>
<p>See the example for fictitious user <code>bjones</code> in the figure below. All directories are accessible from all systems, however a given sub-directory (e.g. <code>lonestar6</code>, <code>frontera</code>) will exist <strong>only</strong> if you have an allocation on that system.  Figure 3 illustrates account-level directories on the <code>$WORK</code> file system (Global Shared File System hosted on Stockyard). Example for fictitious user <code>bjones</code>. All directories usable from all systems. Sub-directories (e.g. <code>lonestar6</code>, <code>frontera</code>) exist only when you have allocations on the associated system.</p>
<figure id="figure3"><img border="1" alt="Stockyard 2022" src="./Stampede2 - TACC HPC Documentation_files/stockyard-2022.jpg" <figcaption="">Figure 3.</figure>

<p>Note that resource-specific sub-directories of <code>$STOCKYARD</code> are nothing more than convenient ways to manage your <u>resource-specific</u> files. You have access to any such <u>sub-directory</u> from any TACC resources. If you are logged into Stampede2, for example, executing the alias <code>cdw</code> (equivalent to <code>cd $WORK</code>) will take you to the <u>resource-specific</u> <u>sub-directory</u> <code>$STOCKYARD/stampede2</code>. But you can access this directory from other TACC systems as well by executing <code>cd $STOCKYARD/stampede2</code>. These commands allow you to share files across TACC systems. In fact, several convenient <u>account-level</u> aliases make it even easier to navigate across the directories you own in the shared file systems:</p>
<p><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#table4">Table 4. Built-in Account Level Aliases</a> { #table4 }</p>
<table>
<thead>
<tr>
<th>Alias</th>
<th>Command</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>cd</code> or <code>cdh</code></td>
<td><code>cd $HOME</code></td>
</tr>
<tr>
<td><code>cdw</code></td>
<td><code>cd $WORK</code></td>
</tr>
<tr>
<td><code>cds</code></td>
<td><code>cd $SCRATCH</code></td>
</tr>
<tr>
<td><code>cdy</code> or <code>cdg</code></td>
<td><code>cd $STOCKYARD</code></td>
</tr>
</tbody>
</table>
<h3 id="files-striping"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#files-striping">Striping Large Files</a></h3>
<p>Stampede2's Lustre file systems look and act like a single logical hard disk, but are actually sophisticated integrated systems involving many physical drives (dozens of physical drives for <code>$HOME</code>, hundreds for <code>$WORK</code> and <code>$SCRATCH</code>).</p>
<p>Lustre can <strong>stripe</strong> (distribute) large files over several physical disks, making it possible to deliver the high performance needed to service input/output (I/O) requests from hundreds of users across thousands of nodes. Object Storage Targets (OSTs) manage the file system's spinning disks: a file with 16 stripes, for example, is distributed across 16 OSTs. One designated Meta-Data Server (MDS) tracks the OSTs assigned to a file, as well as the file's descriptive data.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Before transferring to, or creating large files on Stampede2, be sure to set an appropriate default stripe count on the receiving directory.</p>
</div>
<p>To avoid exceeding your fair share of any given OST, a good rule of thumb is to allow at least one stripe for each 100GB in the file. For example, to set the default stripe count on the current directory to 30 (a plausible stripe count for a directory receiving a file approaching 3TB in size), execute:</p>
<pre class="highlight"><code class="language-cmd-line hljs"><span class="hljs-variable">$ </span>lfs setstripe -c <span class="hljs-number">30</span> $<span class="hljs-variable constant_">PWD</span></code></pre>
<p>Note that an <code>lfs setstripe</code> command always sets both stripe count and stripe size, even if you explicitly specify only one or the other. Since the example above does not explicitly specify stripe size, the command will set the stripe size on the directory to Stampede2's system default (1MB). In general there's no need to customize stripe size when creating or transferring files.</p>
<p>Remember that it's not possible to change the striping on a file that already exists. Moreover, the <code>mv</code> command has no effect on a file's striping if the source and destination directories are on the same file system. You can, of course, use the <code>cp</code> command to create a second copy with different striping; to do so, copy the file to a directory with the intended stripe parameters.</p>
<p>You can check the stripe count of a file using the <code>lfs getstripe</code> command:</p>
<pre class="highlight"><code class="language-cmd-line hljs"><span class="hljs-variable">$ </span>lfs getstripe myfile</code></pre>
<h2 id="transferring"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#transferring">Transferring Files</a></h2>
<h3 id="transferring-scp"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#transferring-scp">with <code>scp</code></a></h3>
<p>You can transfer files between Stampede2 and Linux-based systems using either <a href="http://linux.com/learn/intro-to-linux/2017/2/how-securely-transfer-files-between-servers-scp" target="_blank"><code>scp</code></a> or <a href="http://linux.com/learn/get-know-rsync" target="_blank"><code>rsync</code></a>. Both <code>scp</code> and <code>rsync</code> are available in the Mac Terminal app. Windows <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#access-ssh">ssh clients</a> typically include <code>scp</code>-based file transfer capabilities.</p>
<p>The Linux <code>scp</code> (secure copy) utility is a component of the OpenSSH suite. Assuming your Stampede2 username is <code>bjones</code>, a simple <code>scp</code> transfer that pushes a file named <code>myfile</code> from your local Linux system to Stampede2 <code>$HOME</code> would look like this:</p>
<pre class="highlight"><code class="language-cmd-line hljs">localhost<span class="hljs-variable">$ </span>scp ./myfile bjones<span class="hljs-variable">@stampede2</span>.tacc.utexas.<span class="hljs-symbol">edu:</span>  <span class="hljs-comment"># note colon after net address</span></code></pre>
<p>You can use wildcards, but you need to be careful about when and where you want wildcard expansion to occur. For example, to push all files ending in <code>.txt</code> from the current directory on your local machine to <code>/work/01234/bjones/scripts</code> on Stampede2:</p>
<pre class="highlight"><code class="language-cmd-line hljs">localhost<span class="hljs-variable">$ </span>scp *.txt bjones<span class="hljs-variable">@stampede2</span>.tacc.utexas.<span class="hljs-symbol">edu:</span>/work/<span class="hljs-number">01234</span>/bjones/stampede2</code></pre>
<p>To delay wildcard expansion until reaching Stampede2, use a backslash (<code>\</code>) as an escape character before the wildcard. For example, to pull all files ending in <code>.txt</code> from <code>/work/01234/bjones/scripts</code> on Stampede2 to the current directory on your local system:</p>
<pre class="highlight"><code class="language-cmd-line hljs">localhost<span class="hljs-variable">$ </span>scp bjones<span class="hljs-variable">@stampede2</span>.tacc.utexas.<span class="hljs-symbol">edu:</span>/work/<span class="hljs-number">01234</span>/bjones/stampede2/\*.txt .</code></pre>
<p>You can of course use shell or environment variables in your calls to <code>scp</code>. For example:</p>
<pre class="highlight"><code class="language-cmd-line hljs">localhost<span class="hljs-variable">$ </span>destdir=<span class="hljs-string">"/work/01234/bjones/stampede2/data"</span>
localhost<span class="hljs-variable">$ </span>scp ./myfile bjones<span class="hljs-variable">@stampede2</span>.tacc.utexas.<span class="hljs-symbol">edu:</span>$destdir</code></pre>
<p>You can also issue <code>scp</code> commands on your local client that use Stampede2 environment variables like <code>$HOME</code>, <code>$WORK</code>, and <code>$SCRATCH</code>. To do so, use a backslash (<code>\</code>) as an escape character before the <code>$</code>; this ensures that expansion occurs after establishing the connection to Stampede2:</p>
<pre class="highlight"><code class="language-cmd-line hljs">localhost<span class="hljs-variable">$ </span>scp ./myfile bjones<span class="hljs-variable">@stampede2</span>.tacc.utexas.<span class="hljs-symbol">edu:</span>\<span class="hljs-variable">$WORK</span>/data   <span class="hljs-comment"># Note backslash</span></code></pre>
<p>Avoid using <code>scp</code> for recursive (<code>-r</code>) transfers of directories that contain nested directories of many small files:</p>
<pre class="highlight"><code class="language-cmd-line hljs">localhost<span class="hljs-variable">$ </span>&lt;s&gt;scp -r  ./mydata     bjones<span class="hljs-variable">@stampede2</span>.tacc.utexas.<span class="hljs-symbol">edu:</span>\<span class="hljs-variable">$WORK</span>  <span class="hljs-comment"># DON'T DO THIS</span></code></pre>
<p>Instead, use <code>tar</code> to create an archive of the directory, then transfer the directory as a single file:</p>
<pre class="highlight"><code class="language-cmd-line hljs">localhost<span class="hljs-variable">$ </span>tar cvf ./mydata.tar mydata&lt;<span class="hljs-regexp">/b&gt;                                   # create archive
localhost$ scp     ./mydata</span>.tar bjones<span class="hljs-variable">@stampede2</span>.tacc.utexas.<span class="hljs-symbol">edu:</span>\<span class="hljs-variable">$WORK</span>  <span class="hljs-comment"># transfer archive</span></code></pre>
<h3 id="transferring-rsync"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#transferring-rsync">with <code>rsync</code></a></h3>
<p>The <code>rsync</code> (remote synchronization) utility is a great way to synchronize files that you maintain on more than one system: when you transfer files using <code>rsync</code>, the utility copies only the changed portions of individual files. As a result, <code>rsync</code> is especially efficient when you only need to update a small fraction of a large dataset. The basic syntax is similar to <code>scp</code>:</p>
<pre class="highlight"><code class="language-cmd-line hljs">localhost<span class="hljs-variable">$ </span>rsync       mybigfile bjones<span class="hljs-variable">@stampede2</span>.tacc.utexas.<span class="hljs-symbol">edu:</span>\<span class="hljs-variable">$WORK</span>/data
localhost<span class="hljs-variable">$ </span>rsync -avtr mybigdir  bjones<span class="hljs-variable">@stampede2</span>.tacc.utexas.<span class="hljs-symbol">edu:</span>\<span class="hljs-variable">$WORK</span>/data</code></pre>
<p>The options on the second transfer are typical and appropriate when synching a directory: this is a recursive update (<code>-r</code>) with verbose (<code>-v</code>) feedback; the synchronization preserves time stamps (<code>-t</code>) as well as symbolic links and other meta-data (<code>-a</code>). Because <code>rsync</code> only transfers changes, recursive updates with <code>rsync</code> may be less demanding than an equivalent recursive transfer with <code>scp</code>.</p>
<p>See <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#files-striping">Striping Large Files</a> for additional important advice about striping the receiving directory when transferring or creating large files on TACC systems. </p>
<p>As detailed in <a href="https://docs.tacc.utexas.edu/basics/conduct">Good Conduct</a>, it is important to monitor your quotas on the <code>$HOME</code> and <code>$WORK</code> file systems, and limit the number of simultaneous transfers. Remember also that <code>$STOCKYARD</code> (and your <code>$WORK</code> directory on each TACC resource) is available from several other TACC systems: there's no need for <code>scp</code> when both the source and destination involve sub-directories of <code>$STOCKYARD</code>. See <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#files">Managing Your Files</a> for more information about transfers on <code>$STOCKYARD</code>.</p>
<h3 id="transferring-gct"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#transferring-gct">with  Grid Community Toolkit</a></h3>
<p>The Grid Community Toolkit (GCT) is an open-source fork of the <a href="http://toolkit.globus.org/toolkit" target="_blank">Globus Toolkit</a> and was created in response to the <a href="https://github.com/globus/globus-toolkit/blob/globus_6_branch/support-changes.md" target="_blank">end-of-support</a> of the Globus Toolkit in January 2018.</p>
<p>Stampede2 has two endpoints, one running Globus gridftp v5.4 software available for <a href="http://access-ci.org/" target="_blank">ACCESS</a> (formerly XSEDE) users, and the endpoint running Grid Community Toolkit with CILogon authentication available to all.  </p>
<h3 id="files-sharing"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#files-sharing">Sharing Files with Collaborators</a></h3>
<p>If you wish to share files and data with collaborators in your project, see <a href="https://docs.tacc.utexas.edu/tutorials/sharingprojectfiles" title="Sharing Project Files">Sharing Project Files on TACC Systems</a> for step-by-step instructions. Project managers or delegates can use Unix group permissions and commands to create read-only or read-write shared workspaces that function as data repositories and provide a common work area to all project members.</p>
<h2 id="building"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#building">Building Software</a></h2>
<p>The phrase "building software" is a common way to describe the process of producing a machine-readable executable file from source files written in C, Fortran, or some other programming language. In its simplest form, building software involves a simple, one-line call or short shell script that invokes a compiler. More typically, the process leverages the power of <a href="http://www.gnu.org/software/make/manual/make.html" target="_blank"><code>makefiles</code></a>, so you can change a line or two in the source code, then rebuild in a systematic way only the components affected by the change. Increasingly, however, the build process is a sophisticated multi-step automated workflow managed by a special framework like <a href="http://www.gnu.org/software/automake/manual/html_node/Autotools-Introduction.html" target="_blank">autotools</a> or <a href="http://cmake.org/" target="_blank"><code>cmake</code></a>, intended to achieve a repeatable, maintainable, portable mechanism for installing software across a wide range of target platforms.</p>
<h3 id="building-basics"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#building-basics">Basics of Building Software</a></h3>
<p>This section of the user guide does nothing more than introduce the big ideas with simple one-line examples. You will undoubtedly want to explore these concepts more deeply using online resources. You will quickly outgrow the examples here. We recommend that you master the basics of makefiles as quickly as possible: even the simplest computational research project will benefit enormously from the power and flexibility of a makefile-based build process.</p>
<h4 id="building-basics-intel"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#building-basics-intel">Intel Compilers</a></h4>
<p>Intel is the recommended and default compiler suite on Stampede2. Each Intel module also gives you direct access to <code>mkl</code> without loading an <code>mkl</code> module; see <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#intel-math-kernel-library-mkl">Intel MKL</a> for more information. Here are simple examples that use the Intel compiler to build an executable from source code:</p>
<pre class="highlight"><code class="language-cmd-line hljs"><span class="hljs-variable">$ </span>icc mycode.c                    <span class="hljs-comment"># C source file; executable a.out</span>
<span class="hljs-variable">$ </span>icc main.c calc.c analyze.c     <span class="hljs-comment"># multiple source files</span>
<span class="hljs-variable">$ </span>icc mycode.c     -o myexe       <span class="hljs-comment"># C source file; executable myexe</span>
<span class="hljs-variable">$ </span>icpc mycode.cpp  -o myexe       <span class="hljs-comment"># C++ source file</span>
<span class="hljs-variable">$ </span>ifort mycode.f90 -o myexe       <span class="hljs-comment"># Fortran90 source file</span></code></pre>
<p>Compiling a code that uses OpenMP would look like this:</p>
<pre class="highlight"><code class="language-cmd-line hljs"><span class="hljs-variable">$ </span>icc -qopenmp mycode.c -o myexe  <span class="hljs-comment"># OpenMP</span></code></pre>
<p>See the published Intel documentation, available both <a href="http://software.intel.com/en-us/intel-software-technical-documentation" target="_blank">online</a> and in <code>${TACC_INTEL_DIR}/documentation</code>, for information on optimization flags and other Intel compiler options.</p>
<h4 id="building-basics-gnu"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#building-basics-gnu">GNU Compilers</a></h4>
<p>The GNU foundation maintains a number of high quality compilers, including a compiler for C (<code>gcc</code>), C++ (<code>g++</code>), and Fortran (<code>gfortran</code>). The <code>gcc</code> compiler is the foundation underneath all three, and the term "gcc" often means the suite of these three GNU compilers.</p>
<p>Load a <code>gcc</code> module to access a recent version of the GNU compiler suite. Avoid using the GNU compilers that are available without a <code>gcc</code> module — those will be older versions based on the "system gcc" that comes as part of the Linux distribution.</p>
<p>Here are simple examples that use the GNU compilers to produce an executable from source code:</p>
<pre class="highlight"><code class="language-cmd-line hljs"><span class="hljs-variable">$ </span>gcc mycode.c                    <span class="hljs-comment"># C source file; executable a.out</span>
<span class="hljs-variable">$ </span>gcc mycode.c          -o myexe  <span class="hljs-comment"># C source file; executable myexe</span>
<span class="hljs-variable">$ </span>g++ mycode.cpp        -o myexe  <span class="hljs-comment"># C++ source file</span>
<span class="hljs-variable">$ </span>gfortran mycode.f90   -o myexe  <span class="hljs-comment"># Fortran90 source file</span>
<span class="hljs-variable">$ </span>gcc -fopenmp mycode.c -o myexe  <span class="hljs-comment"># OpenMP; GNU flag is different than Intel</span></code></pre>
<p>Note that some compiler options are the same for both Intel and GNU (e.g. <code>-o</code>), while others are different (e.g. <code>-qopenmp</code> vs <code>-fopenmp</code>). Many options are available in one compiler suite but not the other. See the <a href="http://gcc.gnu.org/onlinedocs/" target="_blank">online GNU documentation</a> for information on optimization flags and other GNU compiler options.</p>
<h4 id="building-basics-complink"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#building-basics-complink">Compiling and Linking as Separate Steps</a></h4>
<p>Building an executable requires two separate steps: (1) compiling (generating a binary object file associated with each source file); and (2) linking (combining those object files into a single executable file that also specifies the libraries that executable needs). The examples in the previous section accomplish these two steps in a single call to the compiler. When building more sophisticated applications or libraries, however, it is often necessary or helpful to accomplish these two steps separately.</p>
<p>Use the <code>-c</code> ("compile") flag to produce object files from source files:</p>
<pre class="highlight"><code class="language-cmd-line hljs"><span class="hljs-variable">$ </span>icc -c main.c calc.c results.c</code></pre>
<p>Barring errors, this command will produce object files <code>main.o</code>, <code>calc.o</code>, and <code>results.o</code>. Syntax for other compilers Intel and GNU compilers is similar.</p>
<p>You can now link the object files to produce an executable file:</p>
<pre class="highlight"><code class="language-cmd-line hljs"><span class="hljs-variable">$ </span>icc main.o calc.o results.o -o myexe</code></pre>
<p>The compiler calls a linker utility (usually <code>/bin/ld</code>) to accomplish this task. Again, syntax for other compilers is similar.</p>
<h4 id="building-basics-inclib"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#building-basics-inclib">Include and Library Paths</a></h4>
<p>Software often depends on pre-compiled binaries called libraries. When this is true, compiling usually requires using the <code>-I</code> option to specify paths to so-called header or include files that define interfaces to the procedures and data in those libraries. Similarly, linking often requires using the <code>-L</code> option to specify paths to the libraries themselves. Typical compile and link lines might look like this:</p>
<pre class="highlight"><code class="language-cmd-line hljs"><span class="hljs-variable">$ </span>icc        -c main.c -I<span class="hljs-variable">${</span><span class="hljs-variable constant_">WORK</span>}/mylib/inc -I<span class="hljs-variable">${</span><span class="hljs-variable constant_">TACC_HDF5_INC</span>}                  <span class="hljs-comment"># compile</span>
<span class="hljs-variable">$ </span>icc main.o -o myexe  -L<span class="hljs-variable">${</span><span class="hljs-variable constant_">WORK</span>}/mylib/lib -L<span class="hljs-variable">${</span><span class="hljs-variable constant_">TACC_HDF5_LIB</span>} -lmylib -lhdf5   <span class="hljs-comment"># link</span></code></pre>
<p>On Stampede2, both the <code>hdf5</code> and <code>phdf5</code> modules define the environment variables <code>$TACC_HDF5_INC</code> and <code>$TACC_HDF5_LIB</code>. Other module files define similar environment variables; see <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#using-modules">Using Modules to Manage Your Environment</a> for more information.</p>
<p>The details of the linking process vary, and order sometimes matters. Much depends on the type of library: static (<code>.a</code> suffix; library's binary code becomes part of executable image at link time) versus dynamically-linked shared (.so suffix; library's binary code is not part of executable; it's located and loaded into memory at run time). The link line can use rpath to store in the executable an explicit path to a shared library. In general, however, the <code>LD_LIBRARY_PATH</code> environment variable specifies the search path for dynamic libraries. For software installed at the system-level, TACC's modules generally modify <code>LD_LIBRARY_PATH</code> automatically. To see whether and how an executable named <code>myexe</code> resolves dependencies on dynamically linked libraries, execute <code>ldd myexe</code>.</p>
<p>A separate section below addresses the <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#intel-math-kernel-library-mkl">Intel Math Kernel Library</a> (MKL).</p>
<h4 id="building-basics-mpi"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#building-basics-mpi">Compiling and Linking MPI Programs</a></h4>
<p>Intel MPI (module <code>impi</code>) and MVAPICH2 (module <code>mvapich2</code>) are the two MPI libraries available on Stampede2. After loading an <code>impi</code> or <code>mvapich2</code> module, compile and/or link using an mpi wrapper (<code>mpicc</code>, <code>mpicxx</code>, <code>mpif90</code>) in place of the compiler:</p>
<pre class="highlight"><code class="language-cmd-line hljs"><span class="hljs-variable">$ </span>mpicc    mycode.c   -o myexe   <span class="hljs-comment"># C source, full build</span>
<span class="hljs-variable">$ </span>mpicc -c mycode.c              <span class="hljs-comment"># C source, compile without linking</span>
<span class="hljs-variable">$ </span>mpicxx   mycode.cpp -o myexe   <span class="hljs-comment"># C++ source, full build</span>
<span class="hljs-variable">$ </span>mpif90   mycode.f90 -o myexe   <span class="hljs-comment"># Fortran source, full build</span></code></pre>
<p>These wrappers call the compiler with the options, include paths, and libraries necessary to produce an MPI executable using the MPI module you're using. To see the effect of a given wrapper, call it with the <code>-show</code> option:</p>
<pre class="highlight"><code class="language-cmd-line hljs"><span class="hljs-variable">$ </span>mpicc -show  <span class="hljs-comment"># Show compile line generated by call to mpicc; similarly for other wrappers</span></code></pre>
<h4 id="building-basics-thirdparty"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#building-basics-thirdparty">Building Third-Party Software in Your Own Account</a></h4>
<p>You're welcome to download third-party research software and install it in your own account. In most cases you'll want to download the source code and build the software so it's compatible with the Stampede2 software environment. You can't use yum or any other installation process that requires elevated privileges, but this is almost never necessary. The key is to specify an installation directory for which you have write permissions. Details vary; you should consult the package's documentation and be prepared to experiment. When using the famous <a href="http://www.gnu.org/software/automake/manual/html_node/Autotools-Introduction.html" target="_blank">three-step autotools</a> build process, the standard approach is to use the <code>PREFIX</code> environment variable to specify a non-default, user-owned installation directory at the time you execute <code>configure</code> or <code>make</code>:</p>
<pre class="highlight"><code class="language-cmd-line hljs">
<span class="hljs-variable">$ </span>export <span class="hljs-variable constant_">INSTALLDIR</span>=<span class="hljs-variable">$WORK</span>/apps/t3pio
<span class="hljs-variable">$ </span>./configure --prefix=<span class="hljs-variable">$INSTALLDIR</span>
<span class="hljs-variable">$ </span>make
<span class="hljs-variable">$ </span>make install</code></pre>
<p>Other languages, frameworks, and build systems generally have equivalent mechanisms for installing software in user space. In most cases a web search like "Python Linux install local" will get you the information you need.</p>
<p>In Python, a local install will resemble one of the following examples:</p>
<pre class="highlight"><code class="language-cmd-line hljs">
<span class="hljs-variable">$ </span>pip install netCDF4     --user                    <span class="hljs-comment"># install netCDF4 package to $HOME/.local</span>
<span class="hljs-variable">$ </span>python setup.py install --user                    <span class="hljs-comment"># install to $HOME/.local</span>
<span class="hljs-variable">$ </span>pip install netCDF4     --prefix=<span class="hljs-variable">$INSTALLDIR</span>      <span class="hljs-comment"># custom location; add to PYTHONPATH</span></code></pre>
<p>Similarly in R:</p>
<pre class="highlight"><code class="language-cmd-line hljs"><span class="hljs-variable">$ </span><span class="hljs-keyword">module</span> load <span class="hljs-title class_">Rstats</span>            <span class="hljs-comment"># load TACC's default R</span>
<span class="hljs-variable">$ </span>R                             <span class="hljs-comment"># launch R</span>
&gt; install.packages(<span class="hljs-string">'devtools'</span>)  <span class="hljs-comment"># R will prompt for install location</span></code></pre>
<p>You may, of course, need to customize the build process in other ways. It's likely, for example, that you'll need to edit a <code>makefile</code> or other build artifacts to specify Stampede2-specific <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#building-basics-inclib">include and library paths</a> or other compiler settings. A good way to proceed is to write a shell script that implements the entire process: definitions of environment variables, module commands, and calls to the build utilities. Include <code>echo</code> statements with appropriate diagnostics. Run the script until you encounter an error. Research and fix the current problem. Document your experience in the script itself; including dead-ends, alternatives, and lessons learned. Re-run the script to get to the next error, then repeat until done. When you're finished, you'll have a repeatable process that you can archive until it's time to update the software or move to a new machine.</p>
<p>If you wish to share a software package with collaborators, you may need to modify file permissions. See <a href="https://docs.tacc.utexas.edu/tutorials/sharingprojectfiles" title="Sharing Project Files">Sharing Files with Collaborators</a> for more information.</p>
<!-- Intel MKL -->
<h3 id="mkl"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#mkl">Intel Math Kernel Library (MKL)</a></h3>
<p>The <a href="http://software.intel.com/intel-mkl" target="_blank">Intel Math Kernel Library</a> (MKL) is a collection of highly optimized functions implementing some of the most important mathematical kernels used in computational science, including standardized interfaces to:</p>
<ul>
<li><a href="http://netlib.org/blas" target="_blank">BLAS</a> (Basic Linear Algebra Subroutines), a collection of low-level matrix and vector operations like matrix-matrix multiplication </li>
<li><a href="http://netlib.org/lapack" target="_blank">LAPACK</a> (Linear Algebra PACKage), which includes higher-level linear algebra algorithms like Gaussian Elimination</li>
<li>FFT (Fast Fourier Transform), including interfaces based on <a href="http://fftw.org/" target="_blank">FFTW</a> (Fastest Fourier Transform in the West)</li>
<li><a href="http://netlib.org/scalapack" target="_blank">ScaLAPACK</a> (Scalable LAPACK), <a href="http://netlib.org/blacs" target="_blank">BLACS</a> (Basic Linear Algebra Communication Subprograms), Cluster FFT, and other functionality that provide block-based distributed memory (multi-node) versions of selected <a href="https://software.intel.com/en-us/mkl-developer-reference-c-lapack-routines" target="_blank">LAPACK</a>, <a href="https://software.intel.com/en-us/mkl-developer-reference-c-blas-and-sparse-blas-routines" target="_blank">BLAS</a>, and <a href="https://software.intel.com/en-us/mkl-developer-reference-c-fft-functions" target="_blank">FFT</a> algorithms;</li>
<li><a href="http://software.intel.com/en-us/node/521751" target="_blank">Vector Mathematics</a> (VM) functions that implement highly optimized and vectorized versions of special functions like sine and square root.</li>
</ul>
<h4 id="mkl-intel"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#mkl-intel">MKL with Intel C, C++, and Fortran Compilers</a></h4>
<p>There is no MKL module for the Intel compilers because you don't need one: the Intel compilers have built-in support for MKL. Unless you have specialized needs, there is no need to specify include paths and libraries explicitly. Instead, using MKL with the Intel modules requires nothing more than compiling and linking with the <code>-mkl</code> option.; e.g.</p>
<pre class="highlight"><code class="language-cmd-line hljs"><span class="hljs-variable">$ </span>icc   -mkl mycode.c
<span class="hljs-variable">$ </span>ifort -mkl mycode.c</code></pre>
<p>The <code>-mkl</code> switch is an abbreviated form of <code>-mkl=parallel</code>, which links your code to the threaded version of MKL. To link to the unthreaded version, use <code>-mkl=sequential</code>. A third option, <code>-mkl=cluster</code>, which also links to the unthreaded libraries, is necessary and appropriate only when using ScaLAPACK or other distributed memory packages. For additional information, including advanced linking options, see the <a href="http://software.intel.com/intel-mkl" target="_blank">MKL documentation</a> and <a href="http://software.intel.com/en-us/articles/intel-mkl-link-line-advisor" target="_blank">Intel MKL Link Line Advisor</a>.</p>
<h4 id="mkl-gnu"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#mkl-gnu">MKL with GNU C, C++, and Fortran Compilers</a></h4>
<p>When using a GNU compiler, load the MKL module before compiling or running your code, then specify explicitly the MKL libraries, library paths, and include paths your application needs. Consult the <a href="http://software.intel.com/en-us/articles/intel-mkl-link-line-advisor" target="_blank">Intel MKL Link Line Advisor</a> for details. A typical compile/link process on a TACC system will look like this:</p>
<pre class="highlight"><code class="language-cmd-line hljs"><span class="hljs-variable">$ </span><span class="hljs-keyword">module</span> load gcc
<span class="hljs-variable">$ </span><span class="hljs-keyword">module</span> load mkl                         <span class="hljs-comment"># available/needed only for GNU compilers</span>
<span class="hljs-variable">$ </span>gcc -fopenmp -I<span class="hljs-variable">$MKLROOT</span>/<span class="hljs-keyword">include</span>         \
         -<span class="hljs-title class_">Wl</span>,-L<span class="hljs-variable">${</span><span class="hljs-variable constant_">MKLROOT</span>}/lib/intel64     \
         -lmkl_intel_lp64 -lmkl_core      \
         -lmkl_gnu_thread -lpthread       \
         -lm -ldl mycode.c</code></pre>
<p>For your convenience the <code>mkl</code> module file also provides alternative TACC-defined variables like <code>$TACC_MKL_INCLUDE</code> (equivalent to <code>$MKLROOT/include</code>). Execute <code>module help mkl</code> for more information.</p>
<h4 id="mkl-thirdparty"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#mkl-thirdparty">Using MKL as BLAS/LAPACK with Third-Party Software</a></h4>
<p>When your third-party software requires BLAS or LAPACK, you can use MKL to supply this functionality. Replace generic instructions that include link options like <code>-lblas</code> or <code>-llapack</code> with the simpler MKL approach described above. There is no need to download and install alternatives like OpenBLAS.</p>
<h4 id="mkl-tacc"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#mkl-tacc">Using MKL as BLAS/LAPACK with TACC's MATLAB, Python, and R Modules</a></h4>
<p>TACC's MATLAB, Python, and R modules all use threaded (parallel) MKL as their underlying BLAS/LAPACK library. These means that even serial codes written in MATLAB, Python, or R may benefit from MKL's thread-based parallelism. This requires no action on your part other than specifying an appropriate max thread count for MKL; see the section below for more information.</p>
<h4 id="mkl-threading"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#mkl-threading">Controlling Threading in MKL</a></h4>
<p>Any code that calls MKL functions can potentially benefit from MKL's thread-based parallelism; this is true even if your code is not otherwise a parallel application. If you are linking to the threaded MKL (using <code>-mkl</code>, <code>-mkl=parallel</code>, or the equivalent explicit link line), you need only specify an appropriate value for the max number of threads available to MKL. You can do this with either of the two environment variables <code>MKL_NUM_THREADS</code> or <code>OMP_NUM_THREADS</code>. The environment variable <code>MKL_NUM_THREADS</code> specifies the max number of threads available to each instance of MKL, and has no effect on non-MKL code. If <code>MKL_NUM_THREADS</code> is undefined, MKL uses <code>OMP_NUM_THREADS</code> to determine the max number of threads available to MKL functions. In either case, MKL will attempt to choose an optimal thread count less than or equal to the specified value. Note that <code>OMP_NUM_THREADS</code> defaults to 1 on TACC systems; if you use the default value you will get no thread-based parallelism from MKL.</p>
<p>If you are running a single serial, unthreaded application (or an unthreaded MPI code involving a single MPI task per node) it is usually best to give MKL as much flexibility as possible by setting the max thread count to the total number of hardware threads on the node (272 on KNL, 96 on SKX, 160 on ICX). Of course things are more complicated if you are running more than one process on a node: e.g. multiple serial processes, threaded applications, hybrid MPI-threaded applications, or pure MPI codes running more than one MPI rank per node. See <a href="http://software.intel.com/en-us/articles/recommended-settings-for-calling-intel-mkl-routines-from-multi-threaded-applications" target="_blank">http://software.intel.com/en-us/articles/recommended-settings-for-calling-intel-mkl-routines-from-multi-threaded-applications</a> and related Intel resources for examples of how to manage threading when calling MKL from multiple processes. </p>
<h4 id="mkl-othercapabilities"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#mkl-othercapabilities">Using ScaLAPACK, Cluster FFT, and Other MKL Cluster Capabilities</a></h4>
<p>See "<a href="https://software.intel.com/en-us/mkl-linux-developer-guide-working-with-the-intel-math-kernel-library-cluster-software" target="_blank">Working with the Intel Math Kernel Library Cluster Software</a>" and "<a href="http://software.intel.com/en-us/articles/intel-mkl-link-line-advisor" target="_blank">Intel MKL Link Line Advisor</a>" for information on linking to the MKL cluster components.</p>
<h3 id="building-performance"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#building-performance">Building for Performance on Stampede2</a></h3>
<h4 id="building-performance-compiler"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#building-performance-compiler">Compiler</a></h4>
<p>When building software on Stampede2, we recommend using the most recent Intel compiler and Intel MPI library available on Stampede2. The most recent versions may be newer than the defaults. Execute <code>module spider intel</code> and <code>module spider impi</code> to see what's installed. When loading these modules you may need to specify version numbers explicitly (e.g. <code>module load intel/18.0.0</code> and <code>module load impi/18.0.0</code>).</p>
<h4 id="building-performance-architecture"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#building-performance-architecture">Architecture-Specific Flags</a></h4>
<p>To compile for KNL only, include <code>-xMIC-AVX512</code> as a build option. The <code>-x</code> switch allows you to specify a <a href="https://software.intel.com/en-us/fortran-compiler-18.0-developer-guide-and-reference-x-qx" target="_blank">target architecture</a>, while <code>MIC-AVX512</code> is the KNL-specific subset of Intel's Advanced Vector Extensions 512-bit <a href="https://software.intel.com/en-us/articles/performance-tools-for-software-developers-intel-compiler-options-for-sse-generation-and-processor-specific-optimizations" target="_blank">instruction set</a>.  Besides all other appropriate compiler options, you should also consider specifying an <a href="https://software.intel.com/en-us/fortran-compiler-18.0-developer-guide-and-reference-o" target="_blank">optimization level</a> using the <code>-O</code> flag:</p>
<pre class="highlight"><code class="language-cmd-line hljs"><span class="hljs-variable">$ </span>icc   -xMIC-<span class="hljs-variable constant_">AVX512</span>  -<span class="hljs-variable constant_">O3</span> mycode.c   -o myexe         <span class="hljs-comment"># will run only on KNL</span></code></pre>
<p>Similarly, to build for SKX or ICX, specify the <code>CORE-AVX512</code> instruction set, which is native to SKX and ICX:</p>
<pre class="highlight"><code class="language-cmd-line hljs"><span class="hljs-variable">$ </span>ifort -xCORE-<span class="hljs-variable constant_">AVX512</span> -<span class="hljs-variable constant_">O3</span> mycode.f90 -o myexe         <span class="hljs-comment"># will run on SKX or ICX</span></code></pre>
<p>Because Stampede2 has two kinds of compute nodes, however, we recommend a more flexible approach when building with the Intel compiler: use <a href="https://software.intel.com/en-us/articles/performance-tools-for-software-developers-sse-generation-and-processor-specific-optimizations-continue#1" target="_blank">CPU dispatch</a> to build a multi-architecture ("fat") binary that contains alternate code paths with optimized vector code for each type of Stampede2 node. To produce a multi-architecture binary for Stampede2, build with the following options:  </p>
<pre><code>-xCORE-AVX2 -axCORE-AVX512,MIC-AVX512
</code></pre>
<p>These particular choices allow you to build on any Stampede2 node (KNL, SKX and ICX nodes), and use <a href="https://software.intel.com/en-us/articles/performance-tools-for-software-developers-sse-generation-and-processor-specific-optimizations-continue#1" target="_blank">CPU dispatch</a> to produce a multi-architecture binary. We recommend that you specify these flags in both the compile and link steps. Specify an optimization level (e.g. <code>-O3</code>) along with any other appropriate compiler switches:</p>
<pre class="highlight"><code class="language-cmd-line hljs"><span class="hljs-variable">$ </span>icc -xCORE-<span class="hljs-variable constant_">AVX2</span> -axCORE-<span class="hljs-variable constant_">AVX512</span>,<span class="hljs-variable constant_">MIC</span>-<span class="hljs-variable constant_">AVX512</span> -<span class="hljs-variable constant_">O3</span> mycode.c -o myexe</code></pre>
<p>The <code>-x</code> option is the target base architecture (instruction set). The base instruction set must run on all targeted processors. Here we specify <code>CORE-AVX2</code>, which is native for older Broadwell processors and supported on all KNL, SKX and ICX nodex. This option allows configure scripts and similar build systems to run test executables on any Stampede2 login or compute node. The <code>-ax</code> option is a comma-separated list of alternate instruction sets: <code>CORE-AVX512</code> for SKX and ICX, and <code>MIC-AVX512</code> for KNL. </p>
<p>Now that we have replaced the original Broadwell login nodes with newer Skylake login nodes, <code>-xCORE-AVX2</code> remains a reasonable (though conservative) base option. Another plausible, more aggressive base option is <code>-xCOMMON-AVX512</code>, which is a subset of <code>AVX512</code> that runs on all KNL, SKX and ICX nodex. </p>
<p><strong>It's best to avoid building with <code>-xHost</code></strong> (a flag that means "optimize for the architecture on which I'm compiling now"). Using <code>-xHost</code> on a SKX login node, for example, will result in a binary that won't run on KNL.</p>
<p>Don't skip the <code>-x</code> flag in a multi-architecture build: the default is the very old SSE2 (Pentium 4) instruction set. <strong>Don't create a multi-architecture build with a base option of either <code>-xMIC-AVX512</code> (native on KNL) or <code>-xCORE-AVX512</code> (native on SKX/ICX);</strong> there are no meaningful, compatible alternate (<code>-ax</code>) instruction sets:</p>
<p></p><pre class="highlight"><code class="language-cmd-line hljs"><span class="hljs-variable">$ </span>icc -xCORE-<span class="hljs-variable constant_">AVX512</span> -axMIC-<span class="hljs-variable constant_">AVX512</span> -<span class="hljs-variable constant_">O3</span> mycode.c -o myexe       <span class="hljs-comment"># NO! Base incompatible with alternate</span></code></pre>
On Stampede2, the module files for newer Intel compilers (Intel 18.0.0 and later) define the environment variable <code>TACC_VEC_FLAGS</code> that stores the recommended architecture flags described above. This can simplify your builds:<p></p>
<pre class="highlight"><code class="language-cmd-line hljs"><span class="hljs-variable">$ </span>echo <span class="hljs-variable">$TACC_VEC_FLAGS</span>                         <span class="hljs-comment"># env variable available only for intel/18.0.0 and later</span>
-xCORE-<span class="hljs-variable constant_">AVX2</span> -axCORE-<span class="hljs-variable constant_">AVX512</span>,<span class="hljs-variable constant_">MIC</span>-<span class="hljs-variable constant_">AVX512</span>
<span class="hljs-variable">$ </span>icc <span class="hljs-variable">$TACC_VEC_FLAGS</span> -<span class="hljs-variable constant_">O3</span> mycode.c -o myexe</code></pre>
<p>Simplicity is a major advantage of this multi-architecture approach: it allows you to build and run anywhere on Stampede2, and performance is generally comparable to single-architecture builds. There are some trade-offs to consider, however. This approach will take a little longer to compile than single-architecture builds, and will produce a larger binary. In some cases, you might also pay a small performance penalty over single-architecture approaches. For more information see the <a href="https://software.intel.com/en-us/articles/performance-tools-for-software-developers-intel-compiler-options-for-sse-generation-and-processor-specific-optimizations" target="_blank">Intel documentation</a>.  </p>
<p>For information on the performance implications of your choice of build flags, see the sections on Programming and Performance for <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-and-performance-knl">KNL</a> and <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-and-performance-skx">SKX and ICX</a> respectively.</p>
<p>If you use GNU compilers, see <a href="https://gcc.gnu.org/onlinedocs/gcc/x86-Options.html" target="_blank">GNU x86 Options</a> for information regarding support for KNL, SKX and ICX. Note that GNU compilers do not support multi-architecture binaries.</p>
<h2 id="running"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running">Running Jobs</a></h2>
<h3 id="jobaccounting"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#jobaccounting">Job Accounting</a></h3>
<p>Like all TACC systems, Stampede2's accounting system is based on node-hours: one unadjusted Service Unit (SU) represents a single compute node used for one hour (a node-hour). For any given job, the total cost in SUs is the use of one compute node for one hour of wall clock time plus any charges or discounts for the use of specialized queues, e.g. Frontera's <code>flex</code> queue, Stampede2's <code>development</code> queue, and Longhorn's <code>v100</code> queue. The <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#queues">queue charge rates</a> are determined by the supply and demand for that particular queue or type of node used and are subject to change.  </p>
<p><strong>Stampede2 SUs billed = (# nodes) x (job duration in wall clock hours) x (charge rate per node-hour)</strong></p>
<p>The Slurm scheduler tracks and charges for usage to a granularity of a few seconds of wall clock time. <strong>The system charges only for the resources you actually use, not those you request.</strong> If your job finishes early and exits properly, Slurm will release the nodes back into the pool of available nodes. Your job will only be charged for as long as you are using the nodes.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>TACC does not implement node-sharing on any compute resource. Each Stampede2 node can be assigned to only one user at a time; hence a complete node is dedicated to a user's job and accrues wall-clock time for all the node's cores whether or not all cores are used.</p>
</div>
<p><strong>Tip</strong>: Your queue wait times will be less if you request only the time you need: the scheduler will have a much easier time finding a slot for the 2 hours you really need than say, for the 12 hours requested in your job script. </p>
<p>Principal Investigators can monitor allocation usage via the <a href="https://tacc.utexas.edu/portal/login" title="TACC Portal login" target="_blank">TACC User Portal</a> under <a href="https://tacc.utexas.edu/use-tacc/allocations/" title="TACC Allocations" target="_blank">"Allocations-&gt;Projects and Allocations"</a>. Be aware that the figures shown on the portal may lag behind the most recent usage. Projects and allocation balances are also displayed upon command-line login.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>To display a summary of your TACC project balances and disk quotas at any time, execute:<br><br><code>login1$ <b>/usr/local/etc/taccinfo</b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Generally more current than balances displayed on the portals.</code></p>
</div>
<h3 id="running-slurm"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-slurm">Slurm Job Scheduler</a></h3>
<p>Stampede2's job scheduler is the <a href="http://schedmd.com/" target="_blank">Slurm Workload Manager</a>. Slurm commands enable you to submit, manage, monitor, and control your jobs. </p>
<h3 id="running-queues"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-queues">Slurm Partitions (Queues)</a></h3>
<p>Currently available queues include those in <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#table5">Stampede2 Production Queues</a>. See <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#overview-phase1computenodes">KNL Compute Nodes</a>, <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#overview-skxcomputenodes">SKX Compute Nodes</a>, <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-knl-memorymodes">Memory Modes</a>, and <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-knl-clustermodes">Cluster Modes</a> for more information on node types.</p>
<p><a id="queues"></a></p><a id="queues">
</a><h4 id="table5"><a id="queues"></a><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#table5">Table 5. Production Queues</a></h4>
<table>
<thead>
<tr>
<th>Queue Name</th>
<th>Node Type</th>
<th>Max Nodes per Job<br> (assoc'd cores)*</th>
<th>Max Duration</th>
<th>Max Jobs in Queue *</th>
<th>Charge Rate<br> (per node-hour)</th>
</tr>
</thead>
<tbody>
<tr>
<td>development</td>
<td>KNL cache-quadrant</td>
<td>16 nodes<br> (1,088 cores)*</td>
<td>2 hrs</td>
<td>1*</td>
<td>0.8 Service Unit (SU)</td>
</tr>
<tr>
<td>normal</td>
<td>KNL cache-quadrant</td>
<td>256 nodes<br> (17,408 cores) *</td>
<td>48 hrs</td>
<td>50 *</td>
<td>0.8 SU</td>
</tr>
<tr>
<td>large **</td>
<td>KNL cache-quadrant</td>
<td>2048 nodes<br> (139,264 cores) **</td>
<td>48 hrs</td>
<td>5 **</td>
<td>0.8 SU</td>
</tr>
<tr>
<td>long</td>
<td>KNL cache-quadrant</td>
<td>32 nodes<br>(2,176 cores) *</td>
<td>120 hrs</td>
<td>2  *</td>
<td>0.8 SU</td>
</tr>
<tr>
<td>flat-quadrant</td>
<td>KNL flat-quadrant</td>
<td>32 nodes<br> (2,176 cores)  *</td>
<td>48 hrs</td>
<td>5  *</td>
<td>0.8 SU</td>
</tr>
<tr>
<td>skx-dev</td>
<td>SKX</td>
<td>4 nodes<br>(192 cores) *</td>
<td>2 hrs</td>
<td>1 *</td>
<td>1 SU</td>
</tr>
<tr>
<td>skx-normal</td>
<td>SKX</td>
<td>128 nodes<br>(6,144 cores) *</td>
<td>48 hrs</td>
<td>20 *</td>
<td>1 SU</td>
</tr>
<tr>
<td>skx-large * *</td>
<td>SKX</td>
<td>868 nodes<br>(41,664 cores) *</td>
<td>48 hrs</td>
<td>3 *</td>
<td>1 SU</td>
</tr>
<tr>
<td>icx-normal</td>
<td>ICX</td>
<td>40 nodes<br>(3,200 cores) *</td>
<td>48 hrs</td>
<td>20 *</td>
<td>1.67 SU</td>
</tr>
</tbody>
</table>
<p>* Queue status as of March 7, 2022. <strong>Queues and limits are subject to change without notice.</strong> Execute <code>qlimits</code> on Stampede2 for real-time information regarding limits on available queues. See <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#monitoring">Monitoring Jobs and Queues</a> for additional information.</p>
<p>** To request more nodes than are available in the normal queue, submit a consulting (help desk) ticket through the <a href="https://tacc.utexas.edu/portal/login" title="TACC Portal login" target="_blank">TACC</a>.  Include in your request reasonable evidence of your readiness to run under the conditions you're requesting. In most cases this should include your own strong or weak scaling results from Stampede2.</p>
<p>*** For non-hybrid memory-cluster modes or other special requirements, submit a ticket through the <a href="https://tacc.utexas.edu/portal/login" title="TACC Portal login" target="_blank">TACC User Portal</a>.</p>
<h3 id="running-sbatch"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-sbatch">Submitting Batch Jobs with <code>sbatch</code></a></h3>
<p>Use Slurm's <code>sbatch</code> command to <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#using-computenodes">submit a batch job</a> to one of the Stampede2 queues:</p>
<pre class="highlight"><code class="language-cmd-line hljs">login1<span class="hljs-variable">$ </span>sbatch myjobscript</code></pre>
<p>Here <code>myjobscript</code> is the name of a text file containing <code>#SBATCH</code> directives and shell commands that describe the particulars of the job you are submitting. The details of your job script's contents depend on the type of job you intend to run. </p>
<p>In your job script you (1) use <code>#SBATCH</code> directives to request computing resources (e.g. 10 nodes for 2 hrs); and then (2) use shell commands to specify what work you're going to do once your job begins. There are many possibilities: you might elect to launch a single application, or you might want to accomplish several steps in a workflow. You may even choose to launch more than one application at the same time. The details will vary, and there are many possibilities. But your own job script will probably include at least one launch line that is a variation of one of the examples described here.</p>
<p>Your job will run in the environment it inherits at submission time; this environment includes the modules you have loaded and the current working directory. In most cases you should <strong>run your applications(s) after loading the same modules that you used to build them</strong>.  You can of course use your job submission script to modify this environment by defining new environment variables; changing the values of existing environment variables; loading or unloading modules; changing directory; or specifying relative or absolute paths to files. <strong>Do not use the Slurm <code>--export</code> option to manage your job's environment</strong>: doing so can interfere with the way the system propagates the inherited environment.</p>
<p>The <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#table6">Common <code>sbatch</code> Options table</a> below describes some of the most common <code>sbatch</code> command options. Slurm directives begin with <code>#SBATCH</code>; most have a short form (e.g. <code>-N</code>) and a long form (e.g. <code>--nodes</code>). You can pass options to <code>sbatch</code> using either the command line or job script; most users find that the job script is the easier approach. The first line of your job script must specify the interpreter that will parse non-Slurm commands; in most cases <code>#!/bin/bash</code> or <code>#!/bin/csh</code> is the right choice. Avoid <code>#!/bin/sh</code> (its startup behavior can lead to subtle problems on Stampede2), and do not include comments or any other characters on this first line. All <code>#SBATCH</code> directives must precede all shell commands. Note also that certain <code>#SBATCH</code> options or combinations of options are mandatory, while others are not available on Stampede2.</p>
<h4 id="table6"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#table6">Table 6. Common sbatch Options</a></h4>
<table>
<thead>
<tr>
<th>Option</th>
<th>Argument</th>
<th>Comments</th>
</tr>
</thead>
<tbody>
<tr>
<td>-p</td>
<td>queue_name</td>
<td>Submits to queue (partition) designated by queue_name</td>
</tr>
<tr>
<td>-J</td>
<td>job_name</td>
<td>Job Name</td>
</tr>
<tr>
<td>-N</td>
<td>total_nodes</td>
<td>Required. Define the resources you need by specifying either:<br>(1) -N and -n; or<br>(2) -N and --ntasks-per-node.</td>
</tr>
<tr>
<td>-n</td>
<td>total_tasks</td>
<td>This is total MPI tasks in this job. See <u>-N</u> above for a good way to use this option. When using this option in a non-MPI job, it is usually best to set it to the same value as -N.</td>
</tr>
<tr>
<td><u>--ntasks-per-node</u><br>or<br>--tasks-per-node</td>
<td>tasks_per_node</td>
<td>This is MPI tasks per node. See -N above for a good way to use this option. When using this option in a non-MPI job, it is usually best to set --ntasks-per-node to 1.</td>
</tr>
<tr>
<td>-t</td>
<td>hh:mm:ss</td>
<td>Required. Wall clock time for job.</td>
</tr>
<tr>
<td>--mail-user=</td>
<td>email_address</td>
<td>Specify the email address to use for notifications. Use with the --mail-type= flag below.</td>
</tr>
<tr>
<td>--mail-type=</td>
<td>begin, end, fail, or  all</td>
<td>Specify when user notifications are to be sent (one option per line).</td>
</tr>
<tr>
<td>-o</td>
<td>output_file</td>
<td>Direct job standard output to output_file (without -e option error goes to this file)</td>
</tr>
<tr>
<td>-e</td>
<td>error_file</td>
<td>Direct job error output to error_file</td>
</tr>
<tr>
<td>-d=</td>
<td>afterok:jobid</td>
<td>Specifies a dependency: this run will start only after the specified job (jobid) successfully finishes</td>
</tr>
<tr>
<td>-A</td>
<td>projectnumber</td>
<td>Charge job to the specified project/allocation number.  This option is only necessary for logins associated with multiple projects.</td>
</tr>
<tr>
<td>-a<br>or<br>--array</td>
<td>N/A</td>
<td>Not available. Use the launcher module for parameter sweeps and other collections of related serial jobs.</td>
</tr>
<tr>
<td>--mem</td>
<td>N/A</td>
<td>Not available. If you attempt to use this option, the scheduler will not accept your job.</td>
</tr>
<tr>
<td>--export=</td>
<td>N/A</td>
<td>Avoid this option on Stampede2. Using it is rarely necessary and can interfere with the way the system propagates your environment.</td>
</tr>
</tbody>
</table>
<p>By default, Slurm writes all console output to a file named <code>slurm-%j.out</code>, where <code>%j</code> is the numerical job ID. To specify a different filename use the <code>-o</code> option. To save <code>stdout</code> (standard out) and <code>stderr</code> (standard error) to separate files, specify both <code>-o</code> and <code>-e</code>.</p>
<h3 id="running-launching"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-launching">Launching Applications</a></h3>
<p>The primary purpose of your job script is to launch your research application. How you do so depends on several factors, especially (1) the type of application (e.g. MPI, OpenMP, serial), and (2) what you're trying to accomplish (e.g. launch a single instance, complete several steps in a workflow, run several applications simultaneously within the same job). While there are many possibilities, your own job script will probably include a launch line that is a variation of one of the examples described in this section:</p>
<ul>
<li><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-launching-serial">Launching One Serial Application</a></li>
<li><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-launching-multi">Launching One Multi-Threaded Application</a></li>
<li><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-launching-mpi">Launching One MPI Application</a></li>
<li><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-launching-hybrid">Launching One Hybrid (MPI+Threads) Application</a></li>
<li><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-launching-serialmorethanone">More Than One Serial Application in the Same Job</a></li>
<li><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-launching-mpisimultaneous">More than One MPI Application Running Concurrently</a></li>
<li><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-launching-openmpsimultaneous">More than One OpenMP Application Running Concurrently</a></li>
</ul>
<h4 id="running-launching-serial"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-launching-serial">Launching One Serial Application</a></h4>
<p>To launch a serial application, simply call the executable. Specify the path to the executable in either the PATH environment variable or in the call to the executable itself:</p>
<pre class="highlight"><code class="language-job-script hljs">myprogram                   <span class="hljs-comment"># executable in a directory listed in $PATH</span>
<span class="hljs-variable">$WORK</span>/apps/myprov/myprogram <span class="hljs-comment"># explicit full path to executable</span>
./myprogram                 <span class="hljs-comment"># executable in current directory</span>
./myprogram -m -k 6 input1  <span class="hljs-comment"># executable with notional input options</span></code></pre>
<h4 id="running-launching-multi"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-launching-multi">Launching One Multi-Threaded Application</a></h4>
<p>Launch a threaded application the same way. Be sure to specify the number of threads. <strong>Note that the default OpenMP thread count is 1</strong>.</p>
<pre class="highlight"><code class="language-job-script hljs"><span class="hljs-built_in">export</span> OMP_NUM_THREADS=68    <span class="hljs-comment"># 68 total OpenMP threads (1 per KNL core)</span>
./myprogram</code></pre>
<h4 id="running-launching-mpi"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-launching-mpi">Launching One MPI Application</a></h4>
<p>To launch an MPI application, use the TACC-specific MPI launcher <code>ibrun</code>, which is a Stampede2-aware replacement for generic MPI launchers like <code>mpirun</code> and <code>mpiexec</code>. In most cases the only arguments you need are the name of your executable followed by any arguments your executable needs. When you call <code>ibrun</code> without other arguments, your Slurm <code>#SBATCH</code> directives will determine the number of ranks (MPI tasks) and number of nodes on which your program runs.</p>
<pre class="highlight"><code class="language-job-script hljs"><span class="hljs-comment">#SBATCH -N 5</span>
<span class="hljs-comment">#SBATCH -n 200</span>
ibrun ./myprogram              <span class="hljs-comment"># ibrun uses the $SBATCH directives to properly allocate nodes and tasks</span>
</code></pre>
<p>To use <code>ibrun</code> interactively, say within an <code>idev</code> session, you can specify:</p>
<pre class="highlight"><code class="language-cmd-line hljs">
login1<span class="hljs-variable">$ </span>idev -N <span class="hljs-number">2</span> -n <span class="hljs-number">80</span> 
c123-<span class="hljs-number">456</span><span class="hljs-variable">$ </span>ibrun ./myprogram    <span class="hljs-comment"># ibrun uses idev's arguments to properly allocate nodes and tasks</span>
</code></pre>
<h3 id="running-launching-hybrid"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-launching-hybrid">Launching One Hybrid (MPI+Threads) Application</a></h3>
<p>When launching a single application you generally don't need to worry about affinity: both Intel MPI and MVAPICH2 will distribute and pin tasks and threads in a sensible way.</p>
<pre class="highlight"><code class="language-job-script hljs"><span class="hljs-built_in">export</span> OMP_NUM_THREADS=8    <span class="hljs-comment"># 8 OpenMP threads per MPI rank</span>
ibrun ./myprogram           <span class="hljs-comment"># use ibrun instead of mpirun or mpiexec</span></code></pre>
<p>As a practical guideline, the product of <code>$OMP_NUM_THREADS</code> and the maximum number of MPI processes per node should not be greater than total number of cores available per node (KNL nodes have 68 cores, SKX nodes have 48 cores, ICX nodes have 80 cores).</p>
<h3 id="running-launching-serialmorethanone"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-launching-serialmorethanone">More Than One Serial Application in the Same Job</a></h3>
<p>TACC's <code>launcher</code> utility provides an easy way to launch more than one serial application in a single job. This is a great way to engage in a popular form of High Throughput Computing: running parameter sweeps (one serial application against many different input datasets) on several nodes simultaneously. The <code>launcher</code> utility will execute your specified list of independent serial commands, distributing the tasks evenly, pinning them to specific cores, and scheduling them to keep cores busy. Execute <code>module load launcher</code> followed by <code>module help launcher</code> for more information.</p>
<h4 id="running-launching-consecutivempi"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-launching-consecutivempi">MPI Applications One at a Time</a></h4>
<p>To run one MPI application after another (or any sequence of commands one at a time), simply list them in your job script in the order in which you'd like them to execute. When one application/command completes, the next one will begin.</p>
<pre class="highlight"><code class="language-job-script hljs">module load git
module list
./preprocess.sh
ibrun ./myprogram input1    <span class="hljs-comment"># runs after preprocess.sh completes</span>
ibrun ./myprogram input2    <span class="hljs-comment"># runs after previous MPI app completes</span></code></pre>
<h4 id="running-launching-mpisimultaneous"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-launching-mpisimultaneous">More than One MPI Application Running Concurrently</a></h4>
<p>To run more than one MPI application simultaneously in the same job, you need to do several things:</p>
<ul>
<li>use ampersands to launch each instance in the background;</li>
<li>include a <code>wait</code> command to pause the job script until the background tasks complete;</li>
<li>use the <code>ibrun "-n</code> and <code>-o</code> switches to specify task counts and hostlist offsets respectively; and</li>
<li>include a call to the <code>task_affinity</code> script in your <code>ibrun</code> launch line.</li>
</ul>
<p>If, for example, you use <code>#SBATCH</code> directives to request N=4 nodes and n=128 total MPI tasks, Slurm will generate a hostfile with 128 entries (32 entries for each of 4 nodes). The <code>-n</code> and <code>-o</code> switches, which must be used together, determine which hostfile entries ibrun uses to launch a given application; execute <code>ibrun --help</code> for more information. <strong>Don't forget the ampersands (<code>&amp;</code>)</strong> to launch the jobs in the background, <strong>and the <code>wait</code> command</strong> to pause the script until the background tasks complete:</p>
<pre class="highlight"><code class="language-job-script hljs">ibrun -n 64 -o  0 task_affinity ./myprogram input1 &amp;   <span class="hljs-comment"># 64 tasks; offset by  0 entries in hostfile.</span>
ibrun -n 64 -o 64 task_affinity ./myprogram input2 &amp;   <span class="hljs-comment"># 64 tasks; offset by 64 entries in hostfile.</span>
<span class="hljs-built_in">wait</span>                                                       <span class="hljs-comment"># Required; else script will exit immediately.</span></code></pre>
<p>The <code>task_affinity</code> script does two things:</p>
<ul>
<li><code>task_affinity</code> manages task placement and pinning when you call <code>ibrun</code> with the <code>-n</code>, <code>-o</code> switches (it's not necessary under any other circumstances); and</li>
<li><code>task_affinity</code> also manages MCDRAM when you run in flat-quadrant mode on the KNL. It does this in the same way as <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#managing-memory"><code>mem_affinity</code></a>. </li>
<li><strong>Don't confuse <code>task_affinity</code> with <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#managing-memory"><code>tacc_affinity</code></a></strong>; the keyword <code>tacc_affinity</code> is now a symbolic link to <code>mem_affinity</code>. The <code>mem_affinity</code> script and the symbolic link <code>tacc_affinity</code> manage MCDRAM in flat-quadrant mode on the KNL, but they do not pin MPI tasks.</li>
</ul>
<h4 id="running-launching-openmpsimultaneous"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-launching-openmpsimultaneous">More than One OpenMP Application Running Concurrently</a></h4>
<p>You can also run more than one OpenMP application simultaneously on a single node, but you will need to <!-- <a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/omp-affinity.html">distribute and pin tasks appropriately</a> --> distribute and pin tasks appropriately. In the example below, <code>numactl -C</code> specifies virtual CPUs (hardware threads). According to the numbering scheme for KNL hardware threads, CPU (hardware thread) numbers 0-67 are spread across the 68 cores, 1 thread per core. Similarly for SKX: CPU (hardware thread) numbers 0-47 are spread across the 48 cores, 1 thread per core, and for ICX: CPU (hardware thread) numbers 0-79 are spread across the 80 cores, 1 thread per core. <!-- See <a href="http://xortal.tacc.utexas.edu/training#/session/64">TACC training materials</a> for more information. --></p>
<pre class="highlight"><code class="language-job-script hljs"><span class="hljs-built_in">export</span> OMP_NUM_THREADS=2
numactl -C 0-1 ./myprogram inputfile1 &amp;  <span class="hljs-comment"># HW threads (hence cores) 0-1. Note ampersand.</span>
numactl -C 2-3 ./myprogram inputfile2 &amp;  <span class="hljs-comment"># HW threads (hence cores) 2-3. Note ampersand.</span>

<span class="hljs-built_in">wait</span></code></pre>
<h3 id="running-idev"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-idev">Interactive Sessions with <code>idev</code> and <code>srun</code></a></h3>
<p>TACC's own <code>idev</code> utility is the best way to begin an interactive session on one or more compute nodes. To launch a thirty-minute session on a single node in the development queue, simply execute:</p>
<pre class="highlight"><code class="language-cmd-line hljs">login1<span class="hljs-variable">$ </span>idev</code></pre>
<p>You'll then see output that includes the following excerpts:</p>
<pre class="highlight"><code class="language-cmd-line hljs">...
-----------------------------------------------------------------
      <span class="hljs-title class_">Welcome</span> to the <span class="hljs-title class_">Stampede2</span> <span class="hljs-title class_">Supercomputer</span>          
-----------------------------------------------------------------
...

-&gt; <span class="hljs-title class_">After</span> your idev job begins to run, a command prompt will appear,
-&gt; <span class="hljs-keyword">and</span> you can <span class="hljs-keyword">begin</span> your interactive development session. 
-&gt; <span class="hljs-title class_">We</span> will report the job status every <span class="hljs-number">4</span> <span class="hljs-symbol">seconds:</span> (<span class="hljs-variable constant_">PD</span>=pending, R=running).

-&gt;job <span class="hljs-symbol">status:</span>  <span class="hljs-variable constant_">PD</span>
-&gt;job <span class="hljs-symbol">status:</span>  <span class="hljs-variable constant_">PD</span>
...
c449-<span class="hljs-number">001</span>$</code></pre>
<p>The <code>job status</code> messages indicate that your interactive session is waiting in the queue. When your session begins, you'll see a command prompt on a compute node (in this case, the node with hostname c449-001). If this is the first time you launch <code>idev</code>, the prompts may invite you to choose a default project and a default number of tasks per node for future <code>idev</code> sessions.</p>
<p>For command line options and other information, execute <code>idev --help</code>. It's easy to tailor your submission request (e.g. shorter or longer duration) using Slurm-like syntax:</p>
<pre class="highlight"><code class="language-cmd-line hljs">login1<span class="hljs-variable">$ </span>idev -p normal -N <span class="hljs-number">2</span> -n <span class="hljs-number">8</span> -m <span class="hljs-number">150</span> <span class="hljs-comment"># normal queue, 2 nodes, 8 total tasks, 150 minutes</span></code></pre>
<p>For more information see the <a href="https://docs.tacc.utexas.edu/software/idev"><code>idev</code> documentation</a>.</p>
<p>You can also launch an interactive session with Slurm's <code>srun</code> command, though there's no clear reason to prefer <code>srun</code> to <code>idev</code>. A typical launch line would look like this:</p>
<pre class="highlight"><code class="language-cmd-line hljs">login1<span class="hljs-variable">$ </span>srun --pty -N <span class="hljs-number">2</span> -n <span class="hljs-number">8</span> -t <span class="hljs-number">2</span><span class="hljs-symbol">:</span><span class="hljs-number">30</span><span class="hljs-symbol">:</span><span class="hljs-number">00</span> -p normal /bin/bash -l <span class="hljs-comment"># same conditions as above</span></code></pre>
<h3 id="running-ssh"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-ssh">Interactive Sessions using <code>ssh</code></a></h3>
<p>If you have a batch job or interactive session running on a compute node, you "own the node": you can connect via <code>ssh</code> to open a new interactive session on that node. This is an especially convenient way to monitor your applications' progress. One particularly helpful example: login to a compute node that you own, execute <code>top</code>, then press the "1" key to see a display that allows you to monitor thread ("CPU") and memory use.</p>
<p>There are many ways to determine the nodes on which you are running a job, including feedback messages following your <code>sbatch</code> submission, the compute node command prompt in an <code>idev</code> session, and the <code>squeue</code> or <code>showq</code> utilities. The sequence of identifying your compute node then connecting to it would look like this:</p>
<pre class="highlight"><code class="language-cmd-line hljs">login1<span class="hljs-variable">$ </span>squeue -u bjones
 <span class="hljs-variable constant_">JOBID</span>       <span class="hljs-variable constant_">PARTITION</span>     <span class="hljs-variable constant_">NAME</span>     <span class="hljs-variable constant_">USER</span> <span class="hljs-variable constant_">ST</span>       <span class="hljs-variable constant_">TIME</span>  <span class="hljs-variable constant_">NODES</span> <span class="hljs-variable constant_">NODELIST</span>(<span class="hljs-variable constant_">REASON</span>)
<span class="hljs-number">858811</span>     development idv46796   bjones  R       <span class="hljs-number">0</span><span class="hljs-symbol">:</span><span class="hljs-number">39</span>      <span class="hljs-number">1</span> c448-<span class="hljs-number">004</span>
1ogin1<span class="hljs-variable">$ </span>ssh c448-<span class="hljs-number">004</span>
...
<span class="hljs-variable constant_">C448</span>-<span class="hljs-number">004</span>$</code></pre>
<h3 id="running-slurmenvvars"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-slurmenvvars">SLURM Environment Variables</a></h3>
<p>Be sure to distinguish between internal Slurm replacement symbols (e.g. <code>%j</code> described above) and Linux environment variables defined by Slurm (e.g. <code>SLURM_JOBID</code>). Execute <code>env | grep SLURM</code> from within your job script to see the full list of Slurm environment variables and their values. You can use Slurm replacement symbols like <code>%j</code> only to construct a Slurm filename pattern; they are not meaningful to your Linux shell. Conversely, you can use Slurm environment variables in the shell portion of your job script but not in an <code>#SBATCH</code> directive. </p>
<div class="admonition danger">
<p class="admonition-title">Danger</p>
<p>For example, the following directive will not work the way you might think:</p>
<pre class="highlight"><code class="language-job-script hljs"><span class="hljs-comment">#SBATCH -o myMPI.o${SLURM_JOB_ID}   # incorrect</span></code></pre>
</div>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>Instead, use the following directive:</p>
<pre class="highlight"><code class="language-job-script hljs"><span class="hljs-comment">#SBATCH -o myMPI.o%j     # "%j" expands to your job's numerical job ID</span></code></pre>
</div>
<p>Similarly, you cannot use paths like <code>$WORK</code> or <code>$SCRATCH</code> in an <code>#SBATCH</code> directive.</p>
<p>For more information on this and other matters related to Slurm job submission, see the <a href="https://slurm.schedmd.com/sbatch.html" target="_blank">Slurm online documentation</a>; the man pages for both Slurm itself (<code>man slurm</code>) and its individual command (e.g. <code>man sbatch</code>); as well as numerous other online resources.</p>
<h2 id="scripts"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#scripts">Job Scripts</a></h2>
<p>This section provides sample Slurm job scripts for each Stampede2 node type: Knight's Landing (KNL), Sky Lake (SKX) and Ice Lake (ICX) nodes. Each section also contains sample scripts for serial, MPI, OpenMP and hybrid (MPI + OpenMP) programming models.  Copy and customize each script for your own applications.</p>
<h3 id="scripts-knl"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#scripts-knl">KNL Nodes</a></h3>
<p>Click on a tab for a customizable job-script.</p>
<div class="tabbed-set" data-tabs="1:4"><input checked="checked" id="__tabbed_1_1" name="__tabbed_1" type="radio"><label for="__tabbed_1_1">Serial Job in Normal Queue</label><div class="tabbed-content">
<pre class="highlight"><code class="language-job-script hljs"><span class="hljs-meta">#!/bin/bash</span>
<span class="hljs-comment">#----------------------------------------------------</span>
<span class="hljs-comment"># Sample Slurm job script</span>
<span class="hljs-comment">#   for TACC Stampede2 KNL nodes</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   *** Serial Job on Normal Queue ***</span>
<span class="hljs-comment"># </span>
<span class="hljs-comment"># Last revised: 20 Oct 2017</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment"># Notes:</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- Copy/edit this script as desired.  Launch by executing</span>
<span class="hljs-comment">#      "sbatch knl.serial.slurm" on a Stampede2 login node.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- Serial codes run on a single node (upper case N = 1).</span>
<span class="hljs-comment">#        A serial code ignores the value of lower case n,</span>
<span class="hljs-comment">#        but slurm needs a plausible value to schedule the job.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- For a good way to run multiple serial executables at the</span>
<span class="hljs-comment">#        same time, execute "module load launcher" followed</span>
<span class="hljs-comment">#        by "module help launcher".</span>

<span class="hljs-comment">#----------------------------------------------------</span>

<span class="hljs-comment">#SBATCH -J myjob           # Job name</span>
<span class="hljs-comment">#SBATCH -o myjob.o%j       # Name of stdout output file</span>
<span class="hljs-comment">#SBATCH -e myjob.e%j       # Name of stderr error file</span>
<span class="hljs-comment">#SBATCH -p normal          # Queue (partition) name</span>
<span class="hljs-comment">#SBATCH -N 1               # Total # of nodes (must be 1 for serial)</span>
<span class="hljs-comment">#SBATCH -n 1               # Total # of mpi tasks (should be 1 for serial)</span>
<span class="hljs-comment">#SBATCH -t 01:30:00        # Run time (hh:mm:ss)</span>
<span class="hljs-comment">#SBATCH --mail-user=username@tacc.utexas.edu</span>
<span class="hljs-comment">#SBATCH --mail-type=all    # Send email at begin and end of job</span>
<span class="hljs-comment">#SBATCH -A myproject       # Allocation name (req'd if you have more than 1)</span>

<span class="hljs-comment"># Other commands must follow all #SBATCH directives...</span>

module list
<span class="hljs-built_in">pwd</span>
<span class="hljs-built_in">date</span>

<span class="hljs-comment"># Launch serial code...</span>

./myprogram         <span class="hljs-comment"># Do not use ibrun or any other MPI launcher</span>

<span class="hljs-comment"># ---------------------------------------------------</span></code></pre>
</div>
<input id="__tabbed_1_2" name="__tabbed_1" type="radio"><label for="__tabbed_1_2">MPI Job in Normal Queue</label><div class="tabbed-content">
<pre class="highlight"><code class="language-job-script hljs"><span class="hljs-meta">#!/bin/bash</span>
<span class="hljs-comment">#----------------------------------------------------</span>
<span class="hljs-comment"># Sample Slurm job script</span>
<span class="hljs-comment">#   for TACC Stampede2 KNL nodes</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   *** MPI Job on Normal Queue ***</span>
<span class="hljs-comment"># </span>
<span class="hljs-comment"># Last revised: 20 Oct 2017</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment"># Notes:</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- Launch this script by executing</span>
<span class="hljs-comment">#      "sbatch knl.mpi.slurm" on Stampede2 login node.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- Use ibrun to launch MPI codes on TACC systems.</span>
<span class="hljs-comment">#      Do not use mpirun or mpiexec.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- Max recommended MPI tasks per KNL node: 64-68</span>
<span class="hljs-comment">#      (start small, increase gradually).</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- If you're running out of memory, try running</span>
<span class="hljs-comment">#      fewer tasks per node to give each task more memory.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#----------------------------------------------------</span>

<span class="hljs-comment">#SBATCH -J myjob           # Job name</span>
<span class="hljs-comment">#SBATCH -o myjob.o%j       # Name of stdout output file</span>
<span class="hljs-comment">#SBATCH -e myjob.e%j       # Name of stderr error file</span>
<span class="hljs-comment">#SBATCH -p normal          # Queue (partition) name</span>
<span class="hljs-comment">#SBATCH -N 4               # Total # of nodes </span>
<span class="hljs-comment">#SBATCH -n 32              # Total # of mpi tasks</span>
<span class="hljs-comment">#SBATCH -t 01:30:00        # Run time (hh:mm:ss)</span>
<span class="hljs-comment">#SBATCH --mail-user=username@tacc.utexas.edu</span>
<span class="hljs-comment">#SBATCH --mail-type=all    # Send email at begin and end of job</span>
<span class="hljs-comment">#SBATCH -A myproject       # Allocation name (req'd if you have more than 1)</span>

<span class="hljs-comment"># Other commands must follow all #SBATCH directives...</span>

module list
<span class="hljs-built_in">pwd</span>
<span class="hljs-built_in">date</span>

<span class="hljs-comment"># Launch MPI code... </span>

ibrun ./myprogram         <span class="hljs-comment"># Use ibrun instead of mpirun or mpiexec</span>

<span class="hljs-comment"># ---------------------------------------------------</span></code></pre>
</div>
<input id="__tabbed_1_3" name="__tabbed_1" type="radio"><label for="__tabbed_1_3">OpenMP Job in Normal Queue</label><div class="tabbed-content">
<pre class="highlight"><code class="language-job-script hljs"><span class="hljs-meta">#!/bin/bash</span>
<span class="hljs-comment">#----------------------------------------------------</span>
<span class="hljs-comment"># Sample Slurm job script</span>
<span class="hljs-comment">#   for TACC Stampede2 KNL nodes</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   *** OpenMP Job on Normal Queue ***</span>
<span class="hljs-comment"># </span>
<span class="hljs-comment"># Last revised: 20 Oct 2017</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment"># Notes:</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- Launch this script by executing</span>
<span class="hljs-comment">#   -- Copy/edit this script as desired.  Launch by executing</span>
<span class="hljs-comment">#      "sbatch knl.openmp.slurm" on a Stampede2 login node.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- OpenMP codes run on a single node (upper case N = 1).</span>
<span class="hljs-comment">#        OpenMP ignores the value of lower case n,</span>
<span class="hljs-comment">#        but slurm needs a plausible value to schedule the job.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- Default value of OMP_NUM_THREADS is 1; be sure to change it!</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- Increase thread count gradually while looking for optimal setting.</span>
<span class="hljs-comment">#        If there is sufficient memory available, the optimal setting</span>
<span class="hljs-comment">#        is often 68 (1 thread per core) or 136 (2 threads per core).</span>

<span class="hljs-comment">#----------------------------------------------------</span>

<span class="hljs-comment">#SBATCH -J myjob           # Job name</span>
<span class="hljs-comment">#SBATCH -o myjob.o%j       # Name of stdout output file</span>
<span class="hljs-comment">#SBATCH -e myjob.e%j       # Name of stderr error file</span>
<span class="hljs-comment">#SBATCH -p normal          # Queue (partition) name</span>
<span class="hljs-comment">#SBATCH -N 1               # Total # of nodes (must be 1 for OpenMP)</span>
<span class="hljs-comment">#SBATCH -n 1               # Total # of mpi tasks (should be 1 for OpenMP)</span>
<span class="hljs-comment">#SBATCH -t 01:30:00        # Run time (hh:mm:ss)</span>
<span class="hljs-comment">#SBATCH --mail-user=username@tacc.utexas.edu</span>
<span class="hljs-comment">#SBATCH --mail-type=all    # Send email at begin and end of job</span>
<span class="hljs-comment">#SBATCH -A myproject       # Allocation name (req'd if you have more than 1)</span>

<span class="hljs-comment"># Other commands must follow all #SBATCH directives...</span>

module list
<span class="hljs-built_in">pwd</span>
<span class="hljs-built_in">date</span>

<span class="hljs-comment"># Set thread count (default value is 1)...</span>

<span class="hljs-built_in">export</span> OMP_NUM_THREADS=34

<span class="hljs-comment"># Launch OpenMP code...</span>

./myprogram         <span class="hljs-comment"># Do not use ibrun or any other MPI launcher</span>

<span class="hljs-comment"># ---------------------------------------------------</span></code></pre>
</div>
<input id="__tabbed_1_4" name="__tabbed_1" type="radio"><label for="__tabbed_1_4">Hybrid Job in Normal Queue</label><div class="tabbed-content">
<pre class="highlight"><code class="language-job-script hljs"><span class="hljs-meta">#!/bin/bash</span>
<span class="hljs-comment">#----------------------------------------------------</span>
<span class="hljs-comment"># Example Slurm job script</span>
<span class="hljs-comment"># for TACC Stampede2 KNL nodes</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   *** Hybrid Job on Normal Queue ***</span>
<span class="hljs-comment"># </span>
<span class="hljs-comment">#       This sample script specifies:</span>
<span class="hljs-comment">#         10 nodes (capital N)</span>
<span class="hljs-comment">#         40 total MPI tasks (lower case n); this is 4 tasks/node</span>
<span class="hljs-comment">#         16 OpenMP threads per MPI task (64 threads per node)</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment"># Last revised: 20 Oct 2017</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment"># Notes:</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- Launch this script by executing</span>
<span class="hljs-comment">#      "sbatch knl.hybrid.slurm" on Stampede2 login node.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- Use ibrun to launch MPI codes on TACC systems.</span>
<span class="hljs-comment">#      Do not use mpirun or mpiexec.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- In most cases it's best to specify no more </span>
<span class="hljs-comment">#      than 64-68 MPI ranks or independent processes </span>
<span class="hljs-comment">#      per node, and 1-2 threads/core. </span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- If you're running out of memory, try running</span>
<span class="hljs-comment">#      fewer tasks and/or threads per node to give each </span>
<span class="hljs-comment">#      process access to more memory.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- IMPI and MVAPICH2 both do sensible process pinning by default.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#----------------------------------------------------</span>

<span class="hljs-comment">#SBATCH -J myjob           # Job name</span>
<span class="hljs-comment">#SBATCH -o myjob.o%j       # Name of stdout output file</span>
<span class="hljs-comment">#SBATCH -e myjob.e%j       # Name of stderr error file</span>
<span class="hljs-comment">#SBATCH -p normal          # Queue (partition) name</span>
<span class="hljs-comment">#SBATCH -N 10              # Total # of nodes </span>
<span class="hljs-comment">#SBATCH -n 40              # Total # of mpi tasks</span>
<span class="hljs-comment">#SBATCH -t 01:30:00        # Run time (hh:mm:ss)</span>
<span class="hljs-comment">#SBATCH --mail-user=username@tacc.utexas.edu</span>
<span class="hljs-comment">#SBATCH --mail-type=all    # Send email at begin and end of job</span>
<span class="hljs-comment">#SBATCH -A myproject       # Allocation name (req'd if you have more than 1)</span>

<span class="hljs-comment"># Other commands must follow all #SBATCH directives...</span>

module list
<span class="hljs-built_in">pwd</span>
<span class="hljs-built_in">date</span>

<span class="hljs-comment"># Set thread count (default value is 1)...</span>

<span class="hljs-built_in">export</span> OMP_NUM_THREADS=16

<span class="hljs-comment"># Launch MPI code... </span>

ibrun ./myprogram         <span class="hljs-comment"># Use ibrun instead of mpirun or mpiexec</span>

<span class="hljs-comment"># ---------------------------------------------------</span></code></pre>
</div>
</div>
<h3 id="scripts-skx"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#scripts-skx">SKX Nodes</a></h3>
<p>Click on a tab for a customizable job-script.</p>
<div class="tabbed-set" data-tabs="2:4"><input checked="checked" id="__tabbed_2_1" name="__tabbed_2" type="radio"><label for="__tabbed_2_1">Serial Job in Normal Queue</label><div class="tabbed-content">
<pre class="highlight"><code class="language-job-script hljs"><span class="hljs-meta">#!/bin/bash</span>
<span class="hljs-comment">#----------------------------------------------------</span>
<span class="hljs-comment"># Sample Slurm job script</span>
<span class="hljs-comment">#   for TACC Stampede2 SKX nodes</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   *** Serial Job on SKX Normal Queue ***</span>
<span class="hljs-comment"># </span>
<span class="hljs-comment"># Last revised: 20 Oct 2017</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment"># Notes:</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- Copy/edit this script as desired.  Launch by executing</span>
<span class="hljs-comment">#      "sbatch skx.serial.slurm" on a Stampede2 login node.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- Serial codes run on a single node (upper case N = 1).</span>
<span class="hljs-comment">#        A serial code ignores the value of lower case n,</span>
<span class="hljs-comment">#        but slurm needs a plausible value to schedule the job.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- For a good way to run multiple serial executables at the</span>
<span class="hljs-comment">#        same time, execute "module load launcher" followed</span>
<span class="hljs-comment">#        by "module help launcher".</span>

<span class="hljs-comment">#----------------------------------------------------</span>

<span class="hljs-comment">#SBATCH -J myjob           # Job name</span>
<span class="hljs-comment">#SBATCH -o myjob.o%j       # Name of stdout output file</span>
<span class="hljs-comment">#SBATCH -e myjob.e%j       # Name of stderr error file</span>
<span class="hljs-comment">#SBATCH -p skx-normal      # Queue (partition) name</span>
<span class="hljs-comment">#SBATCH -N 1               # Total # of nodes (must be 1 for serial)</span>
<span class="hljs-comment">#SBATCH -n 1               # Total # of mpi tasks (should be 1 for serial)</span>
<span class="hljs-comment">#SBATCH -t 01:30:00        # Run time (hh:mm:ss)</span>
<span class="hljs-comment">#SBATCH --mail-user=username@tacc.utexas.edu</span>
<span class="hljs-comment">#SBATCH --mail-type=all    # Send email at begin and end of job</span>
<span class="hljs-comment">#SBATCH -A myproject       # Allocation name (req'd if you have more than 1)</span>

<span class="hljs-comment"># Other commands must follow all #SBATCH directives...</span>

module list
<span class="hljs-built_in">pwd</span>
<span class="hljs-built_in">date</span>

<span class="hljs-comment"># Launch serial code...</span>

./myprogram         <span class="hljs-comment"># Do not use ibrun or any other MPI launcher</span>

<span class="hljs-comment"># ---------------------------------------------------</span></code></pre>
</div>
<input id="__tabbed_2_2" name="__tabbed_2" type="radio"><label for="__tabbed_2_2">MPI Job in Normal Queue</label><div class="tabbed-content">
<pre class="highlight"><code class="language-job-script hljs"><span class="hljs-meta">#!/bin/bash</span>
<span class="hljs-comment">#----------------------------------------------------</span>
<span class="hljs-comment"># Sample Slurm job script</span>
<span class="hljs-comment">#   for TACC Stampede2 SKX nodes</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   *** MPI Job on SKX Normal Queue ***</span>
<span class="hljs-comment"># </span>
<span class="hljs-comment"># Last revised: 20 Oct 2017</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment"># Notes:</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- Launch this script by executing</span>
<span class="hljs-comment">#      "sbatch skx.mpi.slurm" on Stampede2 login node.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- Use ibrun to launch MPI codes on TACC systems.</span>
<span class="hljs-comment">#      Do not use mpirun or mpiexec.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- Max recommended MPI ranks per SKX node: 48</span>
<span class="hljs-comment">#      (start small, increase gradually).</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- If you're running out of memory, try running</span>
<span class="hljs-comment">#      fewer tasks per node to give each task more memory.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#----------------------------------------------------</span>

<span class="hljs-comment">#SBATCH -J myjob           # Job name</span>
<span class="hljs-comment">#SBATCH -o myjob.o%j       # Name of stdout output file</span>
<span class="hljs-comment">#SBATCH -e myjob.e%j       # Name of stderr error file</span>
<span class="hljs-comment">#SBATCH -p skx-normal      # Queue (partition) name</span>
<span class="hljs-comment">#SBATCH -N 4               # Total # of nodes </span>
<span class="hljs-comment">#SBATCH -n 32              # Total # of mpi tasks</span>
<span class="hljs-comment">#SBATCH -t 01:30:00        # Run time (hh:mm:ss)</span>
<span class="hljs-comment">#SBATCH --mail-user=username@tacc.utexas.edu</span>
<span class="hljs-comment">#SBATCH --mail-type=all    # Send email at begin and end of job</span>
<span class="hljs-comment">#SBATCH -A myproject       # Allocation name (req'd if you have more than 1)</span>

<span class="hljs-comment"># Other commands must follow all #SBATCH directives...</span>

module list
<span class="hljs-built_in">pwd</span>
<span class="hljs-built_in">date</span>

<span class="hljs-comment"># Launch MPI code... </span>

ibrun ./myprogram         <span class="hljs-comment"># Use ibrun instead of mpirun or mpiexec</span>

<span class="hljs-comment"># ---------------------------------------------------</span></code></pre>
</div>
<input id="__tabbed_2_3" name="__tabbed_2" type="radio"><label for="__tabbed_2_3">OpenMP Job in Normal Queue</label><div class="tabbed-content">
<pre class="highlight"><code class="language-job-script hljs"><span class="hljs-meta">#!/bin/bash</span>
<span class="hljs-comment">#----------------------------------------------------</span>
<span class="hljs-comment"># Sample Slurm job script</span>
<span class="hljs-comment">#   for TACC Stampede2 SKX nodes</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   *** OpenMP Job on SKX Normal Queue ***</span>
<span class="hljs-comment"># </span>
<span class="hljs-comment"># Last revised: 20 Oct 2017</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment"># Notes:</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- Launch this script by executing</span>
<span class="hljs-comment">#   -- Copy/edit this script as desired.  Launch by executing</span>
<span class="hljs-comment">#      "sbatch skx.openmp.slurm" on a Stampede2 login node.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- OpenMP codes run on a single node (upper case N = 1).</span>
<span class="hljs-comment">#        OpenMP ignores the value of lower case n,</span>
<span class="hljs-comment">#        but slurm needs a plausible value to schedule the job.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- Default value of OMP_NUM_THREADS is 1; be sure to change it!</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- Increase thread count gradually while looking for optimal setting.</span>
<span class="hljs-comment">#        If there is sufficient memory available, the optimal setting</span>
<span class="hljs-comment">#        is often 48 (1 thread per core) but may be higher.</span>

<span class="hljs-comment">#----------------------------------------------------</span>

<span class="hljs-comment">#SBATCH -J myjob           # Job name</span>
<span class="hljs-comment">#SBATCH -o myjob.o%j       # Name of stdout output file</span>
<span class="hljs-comment">#SBATCH -e myjob.e%j       # Name of stderr error file</span>
<span class="hljs-comment">#SBATCH -p skx-normal      # Queue (partition) name</span>
<span class="hljs-comment">#SBATCH -N 1               # Total # of nodes (must be 1 for OpenMP)</span>
<span class="hljs-comment">#SBATCH -n 1               # Total # of mpi tasks (should be 1 for OpenMP)</span>
<span class="hljs-comment">#SBATCH -t 01:30:00        # Run time (hh:mm:ss)</span>
<span class="hljs-comment">#SBATCH --mail-user=username@tacc.utexas.edu</span>
<span class="hljs-comment">#SBATCH --mail-type=all    # Send email at begin and end of job</span>
<span class="hljs-comment">#SBATCH -A myproject       # Allocation name (req'd if you have more than 1)</span>

<span class="hljs-comment"># Other commands must follow all #SBATCH directives...</span>

module list
<span class="hljs-built_in">pwd</span>
<span class="hljs-built_in">date</span>

<span class="hljs-comment"># Set thread count (default value is 1)...</span>

<span class="hljs-built_in">export</span> OMP_NUM_THREADS=48   <span class="hljs-comment"># this is 1 thread/core; may want to start lower</span>

<span class="hljs-comment"># Launch OpenMP code...</span>

./myprogram         <span class="hljs-comment"># Do not use ibrun or any other MPI launcher</span>

<span class="hljs-comment"># ---------------------------------------------------</span></code></pre>
</div>
<input id="__tabbed_2_4" name="__tabbed_2" type="radio"><label for="__tabbed_2_4">Hybrid Job in Normal Queue</label><div class="tabbed-content">
<pre class="highlight"><code class="language-job-script hljs"><span class="hljs-meta">#!/bin/bash</span>
<span class="hljs-comment">#----------------------------------------------------</span>
<span class="hljs-comment"># Example Slurm job script</span>
<span class="hljs-comment"># for TACC Stampede2 SKX nodes</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   *** Hybrid Job on SKX Normal Queue ***</span>
<span class="hljs-comment"># </span>
<span class="hljs-comment">#       This sample script specifies:</span>
<span class="hljs-comment">#         10 nodes (capital N)</span>
<span class="hljs-comment">#         40 total MPI tasks (lower case n); this is 4 tasks/node</span>
<span class="hljs-comment">#         12 OpenMP threads per MPI task (48 threads per node)</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment"># Last revised: 20 Oct 2017</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment"># Notes:</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- Launch this script by executing</span>
<span class="hljs-comment">#      "sbatch skx.mpi.slurm" on Stampede2 login node.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- Use ibrun to launch MPI codes on TACC systems.</span>
<span class="hljs-comment">#      Do not use mpirun or mpiexec.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- In most cases it's best to keep</span>
<span class="hljs-comment">#      ( MPI ranks per node ) x ( threads per rank )</span>
<span class="hljs-comment">#      to a number no more than 48 (total cores).</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- If you're running out of memory, try running</span>
<span class="hljs-comment">#      fewer tasks and/or threads per node to give each </span>
<span class="hljs-comment">#      process access to more memory.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- IMPI and MVAPICH2 both do sensible process pinning by default.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#----------------------------------------------------</span>

<span class="hljs-comment">#SBATCH -J myjob           # Job name</span>
<span class="hljs-comment">#SBATCH -o myjob.o%j       # Name of stdout output file</span>
<span class="hljs-comment">#SBATCH -e myjob.e%j       # Name of stderr error file</span>
<span class="hljs-comment">#SBATCH -p skx-normal      # Queue (partition) name</span>
<span class="hljs-comment">#SBATCH -N 10              # Total # of nodes </span>
<span class="hljs-comment">#SBATCH -n 40              # Total # of mpi tasks</span>
<span class="hljs-comment">#SBATCH -t 01:30:00        # Run time (hh:mm:ss)</span>
<span class="hljs-comment">#SBATCH --mail-user=username@tacc.utexas.edu</span>
<span class="hljs-comment">#SBATCH --mail-type=all    # Send email at begin and end of job</span>
<span class="hljs-comment">#SBATCH -A myproject       # Allocation name (req'd if you have more than 1)</span>

<span class="hljs-comment"># Other commands must follow all #SBATCH directives...</span>

module list
<span class="hljs-built_in">pwd</span>
<span class="hljs-built_in">date</span>

<span class="hljs-comment"># Set thread count (default value is 1)...</span>

<span class="hljs-built_in">export</span> OMP_NUM_THREADS=12

<span class="hljs-comment"># Launch MPI code... </span>

ibrun ./myprogram         <span class="hljs-comment"># Use ibrun instead of mpirun or mpiexec</span>

<span class="hljs-comment"># ---------------------------------------------------</span></code></pre>
</div>
</div>
<h3 id="scripts-icx"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#scripts-icx">ICX Nodes</a></h3>
<p>Click on a tab for a customizable job-script.</p>
<div class="tabbed-set" data-tabs="3:3"><input checked="checked" id="__tabbed_3_1" name="__tabbed_3" type="radio"><label for="__tabbed_3_1">MPI Job in Normal Queue</label><div class="tabbed-content">
<pre class="highlight"><code class="language-job-script hljs"><span class="hljs-meta">#!/bin/bash</span>
<span class="hljs-comment">#----------------------------------------------------</span>
<span class="hljs-comment"># Sample Slurm job script</span>
<span class="hljs-comment">#   for TACC Stampede2 ICX nodes</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   *** MPI Job on ICX Normal Queue ***</span>
<span class="hljs-comment"># </span>
<span class="hljs-comment"># Last revised: 09 Mar 2022</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment"># Notes:</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- Launch this script by executing</span>
<span class="hljs-comment">#      "sbatch icx.mpi.slurm" on Stampede2 login node.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- Use ibrun to launch MPI codes on TACC systems.</span>
<span class="hljs-comment">#      Do not use mpirun or mpiexec.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- Max recommended MPI ranks per ICX node: 80</span>
<span class="hljs-comment">#      (start small, increase gradually).</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- If you're running out of memory, try running</span>
<span class="hljs-comment">#      on more nodes using fewer tasks and/or threads </span>
<span class="hljs-comment">#      per node to give each task access to more memory.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- Don't worry about task layout.  By default, ibrun</span>
<span class="hljs-comment">#      will provide proper affinity and pinning.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- You should always run out of $SCRATCH.  Your input</span>
<span class="hljs-comment">#      files, output files, and exectuable should be </span>
<span class="hljs-comment">#      in the $SCRATCH directory hierarchy.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#----------------------------------------------------</span>

<span class="hljs-comment">#SBATCH -J myjob           # Job name</span>
<span class="hljs-comment">#SBATCH -o myjob.o%j       # Name of stdout output file</span>
<span class="hljs-comment">#SBATCH -e myjob.e%j       # Name of stderr error file</span>
<span class="hljs-comment">#SBATCH -p icx-normal      # Queue (partition) name</span>
<span class="hljs-comment">#SBATCH -N 4               # Total # of nodes </span>
<span class="hljs-comment">#SBATCH -n 320             # Total # of mpi tasks</span>
<span class="hljs-comment">#SBATCH -t 01:30:00        # Run time (hh:mm:ss)</span>
<span class="hljs-comment">#SBATCH --mail-user=myname@myschool.edu</span>
<span class="hljs-comment">#SBATCH --mail-type=all    # Send email at begin and end of job</span>
<span class="hljs-comment">#SBATCH -A myproject       # Allocation name (req'd if you have more than 1)</span>

<span class="hljs-comment"># Other commands must follow all #SBATCH directives...</span>

module list
<span class="hljs-built_in">pwd</span>
<span class="hljs-built_in">date</span>

<span class="hljs-comment"># You should always run out of $SCRATCH.  Your input</span>
<span class="hljs-comment">#      files, output files, and exectuable should be </span>
<span class="hljs-comment">#      in the $SCRATCH directory hierarchy.</span>
<span class="hljs-comment"># Change directories to your $SCRATCH directory where your executable is</span>

<span class="hljs-built_in">cd</span> <span class="hljs-variable">$SCRATCH</span>

<span class="hljs-comment"># Launch MPI code... </span>

ibrun ./myprogram         <span class="hljs-comment"># Use ibrun instead of mpirun or mpiexec</span>

<span class="hljs-comment">#---------------------------------------------------</span></code></pre>
</div>
<input id="__tabbed_3_2" name="__tabbed_3" type="radio"><label for="__tabbed_3_2">OpenMP Job in Normal Queue</label><div class="tabbed-content">
<pre class="highlight"><code class="language-job-script hljs"><span class="hljs-meta">#!/bin/bash</span>
<span class="hljs-comment">#----------------------------------------------------</span>
<span class="hljs-comment"># Sample Slurm job script</span>
<span class="hljs-comment">#   for TACC Stampede2 ICX nodes</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   *** OpenMP Job on ICX Normal Queue ***</span>
<span class="hljs-comment"># </span>
<span class="hljs-comment"># Last revised: 09 Mar 2022</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment"># Notes:</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- Launch this script by executing</span>
<span class="hljs-comment">#   -- Copy/edit this script as desired.  Launch by executing</span>
<span class="hljs-comment">#      "sbatch icx.openmp.slurm" on a Stampede2 login node.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- OpenMP codes run on a single node (upper case N = 1).</span>
<span class="hljs-comment">#        OpenMP ignores the value of lower case n,</span>
<span class="hljs-comment">#        but slurm needs a plausible value to schedule the job.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- Default value of OMP_NUM_THREADS is 1; be sure to change it!</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- Increase thread count gradually while looking for optimal setting.</span>
<span class="hljs-comment">#        If there is sufficient memory available, the optimal setting</span>
<span class="hljs-comment">#        is often 80 (1 thread per core) but may be higher.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- You should always run out of $SCRATCH.  Your input</span>
<span class="hljs-comment">#      files, output files, and exectuable should be </span>
<span class="hljs-comment">#      in the $SCRATCH directory hierarchy.</span>
<span class="hljs-comment">#</span>

<span class="hljs-comment">#----------------------------------------------------</span>

<span class="hljs-comment">#SBATCH -J myjob           # Job name</span>
<span class="hljs-comment">#SBATCH -o myjob.o%j       # Name of stdout output file</span>
<span class="hljs-comment">#SBATCH -e myjob.e%j       # Name of stderr error file</span>
<span class="hljs-comment">#SBATCH -p icx-normal      # Queue (partition) name</span>
<span class="hljs-comment">#SBATCH -N 1               # Total # of nodes (must be 1 for OpenMP)</span>
<span class="hljs-comment">#SBATCH -n 1               # Total # of mpi tasks (should be 1 for OpenMP)</span>
<span class="hljs-comment">#SBATCH -t 01:30:00        # Run time (hh:mm:ss)</span>
<span class="hljs-comment">#SBATCH --mail-user=myname@myschool.edu</span>
<span class="hljs-comment">#SBATCH --mail-type=all    # Send email at begin and end of job</span>
<span class="hljs-comment">#SBATCH -A myproject       # Allocation name (req'd if you have more than 1)</span>

<span class="hljs-comment"># Other commands must follow all #SBATCH directives...</span>

module list
<span class="hljs-built_in">pwd</span>
<span class="hljs-built_in">date</span>

<span class="hljs-comment"># Set thread count (default value is 1)...</span>

<span class="hljs-built_in">export</span> OMP_NUM_THREADS=80   <span class="hljs-comment"># this is 1 thread/core; may want to start lower</span>

<span class="hljs-comment"># You should always run out of $SCRATCH.  Your input</span>
<span class="hljs-comment">#      files, output files, and exectuable should be </span>
<span class="hljs-comment">#      in the $SCRATCH directory hierarchy.</span>
<span class="hljs-comment"># Change directories to your $SCRATCH directory where your executable is</span>

<span class="hljs-built_in">cd</span> <span class="hljs-variable">$SCRATCH</span>

<span class="hljs-comment"># Launch OpenMP code...</span>

./myprogram         <span class="hljs-comment"># Do not use ibrun or any other MPI launcher</span>

<span class="hljs-comment">#---------------------------------------------------</span></code></pre>
</div>
<input id="__tabbed_3_3" name="__tabbed_3" type="radio"><label for="__tabbed_3_3">Hybrid Job in Normal Queue</label><div class="tabbed-content">
<pre class="highlight"><code class="language-job-script hljs"><span class="hljs-meta">#!/bin/bash</span>
<span class="hljs-comment">#----------------------------------------------------</span>
<span class="hljs-comment"># Example Slurm job script</span>
<span class="hljs-comment"># for TACC Stampede2 ICX nodes</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   *** Hybrid Job on ICX Normal Queue ***</span>
<span class="hljs-comment"># </span>
<span class="hljs-comment">#       This sample script specifies:</span>
<span class="hljs-comment">#         10 nodes (capital N)</span>
<span class="hljs-comment">#         40 total MPI tasks (lower case n); this is 4 tasks/node</span>
<span class="hljs-comment">#         20 OpenMP threads per MPI task (80 threads per node)</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment"># Last revised: 09 Mar 2022</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment"># Notes:</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- Launch this script by executing</span>
<span class="hljs-comment">#      "sbatch icx.mpi.slurm" on Stampede2 login node.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- Use ibrun to launch MPI codes on TACC systems.</span>
<span class="hljs-comment">#      Do not use mpirun or mpiexec.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- In most cases it's best to keep</span>
<span class="hljs-comment">#      ( MPI ranks per node ) x ( threads per rank )</span>
<span class="hljs-comment">#      to a number no more than 48 (total cores).</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- If you're running out of memory, try running</span>
<span class="hljs-comment">#      fewer tasks and/or threads per node to give each </span>
<span class="hljs-comment">#      process access to more memory.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- If you're running out of memory, try running</span>
<span class="hljs-comment">#      on more nodes using fewer tasks and/or threads </span>
<span class="hljs-comment">#      per node to give each task access to more memory.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- Don't worry about task layout.  By default, ibrun</span>
<span class="hljs-comment">#      will provide proper affinity and pinning.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#   -- You should always run out of $SCRATCH.  Your input</span>
<span class="hljs-comment">#      files, output files, and exectuable should be </span>
<span class="hljs-comment">#      in the $SCRATCH directory hierarchy.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#----------------------------------------------------</span>

<span class="hljs-comment">#SBATCH -J myjob           # Job name</span>
<span class="hljs-comment">#SBATCH -o myjob.o%j       # Name of stdout output file</span>
<span class="hljs-comment">#SBATCH -e myjob.e%j       # Name of stderr error file</span>
<span class="hljs-comment">#SBATCH -p icx-normal      # Queue (partition) name</span>
<span class="hljs-comment">#SBATCH -N 10              # Total # of nodes </span>
<span class="hljs-comment">#SBATCH -n 40              # Total # of mpi tasks</span>
<span class="hljs-comment">#SBATCH -t 01:30:00        # Run time (hh:mm:ss)</span>
<span class="hljs-comment">#SBATCH --mail-user=myname@myschool.edu</span>
<span class="hljs-comment">#SBATCH --mail-type=all    # Send email at begin and end of job</span>
<span class="hljs-comment">#SBATCH -A myproject       # Allocation name (req'd if you have more than 1)</span>

<span class="hljs-comment"># Other commands must follow all #SBATCH directives...</span>

module list
<span class="hljs-built_in">pwd</span>
<span class="hljs-built_in">date</span>

<span class="hljs-comment"># Set thread count (default value is 1)...</span>

<span class="hljs-built_in">export</span> OMP_NUM_THREADS=20

<span class="hljs-comment"># You should always run out of $SCRATCH.  Your input</span>
<span class="hljs-comment">#      files, output files, and exectuable should be </span>
<span class="hljs-comment">#      in the $SCRATCH directory hierarchy.</span>
<span class="hljs-comment"># Change directories to your $SCRATCH directory where your executable is</span>

<span class="hljs-built_in">cd</span> <span class="hljs-variable">$SCRATCH</span>

<span class="hljs-comment"># Launch MPI code... </span>

ibrun ./myprogram         <span class="hljs-comment"># Use ibrun instead of mpirun or mpiexec</span>
<span class="hljs-comment">#---------------------------------------------------</span></code></pre>
</div>
</div>
<h2 id="monitoring"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#monitoring">Monitoring Jobs and Queues</a></h2>
<p>Several commands are available to help you plan and track your job submissions as well as check the status of the Slurm queues.</p>
<p>When interpreting queue and job status, remember that <strong>Stampede2 doesn't operate on a first-come-first-served basis</strong>. Instead, the sophisticated, tunable algorithms built into Slurm attempt to keep the system busy, while scheduling jobs in a way that is as fair as possible to everyone. At times this means leaving nodes idle ("draining the queue") to make room for a large job that would otherwise never run. It also means considering each user's "fair share", scheduling jobs so that those who haven't run jobs recently may have a slightly higher priority than those who have.</p>
<h3 id="monitoring-queue"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#monitoring-queue">Monitoring Queue Status with <code>sinfo</code> and <code>qlimits</code></a></h3>
<p>To display resource limits for the Stampede2 queues, execute "<strong><code>qlimits</code></strong>". The result is real-time data; the corresponding information in this document's <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-queues">table of Stampede2 queues</a> may lag behind the actual configuration that the <code>qlimits</code> utility displays.</p>
<p>Slurm's "<strong><code>sinfo</code></strong>" command allows you to monitor the status of the queues. If you execute <code>sinfo</code> without arguments, you'll see a list of every node in the system together with its status. To skip the node list and produce a tight, alphabetized summary of the available queues and their status, execute:</p>
<pre class="highlight"><code class="language-cmd-line hljs">login1<span class="hljs-variable">$ </span>sinfo -S+P -o <span class="hljs-string">"%18P %8a %20F"</span>    <span class="hljs-comment"># compact summary of queue status</span></code></pre>
<p>An excerpt from this command's output looks like this:</p>
<pre class="highlight"><code class="language-cmd-line hljs">
<span class="hljs-variable constant_">PARTITION</span>          <span class="hljs-variable constant_">AVAIL</span>    <span class="hljs-variable constant_">NODES</span>(A/I/O/T)
development*       up       <span class="hljs-number">41</span>/<span class="hljs-number">70</span>/<span class="hljs-number">1</span>/<span class="hljs-number">112</span>
normal             up       <span class="hljs-number">3685</span>/<span class="hljs-number">8</span>/<span class="hljs-number">3</span>/<span class="hljs-number">3696</span></code></pre>
<p>The <code>AVAIL</code> column displays the overall status of each queue (up or down), while the column labeled <code>NODES(A/I/O/T)</code> shows the number of nodes in each of several states ("<strong>A</strong>llocated", "<strong>I</strong>dle", "<strong>O</strong>ffline", and "<strong>T</strong>otal"). Execute <code>man sinfo</code> for more information. Use caution when reading the generic documentation, however: some available fields are not meaningful or are misleading on Stampede2 (e.g. <code>TIMELIMIT</code>, displayed using the <code>%l</code> option).</p>
<h3 id="monitoring-squeue"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#monitoring-squeue">Monitoring Job Status with <code>squeue</code></a></h3>
<p>Slurm's <code>squeue</code> command allows you to monitor jobs in the queues, whether pending (waiting) or currently running:</p>
<pre class="highlight"><code class="language-cmd-line hljs">
login1<span class="hljs-variable">$ </span>squeue             <span class="hljs-comment"># show all jobs in all queues</span>
login1<span class="hljs-variable">$ </span>squeue -u bjones   <span class="hljs-comment"># show all jobs owned by bjones</span>
login1<span class="hljs-variable">$ </span>man squeue         <span class="hljs-comment"># more info</span></code></pre>
<p>An excerpt from the default output looks like this:</p>
<pre class="highlight"><code class="language-cmd-line hljs">
 <span class="hljs-variable constant_">JOBID</span>   <span class="hljs-variable constant_">PARTITION</span>     <span class="hljs-variable constant_">NAME</span>     <span class="hljs-variable constant_">USER</span> <span class="hljs-variable constant_">ST</span>       <span class="hljs-variable constant_">TIME</span>  <span class="hljs-variable constant_">NODES</span> <span class="hljs-variable constant_">NODELIST</span>(<span class="hljs-variable constant_">REASON</span>)
<span class="hljs-number">170361</span>      normal   spec12   bjones <span class="hljs-variable constant_">PD</span>       <span class="hljs-number">0</span><span class="hljs-symbol">:</span><span class="hljs-number">00</span>     <span class="hljs-number">32</span> (<span class="hljs-title class_">Resources</span>)
<span class="hljs-number">170356</span>      normal    mal2d slindsey <span class="hljs-variable constant_">PD</span>       <span class="hljs-number">0</span><span class="hljs-symbol">:</span><span class="hljs-number">00</span>     <span class="hljs-number">30</span> (<span class="hljs-title class_">Priority</span>)
<span class="hljs-number">170204</span>      normal   rr2-a2 tg123456 <span class="hljs-variable constant_">PD</span>       <span class="hljs-number">0</span><span class="hljs-symbol">:</span><span class="hljs-number">00</span>      <span class="hljs-number">1</span> (<span class="hljs-title class_">Dependency</span>)
<span class="hljs-number">170250</span> development idv59074  aturing  R      <span class="hljs-number">29</span><span class="hljs-symbol">:</span><span class="hljs-number">30</span>      <span class="hljs-number">1</span> c455-<span class="hljs-number">044</span>
<span class="hljs-number">169669</span>      normal  <span class="hljs-number">04</span>-99a1  aturing <span class="hljs-variable constant_">CG</span>    <span class="hljs-number">2</span><span class="hljs-symbol">:</span><span class="hljs-number">47</span><span class="hljs-symbol">:</span><span class="hljs-number">47</span>      <span class="hljs-number">1</span> c425-<span class="hljs-number">003</span></code></pre>
<p>The column labeled <code>ST</code> displays each job's status: </p>
<ul>
<li><code>PD</code> means "Pending" (waiting); </li>
<li><code>R</code> means "Running";</li>
<li><code>CG</code> means "Completing" (cleaning up after exiting the job script).</li>
</ul>
<p>Pending jobs appear in order of decreasing priority. The last column includes a nodelist for running/completing jobs, or a reason for pending jobs. If you submit a job before a scheduled system maintenance period, and the job cannot complete before the maintenance begins, your job will run when the maintenance/reservation concludes. The <code>squeue</code> command will report <code>ReqNodeNotAvailable</code> ("Required Node Not Available"). The job will remain in the <code>PD</code> state until Stampede2 returns to production.</p>
<p>The default format for <code>squeue</code> now reports total nodes associated with a job rather than cores, tasks, or hardware threads. One reason for this change is clarity: the operating system sees each KNL node's 272 hardware threads (and each SKX node's 96 hardware threads) as "processors", and output based on that information can be ambiguous or otherwise difficult to interpret.</p>
<p>The default format lists all nodes assigned to displayed jobs; this can make the output difficult to read. A handy variation that suppresses the nodelist is:</p>
<pre class="highlight"><code class="language-cmd-line hljs">login1<span class="hljs-variable">$ </span>squeue -o <span class="hljs-string">"%.10i %.12P %.12j %.9u %.2t %.9M %.6D"</span>  <span class="hljs-comment"># suppress nodelist</span></code></pre>
<p>The <code>--start</code> option displays job start times, including very rough estimates for the expected start times of some pending jobs that are relatively high in the queue:</p>
<pre class="highlight"><code class="language-cmd-line hljs">login1<span class="hljs-variable">$ </span>squeue --start -j <span class="hljs-number">167635</span>     <span class="hljs-comment"># display estimated start time for job 167635</span></code></pre>
<h3 id="monitoring-showq"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#monitoring-showq">Monitoring Job Status with <code>showq</code></a></h3>
<p>TACC's <code>showq</code> utility mimics a tool that originated in the PBS project, and serves as a popular alternative to the Slurm <code>squeue</code> command:</p>
<pre class="highlight"><code class="language-cmd-line hljs">
login1<span class="hljs-variable">$ </span>showq            <span class="hljs-comment"># show all jobs; default format</span>
login1<span class="hljs-variable">$ </span>showq -u         <span class="hljs-comment"># show your own jobs</span>
login1<span class="hljs-variable">$ </span>showq -U bjones  <span class="hljs-comment"># show jobs associated with user bjones</span>
login1<span class="hljs-variable">$ </span>showq -h         <span class="hljs-comment"># more info</span></code></pre>
<p>The output groups jobs in four categories: <code>ACTIVE</code>, <code>WAITING</code>, <code>BLOCKED</code>, and <code>COMPLETING/ERRORED</code>. A <strong><code>BLOCKED</code></strong> job is one that cannot yet run due to temporary circumstances (e.g. a pending maintenance or other large reservation.).</p>
<p>If your waiting job cannot complete before a maintenance/reservation begins, <code>showq</code> will display its state as "<strong><code>WaitNod</code></strong> ("Waiting for Nodes"). The job will remain in this state until Stampede2 returns to production.</p>
<p>The default format for <code>showq</code> now reports total nodes associated with a job rather than cores, tasks, or hardware threads. One reason for this change is clarity: the operating system sees each KNL node's 272 hardware threads (and each SKX node's 96 hardware threads) as "processors", and output based on that information can be ambiguous or otherwise difficult to interpret.</p>
<h3 id="monitoring-other"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#monitoring-other">Other Job Management Commands (<code>scancel</code>, <code>scontrol</code>, and <code>sacct</code>)</a></h3>
<p><strong>It's not possible to add resources to a job (e.g. allow more time)</strong> once you've submitted the job to the queue.</p>
<p>To <strong>cancel</strong> a pending or running job, first determine its jobid, then use <code>scancel</code>:</p>
<pre class="highlight"><code class="language-cmd-line hljs">
login1<span class="hljs-variable">$ </span>squeue -u bjones    <span class="hljs-comment"># one way to determine jobid</span>
   <span class="hljs-variable constant_">JOBID</span>   <span class="hljs-variable constant_">PARTITION</span>     <span class="hljs-variable constant_">NAME</span>     <span class="hljs-variable constant_">USER</span> <span class="hljs-variable constant_">ST</span>       <span class="hljs-variable constant_">TIME</span>  <span class="hljs-variable constant_">NODES</span> <span class="hljs-variable constant_">NODELIST</span>(<span class="hljs-variable constant_">REASON</span>)
  <span class="hljs-number">170361</span>      normal   spec12   bjones <span class="hljs-variable constant_">PD</span>       <span class="hljs-number">0</span><span class="hljs-symbol">:</span><span class="hljs-number">00</span>     <span class="hljs-number">32</span> (<span class="hljs-title class_">Resources</span>)
login1<span class="hljs-variable">$ </span>scancel <span class="hljs-number">170361</span>      <span class="hljs-comment"># cancel job</span></code></pre>
<p>For <strong>detailed information</strong> about the configuration of a specific job, use <code>scontrol</code>:</p>
<pre class="highlight"><code class="language-cmd-line hljs">login1<span class="hljs-variable">$ </span>scontrol show job=<span class="hljs-number">170361</span></code></pre>
<p>To view some <strong>accounting data</strong> associated with your own jobs, use <code>sacct</code>:</p>
<pre class="highlight"><code class="language-cmd-line hljs">login1<span class="hljs-variable">$ </span>sacct --starttime <span class="hljs-number">2017</span>-08-<span class="hljs-number">01</span>  <span class="hljs-comment"># show jobs that started on or after this date</span></code></pre>
<h3 id="monitoring-dependent"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#monitoring-dependent">Dependent Jobs using <code>sbatch</code></a></h3>
<p>You can use <code>sbatch</code> to help manage workflows that involve multiple steps: the <code>--dependency</code> option allows you to launch jobs that depend on the completion (or successful completion) of another job. For example you could use this technique to split into three jobs a workflow that requires you to (1) compile on a single node; then (2) compute on 40 nodes; then finally (3) post-process your results using 4 nodes. </p>
<pre class="highlight"><code class="language-cmd-line hljs">login1<span class="hljs-variable">$ </span>sbatch --dependency=<span class="hljs-symbol">afterok:</span><span class="hljs-number">173210</span> myjobscript</code></pre>
<p>For more information see the <a href="http://www.schedmd.com/" target="_blank">Slurm online documentation</a>. Note that you can use <code>$SLURM_JOBID</code> from one job to find the jobid you'll need to construct the <code>sbatch</code> launch line for a subsequent one. But also remember that you can't use <code>sbatch</code> to submit a job from a compute node.</p>
<h2 id="vis"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#vis">Visualization and VNC Sessions</a></h2>
<p>Stampede2 uses the SKX and KNL processors for all visualization and rendering operations. We use the Intel OpenSWR library to render raster graphics with OpenGL, and the Intel OSPRay framework for ray traced images inside visualization software. <strong>On Stampede2, <code>swr</code> replaces <code>vglrun</code> (e.g. <code>swr glxgears</code>) and uses similar syntax.</strong> OpenSWR can be loaded by executing <code>module load swr</code>. We expect most users will notice little difference in visualization experience on KNL. MCDRAM may improve visualization performance for some users. SKX nodes may provide better interactivity for intensive rendering applications.</p>
<p>There is currently no separate visualization queue on Stampede2. All visualization apps are available on all nodes. VNC and DCV sessions are available on any queue, either through the command line or via the <a href="https://tap.tacc.utexas.edu/" target="_blank">TACC Analysis Portal</a>. We recommend submitting to the <code>development</code> queue (for KNL) or the <code>skx-dev</code> queue (for SKX) for interactive sessions. If you are interested in an application that is not yet available, please submit a help desk ticket through the TACC User Portal.</p>
<h3 id="vis-remote"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#vis-remote">Remote Desktop Access</a></h3>
<p>Remote desktop access to Stampede2 is formed through a VNC connection to one or more visualization nodes. Users must first connect to a Stampede2 login node (see System Access) and submit a special interactive batch job that:</p>
<ul>
<li>allocates a set of Stampede2 visualization nodes </li>
<li>starts a vncserver process on the first allocated node </li>
<li>sets up a tunnel through the login node to the vncserver access port </li>
</ul>
<p>Once the vncserver process is running on the visualization node and a tunnel through the login node is created, an output message identifies the access port for connecting a VNC viewer. A VNC viewer application is run on the user's remote system and presents the desktop to the user.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If this is your first time connecting to Stampede2, you must run <code>vncpasswd</code> to create a password for your VNC servers. This should NOT be your login password! This mechanism only deters unauthorized connections; it is not fully secure, as only the first eight characters of the password are saved. All VNC connections are tunneled through SSH for extra security, as described below.</p>
</div>
<p>Follow the steps below to start an interactive session.</p>
<ol>
<li>
<p>Start a Remote Desktop </p>
<p>TACC has provided a VNC job script (<code>/share/doc/slurm/job.vnc</code>) that requests one node in the <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-queues"><code>development</code> queue</a> for two hours, creating a <a href="https://en.wikipedia.org/wiki/VNC" target="_blank">VNC</a> session.</p>
<pre class="highlight"><code class="language-cmd-line hljs">login1<span class="hljs-variable">$ </span>sbatch /share/doc/slurm/job.vnc</code></pre>
<p>You may modify or overwrite script defaults with <code>sbatch</code> command-line options:</p>
<ul>
<li><code>-t <i>hours:minutes:seconds</i></code> modify the job runtime </li>
<li><code>-A <i>projectnumber</i></code> specify the project/allocation to be charged </li>
<li><code>-N <i>nodes</i></code> specify number of nodes needed </li>
<li><code>-p <i>partition</i></code> specify an alternate queue. </li>
</ul>
<p>See more <code>sbatch</code> options in the <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#table6">Common <code>sbatch</code> Options</a> table above.</p>
<p>All arguments after the job script name are sent to the vncserver command. For example, to set the desktop resolution to 1440x900, use:</p>
<pre class="highlight"><code class="language-cmd-line hljs">login1<span class="hljs-variable">$ </span>sbatch /share/doc/slurm/job.vnc -geometry 1440x900</code></pre>
<p>The <code>vnc.job</code> script starts a vncserver process and writes to the output file, <code>vncserver.out</code> in the job submission directory, with the connect port for the vncviewer. Watch for the "To connect via VNC client" message at the end of the output file, or watch the output stream in a separate window with the commands:</p>
<pre class="highlight"><code class="language-cmd-line hljs">login1<span class="hljs-variable">$ </span>touch vncserver.out ; tail -f vncserver.out</code></pre>
<p>The lightweight window manager, <code>xfce</code>, is the default VNC desktop and is recommended for remote performance. Gnome is available; to use gnome, open the <code>~/.vnc/xstartup</code> file (created after your first VNC session) and replace <code>startxfce4</code> with <code>gnome-session</code>. Note that gnome may lag over slow internet connections.</p>
</li>
<li>
<p>Create an SSH Tunnel to Stampede2 </p>
<p>TACC requires users to create an SSH tunnel from the local system to the Stampede2 login node to assure that the connection is secure.   The tunnels created for the VNC job operate only on the <code>localhost</code> interface, so you must use <code>localhost</code> in the port forward argument, not the Stampede2 hostname.  On a Unix or Linux system, execute the following command once the port has been opened on the Stampede2 login node:</p>
<pre class="highlight"><code class="language-cmd-line hljs">
localhost<span class="hljs-variable">$ </span>ssh -f -N -L <span class="hljs-symbol">xxxx:</span><span class="hljs-symbol">localhost:</span>yyyy username<span class="hljs-variable">@stampede2</span>.tacc.utexas.edu</code></pre>
<p>where:</p>
<ul>
<li><code><i>yyyy</i></code> is the port number given by the vncserver batch job </li>
<li><code><i>xxxx</i></code> is a port on the remote system. Generally, the port number specified on the Stampede2 login node, <code><i>yyyy</i></code>, is a good choice to use on your local system as well </li>
<li><code>-f</code> instructs SSH to only forward ports, not to execute a remote command </li>
<li><code>-N</code> puts the <code>ssh</code> command into the background after connecting </li>
<li><code>-L</code> forwards the port </li>
</ul>
<p>On Windows systems find the menu in the Windows SSH client where tunnels can be specified, and enter the local and remote ports as required, then <code>ssh</code> to Stampede2.</p>
</li>
<li>
<p>Connecting vncviewer </p>
<p>Once the SSH tunnel has been established, use a <a href="https://en.wikipedia.org/wiki/Virtual_Network_Computing" target="_blank">VNC client</a> to connect to the local port you created, which will then be tunneled to your VNC server on Stampede2. Connect to <code>localhost:<i>xxxx</i></code>, where <code><i>xxxx</i></code> is the local port you used for your tunnel. In the examples above, we would connect the VNC client to <code>localhost::<i>xxxx</i></code>. (Some VNC clients accept <code>localhost:<i>xxxx</i></code>).</p>
<p>We recommend the <a href="http://sourceforge.net/projects/tigervnc/" target="_blank">TigerVNC</a> VNC Client, a platform independent client/server application.</p>
<p>Once the desktop has been established, two initial xterm windows are presented (which may be overlapping). One, which is white-on-black, manages the lifetime of the VNC server process. Killing this window (typically by typing <code>exit</code> or <code>ctrl-D</code> at the prompt) will cause the vncserver to terminate and the original batch job to end. Because of this, we recommend that this window not be used for other purposes; it is just too easy to accidentally kill it and terminate the session.</p>
<p>The other xterm window is black-on-white, and can be used to start both serial programs running on the node hosting the vncserver process, or parallel jobs running across the set of cores associated with the original batch job. Additional xterm windows can be created using the window-manager left-button menu.</p>
</li>
</ol>
<h3 id="vis-apps"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#vis-apps">Running Applications on the VNC Desktop</a></h3>
<p>From an interactive desktop, applications can be run from icons or from xterm command prompts. Two special cases arise: running parallel applications, and running applications that use OpenGL.</p>
<h3 id="vis-parallelapps"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#vis-parallelapps">Running Parallel Applications from the Desktop</a></h3>
<p>Parallel applications are run on the desktop using the same <code>ibrun</code> wrapper described above (see <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running">Running</a>). The command:</p>
<pre class="highlight"><code class="language-cmd-line hljs">c442-<span class="hljs-number">001</span><span class="hljs-variable">$ </span>ibrun ibrunoptions application applicationoptions</code></pre>
<p>will run application on the associated nodes, as modified by the <code>ibrun</code> options.</p>
<h3 id="vis-opengl"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#vis-opengl">Running OpenGL/X Applications On The Desktop</a></h3>
<p>Stampede2 uses the OpenSWR OpenGL library to perform efficient rendering. At present, the compute nodes on Stampede2 do not support native X instances. All windowing environments should use a VNC desktop launched via the job script in /share/doc/slurm/job.vnc or using the <a href="http://tap.tacc.utexas.edu/" title="TACC Analysis Portal" target="_blank">TACC Analysis Portal</a>. </p>
<p>swr: To access the accelerated OpenSWR OpenGL library, it is necessary to use the swr module to point to the swr OpenGL implementation and configure the number of threads to allocate to rendering.  </p>
<pre class="highlight"><code class="language-cmd-line hljs">c442-<span class="hljs-number">001</span><span class="hljs-variable">$ </span><span class="hljs-keyword">module</span> load swr
c442-<span class="hljs-number">001</span><span class="hljs-variable">$ </span>swr options application application-args</code></pre>
<h3 id="vis-visit"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#vis-visit">Parallel VisIt on Stampede2</a></h3>
<p><a href="https://wci.llnl.gov/simulation/computer-codes/visit/manuals" target="_blank">VisIt</a> was compiled under the Intel compiler and the mvapich2 and MPI stacks. </p>
<p>After connecting to a VNC server on Stampede2, as described above, load the VisIt module at the beginning of your interactive session before launching the Visit application:</p>
<pre class="highlight"><code class="language-cmd-line hljs">c442-<span class="hljs-number">001</span><span class="hljs-variable">$ </span><span class="hljs-keyword">module</span> load swr visit
c442-<span class="hljs-number">001</span><span class="hljs-variable">$ </span>swr visit</code></pre>
<p>VisIt first loads a dataset and presents a dialog allowing for selecting either a serial or parallel engine. Select the parallel engine. Note that this dialog will also present options for the number of processes to start and the number of nodes to use; these options are actually ignored in favor of the options specified when the VNC server job was started.</p>
<h4 id="vis-visit-preparingdata"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#vis-visit-preparingdata">Preparing Data for Parallel Visit</a></h4>
<p>VisIt reads <a href="https://github.com/visit-dav/visit/tree/develop/src/databases" target="_blank">nearly 150 data formats</a>. Except in some limited circumstances (particle or rectilinear meshes in ADIOS, basic netCDF, Pixie, OpenPMD and a few other formats), VisIt piggy-backs its parallel processing off of whatever static parallel decomposition is used by the data producer. This means that VisIt expects the data to be explicitly partitioned into independent subsets (typically distributed over multiple files) at the time of input. Additionally, VisIt supports a metadata file (with a <code>.visit</code> extension) that lists multiple data files of any supported format that hold subsets of a larger logical dataset. VisIt also supports a "brick of values (<code>bov)</code> format which supports a simple specification for the static decomposition to use to load data defined on rectilinear meshes. For more information on importing data into VisIt, see <a href="https://visit-dav.github.io/visit-website/pdfs/GettingDataIntoVisIt2.0.0.pdf?#page=97" target="_blank">Getting Data Into VisIt</a>.</p>
<h3 id="vis-paraview"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#vis-paraview">Parallel ParaView on Stampede2</a></h3>
<p>After connecting to a VNC server on Stampede2, as described above, do the following:</p>
<ol>
<li>
<p>Set up your environment with the necessary modules. Load the <code>swr</code>, <code>qt5</code>, <code>ospray</code>, and <code>paraview</code> modules <strong>in this order</strong>:</p>
<pre class="highlight"><code class="language-cmd-line hljs">c442-<span class="hljs-number">001</span><span class="hljs-variable">$ </span><span class="hljs-keyword">module</span> load swr qt5 ospray paraview</code></pre>
</li>
<li>
<p>Launch ParaView: </p>
<pre class="highlight"><code class="language-cmd-line hljs">c442-<span class="hljs-number">001</span><span class="hljs-variable">$ </span>swr -p <span class="hljs-number">1</span> paraview [paraview client options]</code></pre>
</li>
<li>
<p>Click the "Connect" button, or select File -&gt; Connect </p>
</li>
<li>
<p>Select the "auto" configuration, then press "Connect".  In the Paraview Output Messages window, you'll see what appears to be an 'lmod' error, but can be ignored.  Then you'll see the parallel servers being spawned and the connection established.</p>
</li>
</ol>
<h2 id="programming"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming">Programming and Performance</a></h2>
<p>Programming for performance is a broad and rich topic. While there are no shortcuts, there are certainly some basic principles that are worth considering any time you write or modify code.</p>
<h3 id="programming-general-timingprofiling"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-general-timingprofiling">Timing and Profiling</a></h3>
<p><strong>Measure performance and experiment with both compiler and runtime options.</strong> This will help you gain insight into issues and opportunities, as well as recognize the performance impact of code changes and temporary system conditions.</p>
<p>Measuring performance can be as simple as prepending the shell keyword <code>time</code> or the command <code>perf stat</code> to your launch line. Both are simple to use and require no code changes. Typical calls look like this:</p>
<pre class="highlight"><code class="language-cmd-line hljs">perf stat ./a.out    <span class="hljs-comment"># report basic performance stats for a.out</span>
time ./a.out         <span class="hljs-comment"># report the time required to execute a.out</span>
time ibrun ./a.out   <span class="hljs-comment"># time an MPI code</span>
ibrun time ./a.out   <span class="hljs-comment"># crude timings for each MPI task (no rank info)</span></code></pre>
<p>As your needs evolve you can add timing intrinsics to your source code to time specific loops or other sections of code. There are many such intrinsics available; some popular choices include <a href="http://man7.org/linux/man-pages/man2/gettimeofday.2.html" target="_blank"><code>gettimeofday</code></a>, <a href="https://www.mpich.org/static/docs/v3.2/www3/MPI_Wtime.html" target="_blank"><code>MPI_Wtime</code></a> and <a href="https://www.openmp.org/spec-html/5.0/openmpsu160.html" target="_blank"><code>omp_get_wtime</code></a>. The resolution and overhead associated with each of these timers is on the order of a microsecond.</p>
<p>It can be helpful to compare results with different compiler and runtime options: e.g. with and without <a href="http://software.intel.com/en-us/fortran-compiler-18.0-developer-guide-and-reference-vec-qvec" target="_blank">vectorization</a>, <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-launching-multi">threading</a>, or <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#files-striping">Lustre striping</a>. You may also want to learn to use profiling tools like <a href="http://software.intel.com/en-us/intel-vtune-amplifier-xe" target="_blank">Intel VTune Amplifier</a> <u>(<code>module load vtune</code>)</u> or GNU <a href="http://sourceware.org/binutils/docs/gprof/" target="_blank"><code>gprof</code></a>.</p>
<h3 id="programming-general-datalocality"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-general-datalocality">Data Locality</a></h3>
<p><strong>Appreciate the high cost (performance penalty) of moving data from one node to another</strong>, from disk to RAM, and even from RAM to cache. Write your code to keep data as close to the computation as possible: e.g. in RAM when needed, and on the node that needs it. This means keeping in mind the capacity and characteristics of each level of the memory hierarchy when designing your code and planning your simulations. A simple KNL-specific example illustrates the point: all things being equal, there's a good chance you'll see better performance when you keep your data in the KNL's <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-knl-memorymodes">fast MCDRAM</a> instead of the slower DDR4.</p>
<p>When possible, best practice also calls for so-called "stride 1 access" -- looping through large, contiguous blocks of data, touching items that are adjacent in memory as the loop proceeds. The goal here is to use "nearby" data that is already in cache rather than going back to main memory (a cache miss) in every loop iteration.</p>
<p>To achieve stride 1 access you need to understand how your program stores its data. Here C and C++ are different than (in fact the opposite of) Fortran. C and C++ are row-major: they store 2d arrays a row at a time, so elements <code>a[3][4]</code> and <code>a[3][5]</code> are adjacent in memory. Fortran, on the other hand, is column-major: it stores a column at a time, so elements <code>a(4,3)</code> and <code>a(5,3)</code> are adjacent in memory. Loops that achieve stride 1 access in the two languages look like this:</p>
<table border="1" cellspacing="3" cellpadding="3">
<tbody><tr><th>Fortran example</th><th>C example</th></tr>
<tr><td>
<pre class="highlight"><code class="language-syntax hljs">real*8 :: a(m,n), b(m,n), c(m,n)
 ...
! inner loop strides through col i
do i=1,n
  do j=1,m
    a(j,i)=b(j,i)+c(j,i)
  end do
end do</code></pre>
</td>
<td>
<pre class="highlight"><code class="language-syntax hljs">double a[m][n], b[m][n], c[m][n];
 ...
// inner loop strides through row i
for (i=0;i&lt;m;i++){
  for (j=0;j&lt;n;j++){
    a[i][j]=b[i][j]+c[i][j];
  }
}</code></pre>
</td></tr>
</tbody></table>

<h3 id="programming-general-vectorization"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-general-vectorization">Vectorization</a></h3>
<p><strong>Give the compiler a chance to produce efficient, <a href="http://software.intel.com/en-us/articles/vectorization-essential" target="_blank">vectorized</a> code</strong>. The compiler can do this best when your inner loops are simple (e.g. no complex logic and a straightforward matrix update like the ones in the examples above), long (many iterations), and avoid complex data structures (e.g. objects). See Intel's note on <a href="http://software.intel.com/en-us/node/522571" target="_blank">Programming Guidelines for Vectorization</a> for a nice summary of the factors that affect the compiler's ability to vectorize loops.</p>
<p>It's often worthwhile to generate <a href="http://software.intel.com/en-us/articles/getting-the-most-out-of-your-intel-compiler-with-the-new-optimization-reports" target="_blank">optimization and vectorization reports</a> when using the Intel compiler. This will allow you to see exactly what the compiler did and did not do with each loop, together with reasons why.</p>
<h3 id="programming-general-more"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-general-more">Learning More</a></h3>
<p>The literature on optimization is vast. Some places to begin a systematic study of optimization on Intel processors include: Intel's <a href="http://software.intel.com/en-us/modern-code" target="_blank">Modern Code</a> resources; and the <a href="http://intel.com/content/www/us/en/architecture-and-technology/64-ia-32-architectures-optimization-manual" target="_blank">Intel Optimization Reference Manual</a>. <!-- SDL and <a href="http://xortal.tacc.utexas.edu/training#/session/64">TACC training materials</a>. --></p>
<h3 id="programming-knl"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-knl">Programming and Performance: KNL</a></h3>
<h4 id="programming-knl-architecture"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-knl-architecture">Architecture</a></h4>
<p>KNL cores are grouped in pairs; each pair of cores occupies a tile. Since there are 68 cores on each Stampede2 KNL node, each node has 34 active tiles. These 34 active tiles are connected by a two-dimensional mesh interconnect. Each KNL has 2 DDR memory controllers on opposite sides of the chip, each with 3 channels. There are 8 controllers for the fast, on-package MCDRAM, two in each quadrant.</p>
<p>Each core has its own local L1 cache (32KB, data, 32KB instruction) and two 512-bit vector units. Both vector units can execute <code>AVX512</code> instructions, but only one can execute legacy vector instructions (<code>SSE</code>, <code>AVX</code>, and <code>AVX2</code>). Therefore, to use both vector units, you must compile with <code>-xMIC-AVX512</code>.</p>
<p>Each core can run up to 4 hardware threads. The two cores on a tile share a 1MB L2 cache. Different <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-knl-clustermodes">cluster modes</a> specify the L2 cache coherence mechanism at the node level.</p>
<h4 id="programming-knl-memorymodes"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-knl-memorymodes">Memory Modes</a></h4>
<p>The processor's memory mode determines whether the fast MCDRAM operates as RAM, as direct-mapped L3 cache, or as a mixture of the two. The output of commands like <code>top</code>, <code>free</code>, and <code>ps -v</code> reflect the consequences of memory mode. Such commands will show the amount of RAM available to the operating system, not the hardware (DDR + MCDRAM) installed.</p>
<figure id="figure-knlmemorymodes"><img src="./Stampede2 - TACC HPC Documentation_files/KNL-Memory-Modes.png"><figcaption>Figure 4. KNL Memory Modes</figcaption></figure>

<ul>
<li>
<p><strong>Cache Mode</strong>. In this mode, the fast MCDRAM is configured as an L3 cache. The operating system transparently uses the MCDRAM to move data from main memory. In this mode, the user has access to 96GB of RAM, all of it traditional DDR4. <strong>Most Stampede2 KNL nodes are configured in cache mode.</strong></p>
</li>
<li>
<p><strong>Flat Mode</strong>. In this mode, DDR4 and MCDRAM act as two distinct Non-Uniform Memory Access (NUMA) nodes. It is therefore possible to specify the type of memory (DDR4 or MCDRAM) when allocating memory. In this mode, the user has access to 112GB of RAM: 96GB of traditional DDR and 16GB of fast MCDRAM. By default, memory allocations occur only in DDR4. To use MCDRAM in flat mode, use the <code>numactl</code> utility or the <code>memkind</code> library; see <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-knl-managingmemory">Managing Memory</a> for more information. If you do not modify the default behavior you will have access only to the slower DDR4.</p>
</li>
<li>
<p><strong>Hybrid Mode (not available on Stampede2)</strong>. In this mode, the MCDRAM is configured so that a portion acts as L3 cache and the rest as RAM (a second NUMA node supplementing DDR4).</p>
</li>
</ul>
<h4 id="programming-knl-clustermodes"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-knl-clustermodes">Cluster Modes</a></h4>
<p>The KNL's core-level L1 and tile-level L2 caches can reduce the time it takes for a core to access the data it needs. To share memory safely, however, there must be mechanisms in place to ensure cache coherency. Cache coherency means that all cores have a consistent view of the data: if data value x changes on a given core, there must be no risk of other cores using outdated values of x. This, of course, is essential on any multi-core chip, but it is especially difficult to achieve on manycore processors.</p>
<p>The details for KNL are proprietary, but the key idea is this: each tile tracks an assigned range of memory addresses. It does so on behalf of all cores on the chip, maintaining a data structure (tag directory) that tells it which cores are using data from its assigned addresses. Coherence requires both tile-to-tile and tile-to-memory communication. Cores that read or modify data must communicate with the tiles that manage the memory associated with that data. Similarly, when cores need data from main memory, the tile(s) that manage the associated addresses will communicate with the memory controllers on behalf of those cores.</p>
<p>The KNL can do this in several ways, each of which is called a cluster mode. Each cluster mode, specified in the BIOS as a boot-time option, represents a tradeoff between simplicity and control. There are three major cluster modes with a few minor variations:</p>
<ul>
<li>
<p><strong>All-to-All</strong>. This is the most flexible and most general mode, intended to work on all possible hardware and memory configurations of the KNL. But this mode also may have higher latencies than other cluster modes because the processor does not attempt to optimize coherency-related communication paths.  Stampede2 does not have nodes in this cluster mode.</p>
</li>
<li>
<p><strong>Quadrant (variation: hemisphere)</strong>. This is Intel's recommended default, and the cluster mode of most Stampede2 queues. This mode attempts to localize communication without requiring explicit memory management by the programmer/user. It does this by grouping tiles into four logical/virtual (not physical) quadrants, then requiring each tile to manage MCDRAM addresses only in its own quadrant (and DDR addresses in its own half of the chip). This reduces the average number of "hops" that tile-to-memory requests require compared to all-to-all mode, which can reduce latency and congestion on the mesh.  </p>
</li>
<li>
<p><strong>Sub-NUMA 4 (variation: Sub-NUMA 2)</strong>. This mode, abbreviated <strong>SNC-4</strong>, divides the chip into four NUMA nodes so that it acts like a four-socket processor. SNC-4 aims to optimize coherency-related on-chip communication by confining this communication to a single NUMA node when it is possible to do so. To achieve any performance benefit, this requires explicit manual memory management by the programmer/user (in particular, allocating memory within the NUMA node that will use that memory). Stampede2 does not have nodes in this cluster mode.</p>
</li>
</ul>
<figure id="figure-knlclustermodes"><img src="./Stampede2 - TACC HPC Documentation_files/KNL-Cluster-Modes.png"><figcaption>Figure 5. KNL Cluster Modes</figcaption></figure>

<p>TACC's early experience with the KNL suggests that there is little reason to deviate from Intel's recommended default memory and cluster modes. Cache-quadrant tends to be a good choice for almost all workflows; it offers a nice compromise between performance and ease of use for the applications we have tested. Flat-quadrant is the most promising alternative and sometimes offers moderately better performance, especially when memory requirements per node are less than 16GB. We have not yet observed significant performance differences across cluster modes, and our current recommendation is that configurations other than cache-quadrant and flat-quadrant are worth considering only for very specialized needs. For more information see <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#knl-programming-managingmemory">Managing Memory</a> and <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#knl-programming-bestpractices">Best Known Practices...</a>.</p>
<h4 id="programming-knl-managingmemory"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-knl-managingmemory">Managing Memory</a></h4>
<p>By design, any application can run in any memory and cluster mode, and applications always have access to all available RAM. Moreover, regardless of memory and cluster modes, there are no code changes or other manual interventions required to run your application safely. However, there are times when explicit manual memory management is worth considering to improve performance. The Linux <code>numactl</code> (pronounced "NUMA Control") utility allows you to specify at runtime where your code should allocate memory.</p>
<p>When running in flat-quadrant mode, launch your code with <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#example">simple <code>numactl</code> settings</a> to specify whether memory allocations occur in DDR or MCDRAM. See <a href="https://docs.tacc.utexas.edu/training">TACC Training Materials</a> for additional information.</p>
<pre class="highlight"><code class="language-job-script hljs">numactl       --membind=0    ./a.out    <span class="hljs-comment"># launch a.out (non-MPI); use DDR (default)</span>
ibrun numactl --membind=0    ./a.out    <span class="hljs-comment"># launch a.out (MPI-based); use DDR (default)</span>

numactl       --membind=1    ./a.out    <span class="hljs-comment"># use only MCDRAM</span>
numactl       --preferred=1  ./a.out    <span class="hljs-comment"># (&lt;b&gt;RECOMMENDED&lt;/b&gt;) MCDRAM if possible; else DDR</span>
numactl       --hardware                <span class="hljs-comment"># show numactl settings</span>
numactl       --<span class="hljs-built_in">help</span>                    <span class="hljs-comment"># list available numactl options</span></code></pre>
<p>Examples. Controlling memory in flat-quadrant mode: <code>numactl</code> options  </p>
<p>Intel's new <code>memkind</code> library adds the ability to manage memory in source code with a special memory allocator for C code and a corresponding attribute for Fortran. This makes possible a level of control over memory allocation down to the level of the individual data element. As this library matures it will likely become an important tool for those who need fine-grained control of memory.</p>
<p>When you're running MPI codes in the flat-quadrant queue, the <code>mem_affinity</code> script simplifies memory management by calling <code>numactl</code> "under the hood" to make plausible NUMA (Non-Uniform Memory Access) policy choices. For MPI and hybrid applications, the script attempts to ensure that each MPI process uses MCDRAM efficiently. To launch your MPI code with <code>mem_affinity</code>, simply place <code>mem_affinity</code> immediately after <code>ibrun</code>:</p>
<pre><code>ibrun mem_affinity a.out
</code></pre>
<p>It's safe to use <code>mem_affinity</code> even when it will have no effect (e.g. cache-quadrant mode). Note that <code>mem_affinity</code> and <code>numactl</code> cannot be used together.</p>
<p>On Stampede2 the keyword <code>tacc_affinity</code> was originally an older name for what is now the <code>mem_affinity</code> script. To ensure backward compatibility, <code>tacc_affinity</code> is now a symbolic link to <code>mem_affinity</code>. Note that <code>mem_affinity</code> and the symbolic link <code>tacc_affinity</code> do not pin MPI tasks.</p>
<h4 id="programming-knl-bestpractices"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-knl-bestpractices">Best Known Practices and Preliminary Observations (KNL)</a></h4>
<p><strong>Hyperthreading. It is rarely a good idea to use all 272 hardware threads simultaneously</strong>, and it's certainly not the first thing you should try. In most cases it's best to specify no more than <u>64-68</u> MPI tasks or independent processes per node, and 1-2 threads/core. One exception is worth noting: when calling threaded MKL from a serial code, it's safe to set <code>OMP_NUM_THREADS</code> or <code>MKL_NUM_THREADS</code> to 272. This is because MKL will choose an appropriate thread count less than or equal to the value you specify. See <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#mkl-threading">Controlling Threading in MKL</a> for more information. In any case remember that the default value of <code>OMP_NUM_THREADS</code> is 1.</p>
<p><strong>When measuring KNL performance against traditional processors, compare node-to-node rather than core-to-core.</strong> KNL cores run at lower frequencies than traditional multicore processors. Thus, for a fixed number of MPI tasks and threads, a given simulation may run 2-3x slower on KNL than the same submission ran on Stampede1's Sandy Bridge nodes. A well-designed parallel application, however, should be able to run more tasks and/or threads on a KNL node than is possible on Sandy Bridge. If so, it may exhibit better performance per KNL node than it does on Sandy Bridge.</p>
<p><strong>General Expectations</strong>. From a pure hardware perspective, a single Stampede2 KNL node could outperform Stampede1's dual socket Sandy Bridge nodes by as much as 6x; this is true for both memory bandwidth-bound and compute-bound codes. This assumes the code is running out of (fast) MCDRAM on nodes configured in flat mode (450 GB/s bandwidth vs 75 GB/s on Sandy Bridge) or using cache-contained workloads on nodes configured in cache mode (memory footprint &lt; 16GB). It also assumes perfect scalability and no latency issues. In practice we have observed application improvements between 1.3x and 5x for several HPC workloads typically run in TACC systems. Codes with poor vectorization or scalability could see much smaller improvements. In terms of network performance, the Omni-Path network provides 100 Gbits per second peak bandwidth, with point-to-point exchange performance measured at over 11 GBytes per second for a single task pair across nodes. Latency values will be higher than those for the Sandy Bridge FDR Infiniband network: on the order of 2-4 microseconds for exchanges across nodes.</p>
<p><strong>MCDRAM in Flat-Quadrant Mode</strong>. Unless you have specialized needs, we recommend using <code>mem_affinity</code> or launching your application with <code>numactl --preferred=1</code> when running in flat-quadrant mode (see <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#knl-programming-managingmemory">Managing Memory</a> above). If you mistakenly use <code>--membind=1</code>, only the 16GB of fast MCDRAM will be available. If you mistakenly use <code>--membind=0</code>, you will not be able to access fast MCDRAM at all.</p>
<p><strong>Task Affinity</strong>. If you're running one threaded, MPI, or hybrid application at a time, default affinity settings are usually sensible and often optimal. <!-- SDL See <a href="https://xortal.tacc.utexas.edu/training#/session/41">TACC training materials</a> for more information.--> If you run more than one threaded, MPI, or hybrid application at a time, you'll want to pay attention to affinity. For more information see the appropriate sub-sections under <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#running-launching">Launching Applications</a>. </p>
<p><strong>MPI Initialization</strong>. Our preliminary scaling tests with Intel MPI on Stampede2 suggest that the time required to complete MPI initialization scales quadratically with the number of MPI tasks (lower case <code>-n</code> in your Slurm submission script) and linearly with the number of nodes (upper case <code>-N</code>).</p>
<p><strong>Tuning the Performance Scaled Messaging (PSM2) Library</strong>. When running on KNL with MVAPICH2, set the environment variable <code>PSM2_KASSIST_MODE</code> to the value <code>none</code> per the <a href="http://mvapich.cse.ohio-state.edu/static/media/mvapich/mvapich2-2.3b-userguide.html#x1-890006.19" target="_blank">MVAPICH2 User Guide</a>. Do not use this environment variable with IMPI; doing so may degrade performance. <!-- The <code>ibrun</code> launcher will eventually control this environment variable automatically. --></p>
<h3 id="programming-skx"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-skx">Programming and Performance: SKX and ICX</a></h3>
<p><strong>Hyperthreading. It is rarely a good idea to use all the hardware threads simultaneously</strong>, and it's certainly not the first thing you should try. In most cases it's best to specify no more than 48 MPI tasks or independent processes per SKX node (80 per ICX node), and 1-2 threads/core. One exception is worth noting: when calling threaded MKL from a serial code, it's safe to set <code>OMP_NUM_THREADS</code> or <code>MKL_NUM_THREADS</code> to 96 for SKX or 160 for ICX. This is because MKL will choose an appropriate thread count less than or equal to the value you specify. See <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#mkl-threading">Controlling Threading in MKL</a> for more information.  In any case remember that the default value of <code>OMP_NUM_THREADS</code> is 1.</p>
<p><strong>Clock Speed.</strong> The published nominal clock speed of the Stampede2 SKX processors is 2.1GHz and for the ICX processors it is 2.3GHz. But <a href="https://www.intel.com/content/www/us/en/architecture-and-technology/turbo-boost/turbo-boost-technology.html" target="_blank">actual clock speed varies widely</a>: it depends on the vector instruction set, number of active cores, and other factors affecting power requirements and temperature limits. At one extreme, a single serial application using the <code>AVX2</code> instruction set may run at frequencies approaching 3.7GHz, because it's running on a single core (in fact a single hardware thread). At the other extreme, a large, fully-threaded MKL <code>dgemm</code> (a highly vectorized routine in which all cores operate at nearly full throttle) may run at 1.4GHz.</p>
<p><strong>Vector Optimization and <code>AVX2</code>.</strong> In some cases, using the <code>AVX2</code> instruction set may produce better performance than <code>AVX512</code>. This is largely because cores can run at higher <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#clockspeeds">clock speeds</a> when executing <code>AVX2</code> code. To compile for <code>AVX2</code>, replace the <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#building-performance-architecture">multi-architecture flags</a> described above with the single flag <code>-xCORE-AVX2</code>. When you use this flag you will be able to build and run on any Stampede2 node.</p>
<p><strong>Vector Optimization and 512-Bit ZMM Registers.</strong> If your code can take advantage of wide 512-bit vector registers, you may want to try <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#building-performance-architecture">compiling for SKX and ICX</a> with (for example):</p>
<pre><code>-xCORE-AVX512 -qopt-zmm-usage=high
</code></pre>
<p>The <code>qopt-zmm-usage</code> flag affects the algorithms the compiler uses to decide whether to vectorize a given loop with <code>AVX51</code> intrinsics (wide 512-bit registers) or <code>AVX2</code> code (256-bit registers). When the flag is set to <code>-qopt-zmm-usage=low</code> (the default when compiling for SKX and ICX using <b><code>CORE-AVX512</code>)</b>, the compiler will choose <code>AVX2</code> code more often; this may or may not be the optimal approach for your application. The <code>qopt-zmm-usage</code> flag is available only on Intel compilers newer than 17.0.4. Do not use <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#building-performance-architecture"><code>$TACC_VEC_FLAGS</code></a> when specifying <code>qopt-zmm-usage</code>. This is because <code>$TACC_VEC_FLAGS</code> specifies <code>AVX2-CORE</code> as the base architecture, and the compiler will ignore <code>qopt-zmm-usage</code> unless the base target is a variant of <code>AVX512</code>. See the recent <a href="https://software.intel.com/en-us/articles/tuning-simd-vectorization-when-targeting-intel-xeon-processor-scalable-family" target="_blank">Intel white paper</a>, the <a href="https://software.intel.com/en-us/cpp-compiler-18.0-developer-guide-and-reference-qopt-zmm-usage-qopt-zmm-usage" target="_blank">compiler documentation</a>, the compiler man pages, and the notes above for more information.</p>
<p><strong>Vector Optimization and <code>COMMON-AVX512</code>.</strong> We have encountered a few complex packages that currently fail to build or run when compiled with <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#building-performance-architecture"><code>CORE-AVX512</code></a> (native SKX or ICX). In all cases so far, these packages build and run well on all KNL, SNL and ICX when compiled as a single-architecture binary with <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#building-performance-architecture"><code>-xCOMMON-AVX512</code></a>.</p>
<p><strong>Task Affinity.</strong> If you run one MPI application at a time, the <code>ibrun</code> MPI launcher will spread each node's tasks evenly across an SKX or ICX node's two sockets, with consecutive tasks occupying the same socket when possible.</p>
<p><strong>Hardware Thread Numbering.</strong> Execute <code>lscpu</code> or <code>lstopo</code> on an SKX or ICX node to see the numbering scheme for hardware threads. Note that hardware thread numbers alternate between the sockets: even numbered threads are on NUMA node 0, while odd numbered threads are on NUMA node 1. Furthermore, the two hardware threads on a given core have thread numbers that differ by exactly 48 for SKX and 80 for ICX (e.g. threads 3 and 51 are on the same core on SKX nodes).</p>
<p><strong>Tuning the Performance Scaled Messaging (PSM2) Library</strong>. When running on SKX with MVAPICH2, setting the environment variable <code>PSM2_KASSIST_MODE</code> to the value <code>none</code> may or may not improve performance. For more information see the <a href="http://mvapich.cse.ohio-state.edu/static/media/mvapich/mvapich2-2.3b-userguide.html#x1-890006.19" target="_blank">MVAPICH2 User Guide</a>. Do not use this environment variable with IMPI; doing so may degrade performance. The <code>ibrun</code> launcher will eventually control this environment variable automatically.</p>
<h3 id="programming-fileio"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-fileio">File Operations: I/O Performance</a></h3>
<p>This section includes general advice intended to help you achieve good performance during file operations. See <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#files-filesystems">Navigating the Shared File Systems</a> for a brief overview of Stampede2's Lustre file systems and the concept of striping. See <a href="https://learn.tacc.utexas.edu/" target="_blank">TACC Training material</a> for additional information on I/O performance.</p>
<p><strong>Follow the advice in <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#shared-lustre-file-systems">Good Conduct</a></strong> to avoid stressing the file system.</p>
<p><strong>Stripe for performance</strong>. If your application writes large files using MPI-based parallel I/O (including <a href="http://mpi-forum.org/docs/mpi-3.1/mpi31-report.pdf" target="_blank">MPI-IO</a>, <a href="https://support.hdfgroup.org/HDF5/PHDF5/" target="_blank">parallel HDF5</a>, and <a href="https://www.unidata.ucar.edu/software/netcdf/docs/parallel_io.html" target="_blank">parallel netCDF</a>, you should experiment with stripe counts larger than the default values (2 stripes on <code>$SCRATCH</code>, 1 stripe on <code>$WORK</code>). See <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#files-striping">Striping Large Files</a> for the simplest way to set the stripe count on the directory in which you will create new output files. You may also want to try larger stripe sizes up to 16MB or even 32MB; execute <code>man lfs</code> for more information. If you write many small files you should probably leave the stripe count at its default value, especially if you write each file from a single process. Note that it's not possible to change the stripe parameters on files that already exist. This means that you should make decisions about striping when you <em>create</em> input files, not when you read them.</p>
<p><strong>Aggregate file operations</strong>. Open and close files once. Read and write large, contiguous blocks of data at a time; this requires understanding how a given programming language uses memory to <a href="https://docs.tacc.utexas.edu/hpc/stampede2/#programming-general-datalocality">store arrays</a>.</p>
<p><strong>Be smart about your general strategy</strong>. When possible avoid an I/O strategy that requires each process to access its own files; such strategies don't scale well and are likely to stress a Lustre file system. A better approach is to use a single process to read and write files. Even better is genuinely parallel MPI-based I/O.</p>
<p><strong>Use parallel I/O libraries</strong>. Leave the details to a high performance package like <a href="http://mpi-forum.org/docs/mpi-3.1/mpi31-report.pdf" target="_blank">MPI-IO</a> (built into MPI itself), <a href="https://support.hdfgroup.org/HDF5/PHDF5/" target="_blank">parallel HDF5</a> <u>(<code>module load phdf5</code>)</u>, and <a href="https://www.unidata.ucar.edu/software/netcdf/docs/parallel_io.html" target="_blank">parallel netCDF</a> <u>(<code>module load pnetcdf</code>)</u>.</p>
<p>When using the Intel Fortran compiler, <strong>compile with "<a href="https://software.intel.com/en-us/fortran-compiler-18.0-developer-guide-and-reference-assume" target="_blank"><code>-assume buffered_io</code></a>"</strong>. Equivalently, set the environment variable <a href="https://software.intel.com/en-us/node/680054" target="_blank"><code>FORT_BUFFERED=TRUE</code></a>. Doing otherwise can dramatically slow down access to variable length unformatted files. More generally, direct access in Fortran is typically faster than sequential access, and accessing a binary file is faster than ASCII.</p>
<h2 id="refs"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#refs">References</a></h2>
<ul>
<li><a href="https://docs.tacc.utexas.edu/tutorials/bashstartup" title="Bash Quick Start Guide">Bash Users' Startup Files: Quick Start Guide</a></li>
<li><a href="https://docs.tacc.utexas.edu/software/idev"><code>idev</code> documentation</a></li>
<li><a href="https://www.gnu.org/doc/doc.en.html" target="_blank">GNU documentation</a></li>
<li><a href="http://software.intel.com/en-us/intel-software-technical-documentation" target="_blank">Intel software documentation</a></li>
<li><a href="http://lmod.readthedocs.org/" target="_blank">Lmod's online documentation</a></li>
<li><a href="https://docs.tacc.utexas.edu/tutorials/mfa">Multi-Factor Authentication at TACC</a></li>
<li><a href="https://docs.tacc.utexas.edu/tutorials/sharingprojectfiles" title="Sharing Project Files">Sharing Project Files on TACC Systems</a></li>
<li><a href="http://www.schedmd.com/" target="_blank">Slurm online documentation</a></li>
</ul>
<!-- * [TACC training materials](https://xortal.tacc.utexas.edu/training#/guest?training=upcoming) -->
<ul>
<li><a href="https://tap.tacc.utexas.edu/" target="_blank">TACC Analysis Portal</a></li>
</ul>
<h2 id="help"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#help">Help Desk</a></h2>
<p>TACC Consulting operates from 8am to 5pm CST, Monday through Friday, except for holidays. You can <a href="https://tacc.utexas.edu/about/help/" title="Help Desk" target="_blank">submit a help desk ticket</a> at any time via the TACC User Portal with "Stampede2" in the Resource field. Help the consulting staff help you by following these best practices when submitting tickets. </p>
<ul>
<li>
<p><strong>Do your homework</strong> before submitting a help desk ticket. What does the user guide and other documentation say? Search the internet for key phrases in your error logs; that's probably what the consultants answering your ticket are going to do. What have you changed since the last time your job succeeded?</p>
</li>
<li>
<p><strong>Describe your issue as precisely and completely as you can:</strong> what you did, what happened, verbatim error messages, other meaningful output. When appropriate, include the information a consultant would need to find your artifacts and understand your workflow: e.g. the directory containing your build and/or job script; the modules you were using; relevant job numbers; and recent changes in your workflow that could affect or explain the behavior you're observing.</p>
</li>
<li>
<p><strong><a href="https://accounts.tacc.utexas.edu/subscriptions" title="Subscribe to News" target="_blank">Subscribe to Stampede2 User News</a>.</strong> This is the best way to keep abreast of maintenance schedules, system outages, and other general interest items.</p>
</li>
<li>
<p><strong>Have realistic expectations.</strong> Consultants can address system issues and answer questions about Stampede2. But they can't teach parallel programming in a ticket, and may know nothing about the package you downloaded. They may offer general advice that will help you build, debug, optimize, or modify your code, but you shouldn't expect them to do these things for you.</p>
</li>
<li>
<p><strong>Be patient.</strong> It may take a business day for a consultant to get back to you, especially if your issue is complex. It might take an exchange or two before you and the consultant are on the same page. If the admins disable your account, it's not punitive. When the file system is in danger of crashing, or a login node hangs, they don't have time to notify you before taking action.</p>
</li>
</ul>
<h2 id="history"><a href="https://docs.tacc.utexas.edu/hpc/stampede2/#history">Revision History</a></h2>
<p>"Last Update" at the top of this document is the date of the most recent change to this document. This revision history is a list of non-trivial updates; it excludes routine items such as corrected typos and minor format changes.</p>
<li> 09/14/22 XSEDE project ends. Replace Globus with Grid Community Toolkit. </li>
<li> 03/07/22 Intel Ice Lake nodes introduced.  New `icx-normal` queue. </li>
<li> 04/24/18 Changes to Table 1 and Table 5 associated with new `long` queue. </li>
<li> 04/03/18 Stampede1 decommissioned; removed/revised references to Stampede1 as appropriate. </li>
<li> 03/26/18 Corrected and relocated material on `qopt-zmm-usage`. </li>
<li> 02/23/18 New functionality associated with `task_affinity`, `tacc_affinity`, and `mem_affinity` (scripts related to MPI task pinning and KNL memory management). </li>
<li> 11/30/17 Initial release supporting Phase 2 (SKX). </li>
<li> 08/02/17 Removed references and links to Stampede2 Transition Guide (now deprecated). </li>
<li> 06/12/17 Initial public release. </li>
              
            </div>
          </div>


<footer>
  
  
  

  <hr>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  
  
  
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="https://docs.tacc.utexas.edu/hpc/ranch/" style="color: #fcfcfc">« Previous</a></span>
    
    
      <span><a href="https://docs.tacc.utexas.edu/hpc/stallion/" style="color: #fcfcfc">Next »</a></span>
    
  </span>
</div>
    <script>var base_url = '../..';</script>
    
    
    
    <script src="./Stampede2 - TACC HPC Documentation_files/theme.js" defer=""></script>
      <script src="./Stampede2 - TACC HPC Documentation_files/addPageId.js" defer=""></script>
      <script src="./Stampede2 - TACC HPC Documentation_files/main.js" defer=""></script>
    
      <script src="./Stampede2 - TACC HPC Documentation_files/setTargetForExternalLinks.js" type="module"></script>
      <script src="./Stampede2 - TACC HPC Documentation_files/styleLinksOutsideMainContent.js" type="module"></script>
      <script src="./Stampede2 - TACC HPC Documentation_files/swapImgSvgWithRawSvg.js" type="module"></script>
      <script src="./Stampede2 - TACC HPC Documentation_files/removeThemeClasses.js" type="module"></script>
      <script src="./Stampede2 - TACC HPC Documentation_files/changeNavMarkup.js" type="module"></script>
    
    <script defer="">
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>



</body></html>